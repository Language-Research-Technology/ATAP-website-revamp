{"pageProps":{"post":{"title":"Textbooks, tutorials and tool chests: different ways to learn from notebooks - Part 1","date":"2022-07-27T16:20:35+10:00","slug":"notebooks-part-1","author":"Joel Nothman","content":"# Textbooks, tutorials and tool chests: different ways to learn from notebooks - Part 1\n\n### Written by Joel Nothman\n#### This work was supported by the [Sydney Informatics Hub](https://www.sydney.edu.au/research/facilities/sydney-informatics-hub.html), a Core Research Facility of the University of Sydney\n\nOne objective of the Australian Text Analytics Platform is to provide a library of code notebooks that our users ‚Äì those interested in applying text analytics to research ‚Äì can learn from and build upon. As the curators of such a resource, we need to learn from what‚Äôs been created before for a similar audience; and we need to understand how collections of notebooks can be used as an accessible resource for teaching, critiquing, and creating research using text analytics. These posts therefore summarise the insights we gained from a preliminary survey of key notebook resources.\n\nThis first part provides an introductory overview of the use of notebooks in text analytics and examines the pedagogically focussed genres of textbooks and tutorials. The second part looks at two other genres, tool chests and research articles, and ends with a summary and a consideration of how the discussion can inform the design of ATAP.\n\n## What are notebooks?\n\nNotebooks as popularised by the [Jupyter project](https://jupyter.org/) have become a key medium for experimentation and presentation in research and development. They mix code with narrative text, as well as tables, figures and interactive tools generated by the code.\n\n![Snippet from a notebook](/notebook.png)\n\n*Figure 1 - A screenshot of a part of a notebook*\n\nFigure 1 shows part of a notebook with some text, a code cell and the results generated by running the code\n\nA notebook can be played with interactively, allowing a user to interrogate and tweak the analysis and datasets under construction; but it can also be run as a prefabricated script that fetches a dataset, applies some processing, or generates a report. For researchers, published notebooks provide an opportunity for experimental reproducibility and reusable research pipelines.\n\nNotebooks collected together become a library of tutorials, tools, or reproducible research publications. We found several such collections covering applied text analytics, primarily targeting humanities and social sciences (HASS) researchers, as well as some resources that mix code and narrative, but in which the code cannot be executed directly. To make full use of such resources, it is necessary to copy the code and paste it into a console or another environment where it can be run.\n\nIn addition to having a mix of content, and different approaches to teaching or guiding readers to use text analytics tools, we noted a few broad genres of notebook and types of collection, such as tutorials, tool(kits), apps and research experiments, In this post, we focus on tutorials; the following post will look at the remaining genres..\nBefore going on, we‚Äôd like to note that our survey is by no means complete. We‚Äôve not had time to look in detail at the full diversity of relevant notebook collections, but hope that we have captured useful insights about the state of the art.\n\n## Courses for different horses\n\nWhile the various resources all covered core applied text analytics concepts ‚Äì including acquiring data from various sources, analysing n-gram frequencies, extracting named entities, building and evaluating a topic model, and so on ‚Äì notebook resources reflected different genres and hence user needs. Some sought to present a rigorous curriculum constructed of tutorials or lessons, others treat collections of notebooks as tools for the user to employ in their work, and others demonstrate the use of a notebook as a single-purpose reproducible research article.\n\n### Courses and Textbooks\n\nThe vast majority of notebook collections we identified positioned themselves as courses, with each notebook corresponding to a tutorial. Some tutorials provide scaffolding for exercises (with or without solutions), but at a baseline, they guide readers/users through the use of a tool or concept. Frequently, the intention is for these tutorial notebooks to be walked through in a workshop, but being openly published as notebooks allows users to execute and play with them outside of the classroom.\n\n<table>\n<tr><td><b>Genre</b></td><td>Tutorial</td></tr>\n<tr><td><b>Typical Structure</b></td><td><ul><li>Intro to method</li>\n<li>Prepare data</li>\n<li>Apply method</li>\n<li>Explore parameters</li>\n<li>Analyse results</li>\n<li>Exercises / next steps</li></ul>\n\n</td></tr>\n<tr><td><b>Strengths</b></td><td>Addresses weak skills in methods, tooling, and/or coding ability.\n\nA tutorial in a notebook allows the user to interject with exploratory code or exercises.</td></tr>\n<tr><td><b>Weaknesses</b></td><td>A tutorial may not show its user how to build the notebook they‚Äôre viewing.\n\nNot demonstrative of application to a research goal.</td></tr>\n<tr><td><b>Example</b></td><td><a href='https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/09-Topic-Modeling-Without-Mallet.html'>Topic Modeling Without Mallet</a></td></tr>\n</table>\n\nTutorials from different authors vary in how much they surround their code with narrative and background explanations of the techniques they are applying. Sinclair and Rockwell‚Äôs [The Art of Literary Text Analysis](https://github.com/sgsinclair/alta/blob/master/ipynb/ArtOfLiteraryTextAnalysis.ipynb) (ALTA) provides extensive narrative and appears more like a textbook than a course, not intended for delivery, but for learning and reference. Melanie Walsh‚Äôs [Introduction to Cultural Analytics & Python](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/04-Sentiment-Analysis.html) explicitly presents itself as a text book constituted of notebooks, relying on the [Jupyterbook](https://jupyterbook.org/intro.html) tool to give a glossy, accessible presentation, while also supporting the user to interact with the notebooks. Not unlike ALTA, this course demonstrates a deep appreciation of the breadth of technologies available and how they might be applied ‚Äì and where they might not be applicable or reliable ‚Äì in digital humanities contexts. It often supports the learner by giving multiple examples of how to apply a technique, which incidentally means repeated opportunities to demonstrate data preparation with diverse inputs.\n\nFor the R programming community, Martin Schweinberger‚Äôs [LADAL](https://slcladal.github.io/) shows a thoroughness in developing users‚Äô abilities from statistical basics to a breadth of NLP, corpus and computational linguistic methods. (As part of the ATAP project, LADAL tutorials are undergoing conversion from R markdown to Jupyter notebooks.) Technique-oriented introductory tutorials are then complemented with ‚Äúfocus studies‚Äù on disciplines of linguistic analysis such as learner language, stylistics or linguistic typology.\nIn comparison to the above sites, several other tutorial resources are much more bare, simply demonstrating how to implement a technique with little use of discussion or repeated examples.\nThe varying levels of narrative, and how they present the code, suggests that authors have a mix of goals that we might query:\n- Is the tutorial introducing a technique, or rather its use through code?\n- Is it teaching the application of a technique, or how it works?\n- Does it aim to teach the reader how to read the code? How to experiment with the code? How to compose that code themselves?\n- Does the notebook itself facilitate those engagements with code, or does the more ephemeral teacher?\n\nAlthough not recently updated, we are indebted to Quinn Dombrowski‚Äôs curation of a [list of relevant courses and tutorials](https://github.com/quinnanya/dh-jupyter#course-materials), among other digital humanities notebooks. A recent player in this space is [TAPI (Text Analysis Pedagogy Institute)](https://labs.jstor.org/tapi/), which places open publication of notebook-based courses at the core of their researcher schooling. TAPI incorporates several courses by individual presenters, although there does not currently appear to be a high level of standardisation among the TAPI courses, including in their licensing.\n[The Programming Historian](https://programminghistorian.org/) is also an excellent resource for peer-reviewed digital humanities tutorials, often helpfully grounded in use cases. Not being notebooks (nor R markdown), its tutorials can‚Äôt be directly executed, leaving the user to copy-paste its code or not engage interactively with its content. What sets the Programming Historian apart is its design as a journal of tutorials, i.e. a collaborative enterprise in text analytics pedagogy. While resources we surveyed are all essentially open to third party contributors who may offer amendments, most collections of tutorial notebooks have single or few authors and are not designed as primarily collaborative endeavours.\n\n[Part 2](/posts/notebooks-part-2) of this post looks at the remaining genres we have identified, libraries and tool chests and presentations of research results. Part 2 also includes a discussion of how the information collected here might shape our view of what ATAP can be.\n\n\n\n\nNote 1: Unfortunately, we confined our survey to English-language resources; and with a focus on the Notebook medium, our analysis is biassed to (but not exclusively) the Python programming language.\n\n\n\n\n"},"content":"<h1 id=\"id1\">Textbooks, tutorials and tool chests: different ways to learn from notebooks - Part 1</h1>\n<h3 id=\"id2\">Written by Joel Nothman</h3>\n<h4 id=\"id3\">This work was supported by the <a href=\"https://www.sydney.edu.au/research/facilities/sydney-informatics-hub.html\">Sydney Informatics Hub</a>, a Core Research Facility of the University of Sydney</h4>\n<p>One objective of the Australian Text Analytics Platform is to provide a library of code notebooks that our users ‚Äì those interested in applying text analytics to research ‚Äì can learn from and build upon. As the curators of such a resource, we need to learn from what‚Äôs been created before for a similar audience; and we need to understand how collections of notebooks can be used as an accessible resource for teaching, critiquing, and creating research using text analytics. These posts therefore summarise the insights we gained from a preliminary survey of key notebook resources.</p>\n<p>This first part provides an introductory overview of the use of notebooks in text analytics and examines the pedagogically focussed genres of textbooks and tutorials. The second part looks at two other genres, tool chests and research articles, and ends with a summary and a consideration of how the discussion can inform the design of ATAP.</p>\n<h2 id=\"id4\">What are notebooks?</h2>\n<p>Notebooks as popularised by the <a href=\"https://jupyter.org/\">Jupyter project</a> have become a key medium for experimentation and presentation in research and development. They mix code with narrative text, as well as tables, figures and interactive tools generated by the code.</p>\n<p><img src=\"/notebook.png\" alt=\"Snippet from a notebook\"></p>\n<p><em>Figure 1 - A screenshot of a part of a notebook</em></p>\n<p>Figure 1 shows part of a notebook with some text, a code cell and the results generated by running the code</p>\n<p>A notebook can be played with interactively, allowing a user to interrogate and tweak the analysis and datasets under construction; but it can also be run as a prefabricated script that fetches a dataset, applies some processing, or generates a report. For researchers, published notebooks provide an opportunity for experimental reproducibility and reusable research pipelines.</p>\n<p>Notebooks collected together become a library of tutorials, tools, or reproducible research publications. We found several such collections covering applied text analytics, primarily targeting humanities and social sciences (HASS) researchers, as well as some resources that mix code and narrative, but in which the code cannot be executed directly. To make full use of such resources, it is necessary to copy the code and paste it into a console or another environment where it can be run.</p>\n<p>In addition to having a mix of content, and different approaches to teaching or guiding readers to use text analytics tools, we noted a few broad genres of notebook and types of collection, such as tutorials, tool(kits), apps and research experiments, In this post, we focus on tutorials; the following post will look at the remaining genres..\nBefore going on, we‚Äôd like to note that our survey is by no means complete. We‚Äôve not had time to look in detail at the full diversity of relevant notebook collections, but hope that we have captured useful insights about the state of the art.</p>\n<h2 id=\"id5\">Courses for different horses</h2>\n<p>While the various resources all covered core applied text analytics concepts ‚Äì including acquiring data from various sources, analysing n-gram frequencies, extracting named entities, building and evaluating a topic model, and so on ‚Äì notebook resources reflected different genres and hence user needs. Some sought to present a rigorous curriculum constructed of tutorials or lessons, others treat collections of notebooks as tools for the user to employ in their work, and others demonstrate the use of a notebook as a single-purpose reproducible research article.</p>\n<h3 id=\"id6\">Courses and Textbooks</h3>\n<p>The vast majority of notebook collections we identified positioned themselves as courses, with each notebook corresponding to a tutorial. Some tutorials provide scaffolding for exercises (with or without solutions), but at a baseline, they guide readers/users through the use of a tool or concept. Frequently, the intention is for these tutorial notebooks to be walked through in a workshop, but being openly published as notebooks allows users to execute and play with them outside of the classroom.</p>\n<table>\n<tr><td><b>Genre</b></td><td>Tutorial</td></tr>\n<tr><td><b>Typical Structure</b></td><td><ul><li>Intro to method</li>\n<li>Prepare data</li>\n<li>Apply method</li>\n<li>Explore parameters</li>\n<li>Analyse results</li>\n<li>Exercises / next steps</li></ul>\n</td></tr>\n<tr><td><b>Strengths</b></td><td>Addresses weak skills in methods, tooling, and/or coding ability.\n<p>A tutorial in a notebook allows the user to interject with exploratory code or exercises.</td></tr></p>\n<tr><td><b>Weaknesses</b></td><td>A tutorial may not show its user how to build the notebook they‚Äôre viewing.\n<p>Not demonstrative of application to a research goal.</td></tr></p>\n<tr><td><b>Example</b></td><td><a href='https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/09-Topic-Modeling-Without-Mallet.html'>Topic Modeling Without Mallet</a></td></tr>\n</table>\n<p>Tutorials from different authors vary in how much they surround their code with narrative and background explanations of the techniques they are applying. Sinclair and Rockwell‚Äôs <a href=\"https://github.com/sgsinclair/alta/blob/master/ipynb/ArtOfLiteraryTextAnalysis.ipynb\">The Art of Literary Text Analysis</a> (ALTA) provides extensive narrative and appears more like a textbook than a course, not intended for delivery, but for learning and reference. Melanie Walsh‚Äôs <a href=\"https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/04-Sentiment-Analysis.html\">Introduction to Cultural Analytics &#x26; Python</a> explicitly presents itself as a text book constituted of notebooks, relying on the <a href=\"https://jupyterbook.org/intro.html\">Jupyterbook</a> tool to give a glossy, accessible presentation, while also supporting the user to interact with the notebooks. Not unlike ALTA, this course demonstrates a deep appreciation of the breadth of technologies available and how they might be applied ‚Äì and where they might not be applicable or reliable ‚Äì in digital humanities contexts. It often supports the learner by giving multiple examples of how to apply a technique, which incidentally means repeated opportunities to demonstrate data preparation with diverse inputs.</p>\n<p>For the R programming community, Martin Schweinberger‚Äôs <a href=\"https://slcladal.github.io/\">LADAL</a> shows a thoroughness in developing users‚Äô abilities from statistical basics to a breadth of NLP, corpus and computational linguistic methods. (As part of the ATAP project, LADAL tutorials are undergoing conversion from R markdown to Jupyter notebooks.) Technique-oriented introductory tutorials are then complemented with ‚Äúfocus studies‚Äù on disciplines of linguistic analysis such as learner language, stylistics or linguistic typology.\nIn comparison to the above sites, several other tutorial resources are much more bare, simply demonstrating how to implement a technique with little use of discussion or repeated examples.\nThe varying levels of narrative, and how they present the code, suggests that authors have a mix of goals that we might query:</p>\n<ul>\n<li>Is the tutorial introducing a technique, or rather its use through code?</li>\n<li>Is it teaching the application of a technique, or how it works?</li>\n<li>Does it aim to teach the reader how to read the code? How to experiment with the code? How to compose that code themselves?</li>\n<li>Does the notebook itself facilitate those engagements with code, or does the more ephemeral teacher?</li>\n</ul>\n<p>Although not recently updated, we are indebted to Quinn Dombrowski‚Äôs curation of a <a href=\"https://github.com/quinnanya/dh-jupyter#course-materials\">list of relevant courses and tutorials</a>, among other digital humanities notebooks. A recent player in this space is <a href=\"https://labs.jstor.org/tapi/\">TAPI (Text Analysis Pedagogy Institute)</a>, which places open publication of notebook-based courses at the core of their researcher schooling. TAPI incorporates several courses by individual presenters, although there does not currently appear to be a high level of standardisation among the TAPI courses, including in their licensing.\n<a href=\"https://programminghistorian.org/\">The Programming Historian</a> is also an excellent resource for peer-reviewed digital humanities tutorials, often helpfully grounded in use cases. Not being notebooks (nor R markdown), its tutorials can‚Äôt be directly executed, leaving the user to copy-paste its code or not engage interactively with its content. What sets the Programming Historian apart is its design as a journal of tutorials, i.e. a collaborative enterprise in text analytics pedagogy. While resources we surveyed are all essentially open to third party contributors who may offer amendments, most collections of tutorial notebooks have single or few authors and are not designed as primarily collaborative endeavours.</p>\n<p><a href=\"/posts/notebooks-part-2\">Part 2</a> of this post looks at the remaining genres we have identified, libraries and tool chests and presentations of research results. Part 2 also includes a discussion of how the information collected here might shape our view of what ATAP can be.</p>\n<p>Note 1: Unfortunately, we confined our survey to English-language resources; and with a focus on the Notebook medium, our analysis is biassed to (but not exclusively) the Python programming language.</p>\n","searchContent":{"posts":[{"title":"ATAP Architecture Introduction\n","slug":"atap-arch-preso","content":"\n<a href=\"/post-assets/atap-arch-preso/HASS RDC Technical Advisory Group ATAP intro.pdf\">PDF version</a> \n\n\nThis presentation was given By Moises Sacal Bonequi, Ben Foley and Peter Sefton to the HASS RDC and Indigenous Research Capability Program\nTechnical Advisory Group Meeting on 2022-09-02.\n\n\n    \n\n<section typeof='http://purl.org/ontology/bibo/Slide'>\n<img src='/post-assets/atap-arch-preso/Slide0.png' alt='HASS RDC Technical Advisory Group Meeting  ATAP Architecture Discussion Intro Moises Sacal Bonequi - m.sacalbonequi@uq.edu.au      Ben Foley - b.foley@uq.edu.au    Peter Sefton - p.sefton@uq.edu.au ' title='Slide: 0' border='1'  width='85%%'/>\n\n\n\nThe Language Data Commons of Australia (LDaCA) and the Australian Text Analytics Platform (ATAP) are establishing a scalable and flexible language data and analytics commons. These projects will be part of the Humanities and Social Sciences Research Data Commons (HASS RDC).\n\nThe Data Commons will focus on preservation and discovery of distributed multi-modal language data collections under a variety of governance frameworks. This will include access control that reflects ethical constraints and intellectual property rights, including those of Aboriginal and Torres Strait Islander, migrant and Pacific communities.\n\nThis presentation builds on an [overview of the LDaCA and ATAP architecture](https://www.ldaca.edu.au/posts/rdc-tech-meeting) from 2022-02-11. This time we will zoom in on the Text Analytics side and show progress on linking active workspaces for an analysis with access-controlled data repositories, creating a [FAIR]-ready platform. Parts of this presentation are recycled from the previous one.\n\n[FAIR]: https://www.nature.com/articles/sdata201618\n\n\n\n</section>\n\n\n\n<section typeof='http://purl.org/ontology/bibo/Slide'>\n<img src='/post-assets/atap-arch-preso/Slide1.png' alt='  ' title='Slide: 1' border='1'  width='85%%'/>\n\n\n\nFor this Research Data Commons work we are using the Arkisto Platform (introduced [at eResearch 2020](http://ptsefton.com/2020/11/23/Arkisto/index.html)).\n\nArkisto aims to ensure the long term preservation of data independently of code and services, recognizing the ephemeral nature of software and platforms. We know that sustaining software platforms can be hard and aim to make sure that important data assets are not locked up in database or hard-coded logic of some hard-to-maintain application.\n\n\n\n</section>\n\n\n\n<section typeof='http://purl.org/ontology/bibo/Slide'>\n<img src='/post-assets/atap-arch-preso/Slide2.png' alt='  Repositories: institutional, domain or both       Find / Access services Research Data Management Plan Workspaces:  working storage domain specific tools domain specific services collect describe analyse Reusable, Interoperable  data objects deposit early deposit often Findable, Accessible, Reusable data objects reuse data objects V1.1  ¬© Marco La Rosa, Peter Sefton 2021 https://creativecommons.org/licenses/by-sa/4.0/   üóëÔ∏è Active cleanup processes  workspaces considered ephemeral üóëÔ∏è Policy based data management ' title='Slide: 2' border='1'  width='85%%'/>\n\n\n\nThe above diagram takes a big-picture view of research data management in the context of *doing* research. It makes a distinction between managed repository storage and the places where work is done - ‚Äúworkspaces‚Äù. Workspaces are where researchers collect, analyse and describe data. Examples include the most basic of research IT services, file storage, as well as analytical tools such as Jupyter notebooks (the backbone of ATAP - the text analytics platform). Other examples of workspaces include code repositories such as GitHub or GitLab (a slightly different sense of the word repository), survey tools, electronic (lab) notebooks and bespoke code written for particular research programmes. These workspaces are essential research systems but usually are not set up for long term management of data.\nThe cycle in the centre of  this diagram shows an idealised research practice where data are collected and described and deposited into a repository frequently. Data are made findable and accessible as soon as possible and can be ‚Äúre-collected‚Äù for use and re-use.\n\nFor data to be re-usable by humans and machines (such as ATAP notebook code that consumes datasets in a predictable way) it must be well described. The ATAP and LDaCA approach to this is to use the Research Object Crate (RO-Crate) specification. RO-Crate is essentially a guide to using a number of standards to describe both data and re-runnable software such as workflows or notebooks.\n\n\n\n</section>\n\n\n\n<section typeof='http://purl.org/ontology/bibo/Slide'>\n<img src='/post-assets/atap-arch-preso/Slide3.png' alt='Compute  HPC Cloud Desktop   collect describe analyse üóëÔ∏è Active cleanup processes  workspaces considered ephemeral ‚Ä¶ etc ATAP Notebooks Apps, Code, Workflows  Deposit /Publish PARADISEC Analytics Portal Code discovery Launch / Rerun Data Discovery Authenticated API       Workbench Notebooks Data import by URL Export fully described pkg Stretch goals: Code gen / simple interfaces eg Discursis   BYOData ü•Ç  ‚öôÔ∏è STORAGE (including Cloudstor) . Data Curation &amp; description Reuse Licence Server Identity Management AAF / social media accounts  Data Cleaning OCR / transcription format migration Archive &amp; Preservation Repositoriesinstitutional, domain or both AU Nat. Corpus AusLan (sign) Sydney Speaks ATAP Corpus Reference,Training &amp; BYO Workspaces: working storage domain specific tools domain specific services Harvested external Lang. portal(s) Corpus discovery Item discovery Authenticated API Create virtual corpora   ' title='Slide: 3' border='1'  width='85%%'/>\n\n\n\n\nThis rather messy slide represents the overall high-level architecture for the LDaCA Research Data Commons. There will be an analytical workbench (left of the diagram) which is the basis of the Australian Text Analytics (ATAP) project. This will focus on notebook-style programming using one of the emerging Jupyter notebook platforms in that space. Our engagement lead, Dr Simon Musgrave sees the ATAP work as primarily an educational enterprise encouraging researchers to adopt new research practices, which will be underpinned by services built on the Arkisto standards, allowing for rigorous, re-runnable research.\n\n\n\n\n\n\n\n\n</section>\n\n\n\n<section typeof='http://purl.org/ontology/bibo/Slide'>\n<img src='/post-assets/atap-arch-preso/Slide4.png' alt='  ' title='Slide: 4' border='1'  width='85%%'/>\n\n\n\nThis diagram is a much simpler view zooming in on the core infrastructure components that we have built so far. We are starting with bulk ingest of existing collections and will add one-by-one deposit of individual items after that.\n\nThis shows the OCFL repository at the bottom, with a Data & Access API that mediates access. This API understands the RO-Crate format and in particular its use of the Portland Common Data Model to structure data. The API also enforces access control to objects. Every repository object has a license setting out the terms of use and re-use for its data, which will reflect the way the data were collected. Information about copyright and privacy laws, and whether participants have signed agreements and ethics approvals, are all relevant here. Each license will correspond to a group of people who have agreed to and/or been selected by a data custodian. We are in negotiations with the [Australian Access Federation (AAF)](https://aaf.edu.au/) to use their [CILogon](https://www.cilogon.org/) service for this authorization step and for authentication of users across a wide variety of services including the AAF itself, Google, Microsoft and GitHub.\n\nThere‚Äôs also an access portal which will be based on a full-text index (at this stage we‚Äôre using ElasticSearch) which is designed to help people find data they might be interested in using. This follows current conventions for browse/search interfaces which we‚Äôre familiar with from shopping sites. You can search for text and/or drill down using *facets* (which are called aggregations in Elastic-land), for example, ‚Äúwhich language am I in interested in‚Äù or ‚Äúdo I want [ ] Spoken or [ ] Written material‚Äù? \n\n\n\n\n\n</section>\n\n\n\n<section typeof='http://purl.org/ontology/bibo/Slide'>\n<img src='/post-assets/atap-arch-preso/Slide5.png' alt='skinparam cloud&lt;&lt;workspace&gt;&gt; {    backgroundcolor LightGreen  }   ' title='Slide: 5' border='1'  width='85%%'/>\n\n\n\nThis diagram shows some of the main components in the ATAP ecosystem of services.\n\nA website (where you are reading this) will guide users to data in repository services (green, bottom left) where they can see data and tools together. Here, the user can choose code to run, which will then be instantiated by the BinderHub service (pink, bottom right) in the ‚Äúworkspaces‚Äù.\n\n\nThe website will also aid in discovery of training and events.\n\n\n\n</section>\n\n\n\n<section typeof='http://purl.org/ontology/bibo/Slide'>\n\n\n<iframe width=\"701\" height=\"701\" src=\"https://www.youtube.com/embed/P23IcsoegbE\" title=\"ATAP Data Demo\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n\nThis silent screen recording shows Moises Sacal Bonequi navigating through the first two of many data collections in the [ATAP data repository], looking at discovery information that describes the collections and their context. Each of the collections are linked to Jupyter notebooks that can consume data from the notebooks. When Moises clicks on one of these, he can preview it in the data portal, and launch it into a fresh virtual computing instance via the ATAP BinderHub workspace running on the [ARDC Nectar Research Cloud].\n\nThe recording also shows a couple of Jupyter notebooks that operate on the two data collections in the repository.\n\n1.  The [COOEE notebook](https://github.com/Australian-Text-Analytics-Platform/cooee/blob/main/cooee.ipynb), developed by Ben Foley the Applications and Training lead at ATAP, is a bare-bones demonstration of how to explore the collection, look at files etc.\n\n2.  The [Farms to Freeways notebook](https://github.com/Australian-Text-Analytics-Platform/farms-to-freeways/blob/main/farms-to-freeways.ipynb) was developed by one of the data scientists who was working with us at UQ, Mel Mistica, along with our tech team. This notebook uses the API to get the metadata for a social history collection containing transcribed interviews with women in Western Sydney. The notebook shows how a data scientist might explore what‚Äôs in a collection, such as the age distribution of the participants, and start analysing the content in the collection.\n\nNext steps:\n-  add more data, more notebooks and training material\n-  extend functionality of the BinderHub workshop for wider range of notebooks\n\n\n[ATAP data repository]: https://data.atap.edu.au/\n[ARDC Nectar Research Cloud]: https://ardc.edu.au/services/nectar-research-cloud/\n\n\n\n\n\n</section>\n\n"},{"title":"Discursis\n","slug":"discursis","content":"\n## Discursis\n\nDiscursis is communication analytics technology that allows a user to analyse text based communication data, such as conversations, web forums and training scenarios. It uses natural language processing (NLP) algorithms to automatically process transcribed text to highlight participant interactions around specific topics and over the time-course of the conversation. Discursis can assist practitioners in understanding the structure, information content, and inter-speaker relationships that are present within input data. Discursis also provides quantitative measures of key metrics, such as topic introduction, topic consistency, and topic novelty.\n\nThe NLP algorithms are used to construct a matrix of concept similarity scores between the sections into which a text has been divided. In the typical use case for this tool, that of a discourse with several speakers, those sections will be speaker turns and the similarity matrix provides information about the extent to which any pair of turns share concepts. This information, along with the sequential nature of the interaction, makes it possible to track topics which are maintained, or dropped, or dropped and then picked up again. It is also possible to examine the extent to which speakers are sharing concepts. These possibilities have been used in analysing various kinds of interactions, including medical consultations (see references below).\n\nDiscursis also has tools for visualising the analysis, and you can see an example of this here.\n\n<table>\n<tr><td><img src= '../discursis1.jpg' /></td><td><img src= '../discursis2.jpg' /></td></tr>\n<tr><td>Figure 1</td><td>Figure 2</td></tr>\n</table>\n\nThe data on which these graphics are based is a debate between Kevin Rudd and Tony Abbott held at the National Press Club on 11 August 2013. Figure 1 shows a visualisation of the whole debate, Figure 2 zooms in on a section of the interaction. The boxes on the diagonal represent the speaker turns, and you can see in Figure 2 that hovering the cursor over a box causes the text of that turn to be visible. The boxes back in the matrix represent the conceptual similarity between each pair of turns. A heavily populated column means that the topics in a turn were also in many following turns and a heavily populated row means that a turn shared topics with many preceding turns. Selecting a point of intersection in the matrix displays a similarity score for the turns, and the text of both turns is displayed below the main graphic (not shown here).\n\n[Discursis](https://itee.uq.edu.au/project/discursis)¬†was developed by¬†[Dan Angus](https://www.qut.edu.au/about/our-people/academic-profiles/daniel.angus),¬†[Janet Wiles](https://itee.uq.edu.au/profile/2444/janet-wiles)¬†and Andrew Smith and has been reworked as an open source tool by staff of¬†[Sydney Informatics Hub](https://www.sydney.edu.au/research/facilities/sydney-informatics-hub.html). A version of the tool running in a Jupyter notebook is available in this [Github repository](https://github.com/Australian-Text-Analytics-Platform/discursis).\n\n#### References\n\n<div class=\"reference\"><font size=\"3\">Angus, D., Smith, A. E., & Wiles, J. (2012). Human Communication as Coupled Time Series: Quantifying Multi-Participant Recurrence. <i>IEEE Transactions on Audio, Speech, and Language Processing</i>, 20(6), 1795‚Äì1807. <a href=\"https://doi.org/10.1109/TASL.2012.2189566\" target=\"_blank\">https://doi.org/10.1109/TASL.2012.2189566</a>.</font></div>\n<div class=\"reference\"><font size=\"3\">Angus, D., & Wiles, J. (2018). Social semantic networks: Measuring topic management in discourse using a pyramid of conceptual recurrence metrics. <i>Chaos: An Interdisciplinary Journal of Nonlinear Science</i>, 28(8), 085723. <a href=\"https://doi.org/10.1063/1.5024809\" target=\"_blank\">https://doi.org/10.1063/1.5024809</a>.</font></div>\n"},{"title":"What are the FAIR and CARE principles and why should corpus linguists know about them?","slug":"fair-and-care","tags":["FAIR","CARE"],"content":"# FAIR and CARE\nData is becoming increasingly important in today‚Äôs world, so corpus linguists might feel that the rest of the world is finally catching up. But the rest of the world are bringing with them new approaches to how data is handled. This means that fields such as corpus linguistics may need to reassess their practices. Such reassessment includes addressing concerns about how data is stored and who can access it (data stewardship) ‚Äì concerns that are a part of the [Open Science](https://en.wikipedia.org/wiki/Open_science) movement, ultimately grounded on principles of equity and accountability. \n\nThe most influential approach to data stewardship today is the [FAIR](https://www.go-fair.org/) principles.\nAccording to these principles, data should be:\n- *Findable* \n<br />\n&emsp; Metadata and data should be easy to find for both humans and computers. \n- *Accessible*\n<br />\n&emsp; Once the user finds the required data, she/he/they need to know how can they be accessed, possibly including authentication and authorisation.\n- *Interoperable*\n<br />\n&emsp; The data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing.\n- *Reusable*\n<br />\n&emsp; The ultimate goal of FAIR is to optimise the reuse of data. To achieve this, metadata and data should be well-described so that they can be replicated and/or combined in different settings.\n<br /><br />\n\nIn general, corpus linguists do well on the interoperability criterion. Corpus data is usually stored in non-proprietary formats; even when some structure is imposed on the data, this is almost always in a form which is saved as a simple text file (e.g. csv files or xml annotations). Data stored in such formats is easy to move between applications. But what about the other three criteria? \n\nSome corpus data is easy to discover; it is findable. For example CLARIN, the [portal](https://www.clarin.eu/content/data) to the European Union language resource infrastructure, provides access to many large data collections, as does the [Linguistic Data Consortium](https://www.ldc.upenn.edu/) in the USA. However, some data is never made part of a large collection and often remains under the control of individual researchers or research teams. Such data may be almost impossible to find. Even if we can find such data, it is unlikely to be accompanied by good descriptions of the data and metadata, making reusability problematic. Of course, big corpora such as the [British National Corpus](http://www.natcorp.ox.ac.uk/) will be both findable and accompanied by comprehensive corpus manuals. However, it is worth considering how to make other corpora more findable, including the provision of corpus manuals or corpus descriptions. Corpus resource databases such as [CoRD](https://varieng.helsinki.fi/CoRD/) do aim to work towards this principle.\n\nAccessibility may also be an issue for some data. Copyright law may allow use of material for individual research but prohibit any further distribution of the material. The FAIR approach to such cases is that metadata should be available so that interested parties can know that a data holding exists (F), and the metadata will include information about the conditions under which the data may or may not be shared or reused (A and R). \n\n![FAIR and CARE principles](/fair-care.png)\n\nImage from Global Indigenous Data Alliance (https://www.gida-global.org/)\n<br /><br />\nFor linguists, there is another very important set of principles concerning data, the CARE principles developed by the Global Indigenous Data Alliance:\n<br />\n- *Collective Benefit*\n<br />\n&emsp; Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data.\n<br />\n- *Authority to control*\n<br />\n&emsp;Indigenous Peoples‚Äô rights and interests in Indigenous data must be recognised and their authority to control such data be empowered.\n<br />\n- *Responsibility*\n<br />\n&emsp;Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples‚Äô self-determination and collective benefit.\n<br />\n- *Ethics*\n<br />\n&emsp;Indigenous Peoples‚Äô rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem.\n\n<br />\nThese principles are presented as applying particularly to Indigenous data, but we believe that researchers should adopt this approach in all cases where the people who participate in our research can be seen to have some moral rights in the information they have contributed. Respecting those moral rights should be demonstrated by recognising the participants‚Äô authority to control how data is used, by seeking to ensure that participants derive benefit from use of the data, and by acting ethically and transparently in our relations with the participants. Deborah Cameron and her colleagues (Cameron et al 1993) raised similar issues almost 20 years ago, arguing that the imbalance of power in the relation between researchers and participants needed to be reduced. The CARE principles continue along this path, but go even further in explicitly returning power to the sources of information. \n\nCorpus data is often written language. We have already mentioned that copyright law is relevant to some such material, and that body of law protects at least some rights for the creators of the material. But corpus linguists also work with other kinds of data such as spoken language (spontaneous or produced as a response to some prompt) or written material produced by research participants according to some protocol. In such cases, ethical research practice should include addressing the issues raised by the CARE principles. Some aspects of this practice will fall under institutional ethics requirements (for example, thinking carefully about what permissions we request on consent forms), but other questions must be part of the relationship between the researcher and the research participants. Corpus linguists working with spoken, computer-mediated, or otherwise particularly sensitive data have been aware of at least some of these issues, but the CARE principles offer an opportunity to go further.\n\nAcquiring data for linguistic research takes effort and often that means money. It is therefore a good use of resources if any data we collect can be used by others. The FAIR principles provide a framework to make sharing and reusing data easier, and applying the CARE principles where relevant helps to ensure that our research has a sound ethical basis.\n\n<br />\n<hr />\n<br />\n\nNote: This post is based on the presentation ‚ÄòAdvance Australia FAIR‚Äô,  given by Simon Musgrave and Michael Haugh to the 4th Forum on Englishes in Australia (LaTrobe University, August 27, 2021). \n\n<br />\n\nThanks to Leah Gustafson and Monika Bednarek for helpful comments on drafts.\n\n<br />\n\n**Reference:**\nCameron, Deborah, Elizabeth Frazer, Penelope Harvey, Ben Rampton & Kay Richardson. 1993. Ethics, advocacy and empowerment: Issues of method in researching language. Language & Communication 13(2). 81‚Äì94. [https://doi.org/10.1016/0271-5309(93)90001-4](https://doi.org/10.1016/0271-5309(93)90001-4)\n\n\n\n"},{"title":"Textbooks, tutorials and tool chests: different ways to learn from notebooks - Part 1","slug":"notebooks-part-1","tags":["Jupyter","notebooks","text analysis"],"content":"# Textbooks, tutorials and tool chests: different ways to learn from notebooks - Part 1\n\n### Written by Joel Nothman\n#### This work was supported by the [Sydney Informatics Hub](https://www.sydney.edu.au/research/facilities/sydney-informatics-hub.html), a Core Research Facility of the University of Sydney\n\nOne objective of the Australian Text Analytics Platform is to provide a library of code notebooks that our users ‚Äì those interested in applying text analytics to research ‚Äì can learn from and build upon. As the curators of such a resource, we need to learn from what‚Äôs been created before for a similar audience; and we need to understand how collections of notebooks can be used as an accessible resource for teaching, critiquing, and creating research using text analytics. These posts therefore summarise the insights we gained from a preliminary survey of key notebook resources.\n\nThis first part provides an introductory overview of the use of notebooks in text analytics and examines the pedagogically focussed genres of textbooks and tutorials. The second part looks at two other genres, tool chests and research articles, and ends with a summary and a consideration of how the discussion can inform the design of ATAP.\n\n## What are notebooks?\n\nNotebooks as popularised by the [Jupyter project](https://jupyter.org/) have become a key medium for experimentation and presentation in research and development. They mix code with narrative text, as well as tables, figures and interactive tools generated by the code.\n\n![Snippet from a notebook](/notebook.png)\n\n*Figure 1 - A screenshot of a part of a notebook*\n\nFigure 1 shows part of a notebook with some text, a code cell and the results generated by running the code\n\nA notebook can be played with interactively, allowing a user to interrogate and tweak the analysis and datasets under construction; but it can also be run as a prefabricated script that fetches a dataset, applies some processing, or generates a report. For researchers, published notebooks provide an opportunity for experimental reproducibility and reusable research pipelines.\n\nNotebooks collected together become a library of tutorials, tools, or reproducible research publications. We found several such collections covering applied text analytics, primarily targeting humanities and social sciences (HASS) researchers, as well as some resources that mix code and narrative, but in which the code cannot be executed directly. To make full use of such resources, it is necessary to copy the code and paste it into a console or another environment where it can be run.\n\nIn addition to having a mix of content, and different approaches to teaching or guiding readers to use text analytics tools, we noted a few broad genres of notebook and types of collection, such as tutorials, tool(kits), apps and research experiments, In this post, we focus on tutorials; the following post will look at the remaining genres..\nBefore going on, we‚Äôd like to note that our survey is by no means complete. We‚Äôve not had time to look in detail at the full diversity of relevant notebook collections, but hope that we have captured useful insights about the state of the art.\n\n## Courses for different horses\n\nWhile the various resources all covered core applied text analytics concepts ‚Äì including acquiring data from various sources, analysing n-gram frequencies, extracting named entities, building and evaluating a topic model, and so on ‚Äì notebook resources reflected different genres and hence user needs. Some sought to present a rigorous curriculum constructed of tutorials or lessons, others treat collections of notebooks as tools for the user to employ in their work, and others demonstrate the use of a notebook as a single-purpose reproducible research article.\n\n### Courses and Textbooks\n\nThe vast majority of notebook collections we identified positioned themselves as courses, with each notebook corresponding to a tutorial. Some tutorials provide scaffolding for exercises (with or without solutions), but at a baseline, they guide readers/users through the use of a tool or concept. Frequently, the intention is for these tutorial notebooks to be walked through in a workshop, but being openly published as notebooks allows users to execute and play with them outside of the classroom.\n\n<table>\n<tr><td><b>Genre</b></td><td>Tutorial</td></tr>\n<tr><td><b>Typical Structure</b></td><td><ul><li>Intro to method</li>\n<li>Prepare data</li>\n<li>Apply method</li>\n<li>Explore parameters</li>\n<li>Analyse results</li>\n<li>Exercises / next steps</li></ul>\n\n</td></tr>\n<tr><td><b>Strengths</b></td><td>Addresses weak skills in methods, tooling, and/or coding ability.\n\nA tutorial in a notebook allows the user to interject with exploratory code or exercises.</td></tr>\n<tr><td><b>Weaknesses</b></td><td>A tutorial may not show its user how to build the notebook they‚Äôre viewing.\n\nNot demonstrative of application to a research goal.</td></tr>\n<tr><td><b>Example</b></td><td><a href='https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/09-Topic-Modeling-Without-Mallet.html'>Topic Modeling Without Mallet</a></td></tr>\n</table>\n\nTutorials from different authors vary in how much they surround their code with narrative and background explanations of the techniques they are applying. Sinclair and Rockwell‚Äôs [The Art of Literary Text Analysis](https://github.com/sgsinclair/alta/blob/master/ipynb/ArtOfLiteraryTextAnalysis.ipynb) (ALTA) provides extensive narrative and appears more like a textbook than a course, not intended for delivery, but for learning and reference. Melanie Walsh‚Äôs [Introduction to Cultural Analytics & Python](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/04-Sentiment-Analysis.html) explicitly presents itself as a text book constituted of notebooks, relying on the [Jupyterbook](https://jupyterbook.org/intro.html) tool to give a glossy, accessible presentation, while also supporting the user to interact with the notebooks. Not unlike ALTA, this course demonstrates a deep appreciation of the breadth of technologies available and how they might be applied ‚Äì and where they might not be applicable or reliable ‚Äì in digital humanities contexts. It often supports the learner by giving multiple examples of how to apply a technique, which incidentally means repeated opportunities to demonstrate data preparation with diverse inputs.\n\nFor the R programming community, Martin Schweinberger‚Äôs [LADAL](https://slcladal.github.io/) shows a thoroughness in developing users‚Äô abilities from statistical basics to a breadth of NLP, corpus and computational linguistic methods. (As part of the ATAP project, LADAL tutorials are undergoing conversion from R markdown to Jupyter notebooks.) Technique-oriented introductory tutorials are then complemented with ‚Äúfocus studies‚Äù on disciplines of linguistic analysis such as learner language, stylistics or linguistic typology.\nIn comparison to the above sites, several other tutorial resources are much more bare, simply demonstrating how to implement a technique with little use of discussion or repeated examples.\nThe varying levels of narrative, and how they present the code, suggests that authors have a mix of goals that we might query:\n- Is the tutorial introducing a technique, or rather its use through code?\n- Is it teaching the application of a technique, or how it works?\n- Does it aim to teach the reader how to read the code? How to experiment with the code? How to compose that code themselves?\n- Does the notebook itself facilitate those engagements with code, or does the more ephemeral teacher?\n\nAlthough not recently updated, we are indebted to Quinn Dombrowski‚Äôs curation of a [list of relevant courses and tutorials](https://github.com/quinnanya/dh-jupyter#course-materials), among other digital humanities notebooks. A recent player in this space is [TAPI (Text Analysis Pedagogy Institute)](https://labs.jstor.org/tapi/), which places open publication of notebook-based courses at the core of their researcher schooling. TAPI incorporates several courses by individual presenters, although there does not currently appear to be a high level of standardisation among the TAPI courses, including in their licensing.\n[The Programming Historian](https://programminghistorian.org/) is also an excellent resource for peer-reviewed digital humanities tutorials, often helpfully grounded in use cases. Not being notebooks (nor R markdown), its tutorials can‚Äôt be directly executed, leaving the user to copy-paste its code or not engage interactively with its content. What sets the Programming Historian apart is its design as a journal of tutorials, i.e. a collaborative enterprise in text analytics pedagogy. While resources we surveyed are all essentially open to third party contributors who may offer amendments, most collections of tutorial notebooks have single or few authors and are not designed as primarily collaborative endeavours.\n\n[Part 2](/posts/notebooks-part-2) of this post looks at the remaining genres we have identified, libraries and tool chests and presentations of research results. Part 2 also includes a discussion of how the information collected here might shape our view of what ATAP can be.\n\n\n\n\nNote 1: Unfortunately, we confined our survey to English-language resources; and with a focus on the Notebook medium, our analysis is biassed to (but not exclusively) the Python programming language.\n\n\n\n\n"},{"title":"Textbooks, tutorials and tool chests: different ways to learn from notebooks - Part 2","slug":"notebooks-part-2","tags":["Jupyter","notebooks","text analysis"],"content":"# Textbooks, tutorials and tool chests: different ways to learn from notebooks - Part 2\n### More genres of notebooks and what it all means for ATAP\n\n### Written by Joel Nothman\n#### This work was supported by the [Sydney Informatics Hub](https://www.sydney.edu.au/research/facilities/sydney-informatics-hub.html), a Core Research Facility of the University of Sydney\n\n[Part 1](/posts/notebooks-part-1) of this post introduced notebooks in general and identified various broad genres of notebook which have been used in providing text analytic tools. That part looked at notebooks as tutorials and the grouping of such materials into courses or textbooks. In this part, we look at notebooks which introduce individual tools and notebooks which present research results. The post ends with a consideration of how our analysis of different genres of notebook might influence the design of ATAP.\n\n### Libraries and tool chests\n\nA ‚Äúcourse‚Äù comes with a sense of order and completeness; a ‚Äútutorial‚Äù with a sense of guidance and demonstration. Some notebook collections appear more designed for reference or application than teaching, as if each notebook was a tool you could take from a chest and utilise for your needs.\n\n<table>\n<tr><td><b>Genre</b></td><td>TOOLKIT</td></tr>\n<tr><td><b>Typical Structure</b></td><td>(Idealised! There are few examples of this done thoroughly.)<ul><li>Pointers to background reading</li>\n<li>How to load data</li>\n<li>How to set parameters</li>\n<li>Apply method</li>\n<li>Validate results</li>\n<li>Questions to consider</li>\n<li>Analyse results</li>\n<li>Save output</li></ul>\n\n</td></tr>\n<tr><td><b>Strengths</b></td><td>Oriented around decision making, critical application, multiple pathways or uses, and iterative improvement of methods.\n\nMeaningful scope for each notebook.\n</td></tr>\n<tr><td><b>Weaknesses</b></td><td>Some processing steps will be optional, notebooks are not very suitable for.</td></tr>\n<tr><td><b>Example</b></td><td><a href='https://nbviewer.org/github/GLAM-Workbench/pm-transcripts/blob/master/harvest_transcripts.ipynb'>GLAM Workbench - Harvest Transcripts</a></td></tr>\n</table>\n\nA notable collection in this space is Tim Sherratt‚Äôs [GLAM Workbench](https://glam-workbench.net/). Tim often titles a directory of notebooks as ‚Äútools, tips and examples‚Äù, which nicely summarises the mix of content that users can retrieve from the Workbench. ‚ÄúTools‚Äù might include a [notebook for acquiring Australian parliament‚Äôs Hansard transcripts](https://nbviewer.org/github/GLAM-Workbench/australian-commonwealth-hansard/blob/master/Harvesting-Commonwealth-Hansard.ipynb), offering parameters that the user might change for their own project; or an [app that summarises query results](https://github.com/GLAM-Workbench/trove-newspapers/blob/master/querypic.ipynb) from the National Library of Australia‚Äôs [Trove](https://trove.nla.gov.au/). An app, here, is distinguished by providing interactive widgets within a notebook, rather than expecting the user to set parameters in code and hit ‚Äúrun‚Äù. ‚ÄúExamples‚Äù includes a [case study](https://nbviewer.org/github/GLAM-Workbench/ozglam-workbench-naa-wap/blob/master/RecordSearch/2.%20Analyse%20a%20series.ipynb) of filtering a corpus by its metadata, while a [walkthrough on language detection](https://github.com/GLAM-Workbench/trove-newspapers/blob/master/find-non-english-newspapers.ipynb) applied to Trove incorporates many ‚Äútips‚Äù which help to identify the author‚Äôs thinking as his code takes each turn. Case studies like these lie on a spectrum between the genericity of tutorials often applied to demonstration or toy data, and the hypothesis-driven specificity of a research article. Grounding a topic in research use cases may help to evoke a sense of relevance to the user.\n\n<table>\n<tr><td><b>Genre</b></td><td>APP</td></tr>\n<tr><td><b>Typical Structure</b></td><td><ul><li>Enter parameters / upload data</li>\n<li>See analysis</li>\n<li>Interact</li>\n</td></tr>\n<tr><td><b>Strengths</b></td><td>User friendly way to analyse text.\n\nNotebooks enable rapid development of tools.\n\n</td></tr>\n<tr><td><b>Weaknesses</b></td><td>Discourages understanding of the internal workings. Comes with similar constraints as desktop tools, including limited reproducibility.</td></tr>\n<tr><td><b>Example</b></td><td><a href='https://github.com/GLAM-Workbench/trove-newspapers/blob/master/querypic.ipynb'>Visualise searches in Trove's newspapers and gazettes</a></td></tr>\n</table>\n\nNotebooks presented as reusable tools may also be very similar to tutorials. Until recently, Ithaka‚Äôs [Constellate.org](https://constellate.org/) ‚Äì a core part of TAPI ‚Äì included tutorial notebooks, and corresponding editions of the same notebook ‚Äúfor research‚Äù. (Here‚Äôs a [tutorial](https://github.com/ithaka/tdm-notebooks/blob/ea3c48e0094303a7d40dec4890c6cfdca275d22b/finding-significant-terms.ipynb) and [research](https://github.com/ithaka/tdm-notebooks/blob/ea3c48e0094303a7d40dec4890c6cfdca275d22b/finding-significant-terms-for-research.ipynb) pair on finding key terms in a corpus.) Each ‚Äúresearch‚Äù notebook omits some of the guidance included in the tutorial, and was rather intended to be applied to a user-supplied dataset. The removal of these applied notebooks notwithstanding, it appears that the Constellate tutorials should dually function as reusable tools. The Constellate platform allows the user to create a corpus from JSTOR content, and select a tutorial/tool notebook from their collection, which is then applied to the new corpus. An important usability feature of reusable tools, and tutorials with this capability, is how they guide or support the user to apply the notebook to their own data.\n\nAlthough targeting experienced corporate data scientists, rather than HASS researchers, data infrastructure provider Databricks openly publishes another relevant tool chest of notebooks (albeit not designed for the Jupyter platform). Their [Solution Accelerators](https://databricks.com/solutions/accelerators) are designed to support a fairly code-savvy analyst in tackling standard data science problems in industry. (Read ‚ÄúSolution Accelerator‚Äù as a less patronising, more businessy, revamp of the term ‚Äústarter kit‚Äù.) A Solution Accelerator notebook mixes narrative and code, inviting the user to bring their own data. It may give them data transformation code, models to apply, questions to ask, and diagnostics to perform. The library of Solution Accelerators shows how even a capable user may appreciate a notebook as a launchpad when working with new techniques. The apparent value of Solution Accelerators to Databricks customers highlights the risk and cost of undertaking analysis without workflow guidance from a notebook.\n\nUnlike tutorials, a notebook tool chest does not make great efforts to build up a user‚Äôs skills from scratch, but supports them in selecting a tool that might be appropriate, seeing its relevance, and applying it to their needs.\n\n### Journals and Anthologies\n\nThrough their mix of code, narrative and generated tables and figures, notebooks are a powerful tool for presenting reproducible experimental results. A research paper implemented as a notebook may be rebuilt from its code, reproducing tables, figures and examples as the researcher modifies it. An article-as-notebook is distinguished by its focus on a specific research question, and application of whichever tools are appropriate to draw reliable conclusions. Although individual humanities researchers using digital methods might publish their research as code notebooks (see Quinn Dombrowski‚Äôs [list](https://github.com/quinnanya/dh-jupyter#research--projects) for instance), institutional or discipline-specific collections of research notebooks are not yet common in applied text analysis and HASS. For comparison, a move towards reproducibility in other sciences allows one to find a multitude of research notebooks in venues like [paperswithcode.com](https://paperswithcode.com/) and [Code Ocean](https://codeocean.com/explore/capsules).\n\n<table>\n<tr><td><b>Genre</b></td><td>RESEARCH EXPERIMENT</td></tr>\n<tr><td><b>Typical Structure</b></td><td><ul><li>Introduction</li>\n<li>Preparation</li>\n<li>Repeat for each tool or step:\n\nModelling, Analysis, Interpretation</li>\n<li>Discussion</li></ul>\n\n</td></tr>\n<tr><td><b>Strengths</b></td><td>Demonstrates application of tools to answer a question.\n\nDemonstrates combination of tools.\n\nEncourages reuse, reproducibility and open science.\n</td></tr>\n<tr><td><b>Weaknesses</b></td><td>May not support the reader to adopt the techniques.</td></tr>\n<tr><td><b>Example</b></td><td><a href='https://journalofdigitalhistory.org/en/article/4yxHGiqXYRbX'>Topic Specific Corpus Building</a></td></tr>\n</table>\n\nWhen driven by a research question, the notebook author is less interested in demonstrating what techniques are *available* to apply, and more selective in using techniques that may helpfully harness their data towards a specific goal or narrative. The research thesis thus provides a way to specifically evaluate the output of an analysis: does it support a hypothesis? does it induce a new hypothesis? does it suggest problems in the modelling? The final research notebook may hide aspects of this iterative experimentation, evaluation, and the response of further investigation, issue mitigation, or discarding a path of investigation. (The ideal reproducible research notebook would keep track of dead ends too!)\nTutorials and tools may emulate these processes of iterative development, or provide a range of possible pathways for analysis; the research notebook, however, must keep its focus on the thesis being explored.\n\nThe [Journal of Digital History](https://journalofdigitalhistory.org/) is a new player in this space, releasing its first issue in October 2021. This project of the Luxembourg Centre for Contemporary and Digital History and De Gruyter ensures some reproducibility by requiring the code which constructs the research article to be executable and its data available to reproduce tables and figures, while allowing the reader to evaluate and reuse the research implementation. It goes further in harnessing the notebook medium to add layers of depth to peer-reviewed publications in digital history: viewing a JDH publication, a reader is able to hide what they call the ‚Äúhermeneutic layer‚Äù of the work, which provides methodological detail, as distinct from the narrative arc of the research. The notebook medium applied in this way is able to shift several paradigms about how research is constructed, presented, and accessed.\n\n### Bringing it back to users\n\nIn positioning the Australian Text Analytics Platform‚Äôs role amidst ongoing and past resource creation, we have to consider how our library of notebooks might build outcomes for our users. Here are some outcomes we are considering.\n\n#### Reference and Application\n- User has access to starters‚Äô kits for a wide range of techniques, covering data modelling, validation and analysis, archiving, etc.\n- User can experiment with standard tools applied to demo data or their own.\n- User can adapt code notebooks to their own use cases.\n- User is able to compare alternative methods for similar goals.\n\n#### Growth and Learning\n- User is familiar with what selected research methods look like as code.\n- User is familiar with what end-to-end research looks like as code.\n- User can prepare their own data for existing or standard tools.\n- User understands how their research problem might translate to a computational problem, and the applicable techniques.\n- User is able to combine multiple techniques to achieve their research goals.\n- User understands how to build good research software.\n- User can confidently build a notebook from scratch for their own research.\n\nWe also need to define who our user personas are: is it someone with academic expertise, but little in code? or someone with moderate code literacy, but stuck with how to create notebooks for reproducible research? The latter might be more interested in published starter kits, case studies and experiments, while the former needs the guidance provided by a tutorial.\n\n### Where to from here?\n\nBy surveying several collections of notebooks for text analytics, we have a stronger understanding of the spectrum of materials ATAP‚Äôs library can incorporate, both in content and in genre.\n\nOne apparent opportunity is to curate or improve a catalogue of existing resources that will meet the needs of the Australian text analytics research community. It‚Äôs also possible to see areas where existing resources may fail to meet the needs of current users of desktop and web tools for corpus analysis from [Voyant Tools](https://voyant-tools.org/) to [AntConc](https://www.laurenceanthony.net/software/antconc/); those tools provide interactivity that carries the user across views of statistics, query matches and texts, but tend to be limited in facilitated reproducible research and in extensibility to arbitrary corpus cleaning and analysis procedures. Tying the resources we‚Äôve looked at to user outcomes also gives a better understanding how to evaluate accessibility of a notebook and the learning opportunities arising from it, such as to what extent a resource:\n- supports the user getting their own data into the notebook.\n- demonstrates the combination of multiple techniques into research.\n- helps a researcher choose techniques relevant to their research question.\n- supports the user to experiment with the code of the notebook\n- demonstrates the processes and challenges around using/applying a technique and the code needed to implement those decisions e.g. decisions to make before and after applying topic models. \n\nOverall, we‚Äôve talked little about how notebooks present their content, and how that presentation might affect user engagement and the sustainability of the resource. Of course, those concerns are also important in looking at principles for quality in notebook resources for digital humanities and other text analytics users.\n\n"}],"pages":[{"title":"Contact us","slug":"contact","content":"\n### News\n\n#### Launch of ATAP\n\nATAP was officially launched on November 1 2022 at the [ResBaz Queensland](https://resbaz.github.io/resbaz2022qld/) festival. The event brought together our partners ([UQ](https://www.uq.edu.au/), [University of Sydney](https://www.sydney.edu.au/) and [AARNet](https://www.aarnet.edu.au/)) and our co-investor ([ARDC](https://ardc.edu.au/)). You can read more about the launch [here](https://ardc.edu.au/article/australian-text-analytics-platform-launches/), and the video package which was shown will be available on our [Vimeo channel](https://vimeo.com/user172843579) soon.\n\n#### Graduate Digital Research Fellowships\n\nAt the ATAP launch, we also announced a Graduate Digital Research Fellowship program to run in the first part of 2023. If you are a confirmed research student (in SE Queensland) who would like deeper knowledge of how to use digital methods and tools in your research area, GDRF could give you the opportunity to hone your digital skills and to enhance your current research (or to work on an independent digital project).\n\n[More information](/fellowships)\n\n### Contact us\n\nYou can contact the Australian Text Analytics Platform by [email](mailto:info@atap.edu.au)\n\nWe share a [Twitter account](https://twitter.com/LDaCA_Program) with the [Language Data Commons of Australia](https://www.ldaca.edu.au):<br>\n"},{"title":"Preparing Text Data","slug":"data_prep","content":"\n### Introducing data preparation concepts \n\n<figure>\n  <img\n    src=\"data-prep-approach-time.png\" \n    alt=\"What data scientists spend the most time doing - pie chart\"  \n  />\n  <figcaption align = \"center\"><b>What data scientists spend the most time doing - pie chart</b></figcaption>\n</figure>\n \nThis graphic is, sadly, all too true. Data scientists and those who are using \ndata as part of their research spend much of their time preparing their \ndataset and transforming its structure into a format that can be used (often \nreferred to as [data wrangling](https://online.hbs.edu/blog/post/data-wrangling)\nor data munging). The Australian Text Analytics Platform will offer a range \nof tools to assist in cleaning text data and performing other preliminary \noperations which can prepare the data for analysis. ATAP analysis notebooks \nassume a common data structure, however the platform will provide notebooks \ncontaining code for transforming data into the structure that is needed for \nthe procedure(s) in the analysis notebooks.\n\nThere are two main processes that are needed to prepare text data for analysis\n, [cleaning](#common-cleaning-techniques) and [annotation](#annotation), and \nthe ones that you will need to use will depend on the dataset that you are \nusing. \n\n### Common Cleaning Techniques\n- **Making all of the text lower case:**\n  This ensures that e.g. *dog* and *Dog* will not be treated as different items\n  and is important if you are going to use analytic methods which rely on \n  counting items. However, if you are planning to extract entities from your \n  text data, retaining capital letters may be important.\n\n- **Standardising spelling:**\n  At least for English text, there are some well-known spelling variations, \n  some with a geographical context (*colour/color*) and some that are more a \n  matter of personal preference (*recognise/recognize*). As with case, \n  standardising spelling ensures that pairs like the examples are treated as \n  tokens of the same type.\n\n- **Removing stopwords:**\n  Stopwords are function words that are not interesting for many analyses and \n  we can safely remove them from our data using a stoplist. The 20 most \n  frequently occurring words in the \n  [British National Corpus](https://www.english-corpora.org/bnc/) are *the, of\n  , and, a, in, to, it, is, was, to, I, for, you, he, be, with, on, that, by\n  *, and *at*. The equivalent list for the Corpus of Contemporary American \n  English ([COCA](https://www.english-corpora.org/coca/)) is *the, be, and, of\n  , a, in, to, have, to, it, I, that, for, you, he, with, on, do, say* and *\n  this*. \n  Some of the differences between the two are because the COCA counts are of \n  lemmas (see Lemmatization below) which are the base forms of a word (think *\n  dog* and *dogs*). Packages such as [nltk](https://www.nltk.org/) and \n  [spaCy  ](https://spacy.io/) include standard stoplists for various \n  languages, and it is possible to specify other words to be excluded.\n\n- **Removing punctuation:**\n  Punctuation can change how a text analysis package identifies a word. For \n  instance to be sure that *dog* and *dog?* are not treated as different items,\n  removing punctuation is good practice.\n\n- **Removing numbers:** \n  Sometimes the presence of numbers in documents can lead to artefacts in \n  analysis. For example, in a collection of documents with page numbering, \n  the numbers might show up as collocates of words or as part of a topic in a \n  topic model. To avoid this, removing numbers is also good practice. This \n  can present a challenge where numbers might be of interest (e.g. a study of \n  mathematics textbooks).\n\n- **Removing whitespace:** \n  Whitespace can be another possible source of artefacts in analysis, \n  especially if the source material uses a lot of tabs.\n\n### Annotation\nAnnotation is the process of adding information to your base dataset in order \nto make it possible to apply analytic techniques. In some cases, this may be \na manual process. For example, much of the annotation which is described in \nthe Text Encoding Initiative [Guidelines](https://tei-c.org/guidelines/) \nrequires a human making decisions although, in some cases, manual annotation \nprocesses may also be scaled up to large text corpora using text \nclassification or information extraction technologies. \n\nHowever some annotation can be carried out automatically, and there are two \nimportant kinds of annotation for text which fall into this category.\n\n\n- **Part-of-speech tagging (POS-tagging):** \n  For some analytic procedures, knowing the part of speech (or class of words) \n  that an item belongs to is important. For languages where good pre-trained \n  models exist, this annotation can be carried out automatically to a high \n  level of accuracy ‚Äì for English, we expect an accuracy rate better than 95%. \n  POS-taggers generally provide more information than just whether an item is a \n  noun or a verb, they also distinguish singular and plural forms of nouns and \n  tell us whether a verb‚Äôs form is present tense form or a past tense. The tag \n  sets which are used can therefore be quite extensive, and there are various \n  tag sets in use such as the \n  [Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n  tags and the [CLAWS](https://ucrel.lancs.ac.uk/claws5tags.html) tags used by \n  the British National Corpus.\n\n[LADAL](https://slcladal.github.io/) has some excellent resources that \ndiscuss POS tagging in more detail.\n\n- **Lemmatization:** \n  The distinctions between different forms of a single lexeme can be a \n  hindrance in analysis especially if we are interested in lexical semantics in \n  texts. Lemmatization identifies the base forms of words (or lemmas) in a text \n  so that all forms of an item are treated together. For example: *dog* and *dogs* \n  will both be instances of the lemma DOG. *eat, eats, eating* and *ate* \n  will all be treated as tokens of the lemma EAT. \n  As noted above, POS-tags give information about the form of words and are \n  generally part of the annotation in lemmatization. A lemma, along with a POS-\n  tag, can be reconstructed to the original form if \n  necessary.\n"},{"title":"Events","slug":"events","content":"\n# Events\n\n[Webinars](#webinars) &emsp;&emsp;\n[Forthcoming workshops](#forthcoming-workshops) &emsp;&emsp;\n[Previous workshops](/workshops) &emsp;&emsp;\n[Office Hours](#office-hours)\n\n### Our Launch!\n\nWe are holding a launch event as part of [ResBazQld](https://resbaz.github.io/resbaz2022qld/). We will also be announcing our digital research fellowships for graduate students to commence in 2023.\n\n**When: Tuesday November 1 from 5:00 PM**<br>\n**Where: Griffith University, Building N53, Level Minus 1**\n\nThere will be pizza and soft drinks!\n\nFurther information: [Dr Simon Musgrave](mailto:s.musgrave@uq.edu.au)\n\n### Webinars {#webinars}\n\nOur webinar series is a joint initiative with the Language Technology and Data Analysis Laboratory ([LADAL](https://slcladal.github.io/index.html)), (School of Languages and Cultures, University of Queensland). LADAL sponsored [webinars](https://slcladal.github.io/webinars2022.html) take place in the alternate months.\n\nAll webinars take place at 8:00PM Brisbane time which is UTC+10. Zoom links will be available one week prior to the event.\n\n#### October 3 2022 - Pawe≈Ç Kamocki: European Union Data Protection initiatives and their consequences for research\n\n**Abstract**:<br>\nThe European Union, with its large population and GDP, is a leading force in regulatory globalisation. This webinar will discuss recent developments in legal frameworks affecting research data in Europe. Apart from the General Data Protection Regulation which, since its entry into application in 2018, has become an international standard of personal data protection, the recent introduction of statutory copyright exceptions for Text and Data Mining will also be discussed. Moreover, the webinar will also include a presentation of the most recent changes in EU law, such as the Data Governance Act and the Artificial Intelligence Act, which are expected to enter into application in the coming years.\n\n**Pawe≈Ç Kamocki** is a legal expert in Leibniz-Institut f√ºr Deutsche Sprache, Mannheim. He studied linguistics and law, and in 2017 obtained his doctorate in law from the universities of Paris and M√ºnster for a thesis on legal aspects of data-intensive university research, with a focus on Knowledge Commons. He worked as a research and teaching assistant at the Paris Descartes university (now: Universit√© de Paris), then also in the private sector. He is certified to work as an attorney in France. An active member of the [CLARIN](https://www.clarin.eu/) community since 2012, he currently chairs the CLARIN Legal and Ethical Issues Committee. He also worked with other projects and initiatives in the field of research data policy (RDA, EUDAT) and co-created several LegalTech tools for researchers. One of his main research interests are legal issues in Machine Translation.\n\n[Zoom link](https://uqz.zoom.us/j/82090438697?from=addon)\n\n#### August 1 2022 - V√°clav Cvrƒçek: The Czech national Corpus\n\n[V√°clav Cvrƒçek](https://ucnk.ff.cuni.cz/en/institute/people/vaclav-cvrcek-2/) is a linguist who deals with the description of the Czech language, especially with the use of large electronic corpora and quantitative methods. In 2013-2016 he worked as the director of the [Czech National Corpus](https://ucnk.ff.cuni.cz/en/) project, since 2016 he has been the deputy director. Recently, he has been focusing on research on textual variability and corpus-based discourse analysis with a focus on online media.\n\n#### June 6 2022 - Barbara McGillivray: The _Journal of Open Humanities Data_\n\nBarbara McGillivray is a Turing Research Fellow at [The Alan Turing Institute](https://www.turing.ac.uk/), and Editor in Chief of the [Journal of Open Humanities Data](https://openhumanitiesdata.metajnl.com/). Since September 2021 she is also a lecturer in Digital Humanities and Cultural Computation at the [Department of Digital Humanities of King's College London](https://www.kcl.ac.uk/ddh). Before joining the Turing, she was language technologist in the Dictionary division of Oxford University Press and data scientist in the Open Research Group of Springer Nature. Her research at the Turing is on how words change meaning over time and how to model this change in computational ways. She works on machine-learning models for the change in meaning of words in historical times (Ancient Greek, Latin, eighteen-century English) and in contemporary texts (Twitter, web archives, emoji). Her interdisciplinary contribution covers Data Science, Natural Language Processing, Historical Linguistics and other humanistic fields, to push the boundaries of what academic disciplines separately have achieved so far on this topic.\n\n#### 4 April 2022 - Keoni Mahelona: A practical approach to Indigenous data sovereignty\n\nKeoni Mahelona is the Chief Technical Officer of [Te Hiku Media](https://tehiku.nz/) where he is a part of the team developing the Kaitiakitanga Licence. This licence seeks to balance the importance of publicly accessible data with the reality that indigenous peoples may not have access to the resources that enable them to benefit from public data. By simply opening access to data and knowledge, indigenous people could be further colonised and taken advantage of in a digital, modern world. Therefore Keoni is committed to devising data governance regimes which enable Indigenous people to reclaim and maintain sovereignty over indigenous data.\n\n### Forthcoming workshops {#forthcoming-workshops}\n\n#### Pre-conference workshop (before the 2022 Conference of the Australian Linguistic Society)\n\nThe Australian Text Analytics Platform and the [Language Data Commons of Australia](https://www.ldaca.edu.au/) will present a day of workshop activities to give ALS conference delegates (and anyone else who is interested) the opportunity to learn more about the work of the two projects. The day will include:\n\n- an overview of the projects and the work to date\n- reports on specific sub-projects\n- introductory workshops on the tools and resources being developed\n- a workshop on using Discursis, a tool for tracking topics in interactive use of language\n- the opportunity to influence future work by exploring and providing feedback on the data interface which we are building.\n\nThe activities will be of interest to anyone who conducts research which includes language data, especially those who use or would like to use computational tools in their research. Participants should bring their own computer; no software is required beyond a web browser.\n\nCatering is provided and therefore a registration fee is being charged for this event to partially cover the cost ($25 full rate and $15 discount).\n\n[Further details](https://www.ldaca.edu.au/pre-als-activities) (including full program)\n\n[Registration](https://als.asn.au/Conference/Conference2022/Workshops)\n\n### Office Hours {#office-hours}\n\nWe invite Australian researchers working with linguistics, text analytics, digital and computational methods, social media and web archives, and much more to attend our regular online office hours, jointly hosted with the [Digital Observatory](https://research.qut.edu.au/digitalobservatory/). Bring your technical questions, research problems and rough ideas and get advice and feedback from the combined expertise of our ARDC research infrastructure projects. No question is too small, and even if we don‚Äôt know the answer we are likely to be able to point you to someone who does.\n\nThese sessions run over Zoom from 2-3pm (Australia/Sydney time) every second Tuesday - [details](https://research.qut.edu.au/digitalobservatory/office-hours/).\n"},{"title":"Graduate Digital Research Fellowship Program 2023","slug":"fellowships","content":"\n### Graduate Digital Research Fellowship Program 2023\n\nThe **Graduate Digital Research Fellowship Program** is designed to prepare research students for academic or non-academic careers in digital scholarship. Fellows are confirmed research students who will spend 12-15 weeks honing their digital skills to enhance their current research/thesis topic or to work on an independent digital project.\n\nThey will work with local and international digital research practitioners to create a scholarly work that uses digital research methods (digital publication, software, data sets etc.) The fellowship program encourages (but is not limited to) applications in the following areas:\n\n- Computational analysis of text\n- Social media analytics\n- Spatio-temporal mapping\n- Machine learning applications\n- Indigenous perspectives on digital research\n\nFellows will meet regularly as a group for activities such as seminars, reading groups and training workshops, and will have access to digital research support staff who can advise and collaborate on their digital projects. Fellows may also participate in other activities, such as undergraduate student project supervision and independent study, to ensure adequate progress of their project and to develop a sound understanding of digital research methods and tools.\n\nExamples of areas in which fellows might locate their work:\n\n- **Data analysis**: analyse quantitative or qualitative data using computational methods. The fellow will go through all stages in the workflow: data gathering, preparation, analysis and presentation.\n- **Visualisation of research data**: the fellow will investigate visual design opportunities and constraints to represent and interact with data sets, thereby finding new ways to explore a research topic.\n- **Serving communities**: the fellow will develop new modes of scholarly communication based on digital tools to engage local communities and the public, such as storytelling, the provision of relevant practical information or tools that support decision-making.\n\nBy the end of the fellowship, fellows will have a comprehensive understanding of current practices in digital research as well as deeper knowledge of how to use digital methods and tools in their research area. A symposium will be organised toward the end of the fellowship to showcase the fellows' digital projects.\n\nPlease note that the fellowship does not provide financial support. However, fellows may be eligible for the [Career Development Scholarship Extension](https://cdf.graduate-school.uq.edu.au/uq-career-development-scholarship-guidelines), check the guidelines for details.\n\nExpressions of interest are sought from current HDR students who are confirmed, or will be prior to December 2022.\n\n**Expressions of interest close: 9 December 2022.**\n\n**Applicants notified of outcome: 16 December 2022.**\n\n**Fellowship period: February - May 2023.**\n\nDownload the expression of interest form [here](/GraduateDigitalResearchFellowship-EoI-2022.docx) (DOCX, 40.8 KB). Once complete, please email it along with your current CV to s.musgrave@uq.edu.au.\n\nTo discuss potential independent projects or if you have any further questions, please contact [Dr Simon Musgrave](https://auslanguage.net/simon-musgrave/) at s.musgrave@uq.edu.au.\n"},{"slug":"home","content":"\n__The Australian Text Analytics Platform is an open source environment \nthat provides researchers with tools and training for analysing, processing,\nand exploring text.__\n\nText analytics is a suite of methods which enable data-driven research \nby extracting and analysing machine-readable information from within \nunstructured text. Due to the increasing availability of large amounts of\nunstructured text, such techniques are becoming more and more important across\ndiverse research disciplines.\n\nText analysis tends to happen at either a basic, generic level (handled with \nexisting software packages) or with custom code specifically developed by \nprogrammers for a particular project. ATAP will support researchers \ntransitioning to code-based text analysis, with the resultant benefits of \nflexibility, reproducibility and reuse, and the possibility of exporting their \nresults and workflows as a fully documented research object.\n\nATAP will be a collaborative, cloud-based workbench environment, bringing \ntogether users and providers of data and text analytics tools. It will \nencourage researchers to adopt new methods, leading to greater flexibility and \ntransparency in research workflows. The platform will be accessible to \nresearchers with a broad range of experience and skills (including beginners) \nand across a range of disciplines. Support provided by the platform will \ninclude hands-on workshops, online training modules and online office hours, \nas well as advice and collaboration in selected partnerships.\n\n![ARDC logos](/AcknowledgeARDC.png)\n\nThe Australian Text Analytics Platform (ATAP) projects \n[received investment](https://doi.org/10.47486/PL074) from the Australian \nResearch Data Commons (ARDC). The ARDC is funded by the National Collaborative \nResearch Infrastructure Strategy (NCRIS).\n\nATAP acknowledges and pays respects to the Elders and Traditional Owners of the \nlands on which we live and work.\n"},{"title":"Useful Methods","slug":"methods","content":"\n\n[Counting words](#counting-words) &emsp;&emsp; \n[More complex methods - Classification](#classification) &emsp;&emsp; \n[More complex methods - Others](#others) &emsp;&emsp; \n[Visualisation](#visualisation)\n\n### Introduction\nThroughout this page, we have given links to further information in Wikipedia and in the tutorials provided by the Language Technology and Data Analysis Laboratory [LADAL](https://slcladal.github.io/) at the University of Queensland. We also have given references to published research using the methods we discuss.\n\nLADAL has an overview of [text analysis and distant reading](https://slcladal.github.io/textanalysis.html). \n\n\n\n### Counting Words {#counting-words}\n\n#### Word frequency\n\nKnowing how frequently words occur in a text can already give us information about that text and frequency lists based on large corpora are a useful tool in themselves - you can [download](https://kilgarriff.co.uk/bnc-readme.html) such lists for the (original) [British National Corpus](http://www.natcorp.ox.ac.uk/).\n\nTracking changes in the frequency of use of words across time has become popular since Google‚Äôs [n-gram viewer](https://books.google.com/ngrams) has been available. However, results from this tool have to treated with caution for reasons set out in this [blog-post](https://broadstreet.blog/2021/08/11/bad-ngrams/). \n\nComparing patterns of word frequency across texts can be part of authorship attribution. Patrick Juola [describes using this method](https://languagelog.ldc.upenn.edu/nll/?p=5315) when he tried to decide whether Robert Galbraith was really J.K Rowling.\n\n\nThis paper uses frequency and concordance analysis, with Australian data:\n\n<div class=\"reference\"><font size=\"3\">Bednarek, Monika. 2020. Invisible or high-risk: Computer-assisted discourse analysis of references to Aboriginal and Torres Strait Islander people(s) and issues in a newspaper corpus about diabetes. <i>PLoS ONE</i> 15/6: e0234486. <a href=\"https://doi.org/10.1371/journal.pone.0234486\" target=\"_blank\">https://doi.org/10.1371/journal.pone.0234486</a> </font></div>\n\n\nThe ratio of types and tokens in a text has been used as a measure of lexical diversity in developmental and clinical studies as well as in stylistics. It has also been applied to theoretical problems in linguistics:\n\n<div class=\"reference\"><font size=\"3\">Kettunen, Kimmo. 2014. Can Type-Token Ratio be Used to Show Morphological Complexity of Languages? <i>Journal of Quantitative Linguistics</i> 21(3). 223‚Äì245. <a href=\"https://doi.org/10.1080/09296174.2014.911506\" target=\"_blank\">https://doi.org/10.1080/09296174.2014.911506</a>.</font></div>\n\n\n#### Concordance\nA concordance allows the researcher to see all instances of a word or phrase in a text, neatly aligned in a column and with preceding and following context (see image below).\nConcordances are often a first step in analysis. The concordance allows a researcher to see how a word is used and in what contexts. Most concordancing tools allow sorting of results by either preceding or following words ‚Äì the coloured text in the example below shows that in this case the results have been sorted hierarchically on the three following words. This possibility can help in discovering patterns of co-occurrence. Concordances are also very useful when looking for good examples to illustrate a point. (The type of display seen in the example is often referred to as KeyWord In Context ‚Äì KWIC. There is a possibility of confusion here, as there is a separate analytic method commonly referred to as [Keywords](#keywords).)\n\n![Example of a concordance](/concordance.png)\n\n(The example here was produced by [Antconc](https://www.laurenceanthony.net/software/antconc/)) \n\nThis [tutorial](https://slcladal.github.io/kwics.html) from LADAL on concordancing uses a notebook containing R code as a method of extracting concordance data.\n\n\n\n\n#### Clusters and collocations\nTwo methods can be used for counting the co-occurrence of items in text. \nClusters (sometimes known as n-grams) are sequences of adjacent items. A bigram is a sequence of two items, a trigram (3-gram) is a sequence of three items and so on. n-grams are types made up of more than one item, and therefore we can count the number of tokens of each n-gram in texts. n-grams are also the basis for a class of language models. (Google created a very large data set of English n-grams in developing their language-based algorithms and this [data is available](https://storage.googleapis.com/books/ngrams/books/datasetsv3.html).)\nCollocations are patterns of co-occurrence in which the items are not necessarily adjacent. An example of why this is important is verbs and their objects in English. The object of a verb is a noun phrase and in many cases the first item in an English noun phrase is a determiner. This means that for many transitive verbs, the bigram *verb the* will occur quite frequently. But it is much more interesting to know whether there are patterns relating verbs and the entities which are their objects. \nCollocation analysis uncovers such patterns by looking at co-occurrences within a window of a certain size, for example three tokens on either side of the target. Collocation analysis gives information about the frequency of the co-occurrence of words and also a statistical measure of how likely that frequency is, given the overall frequencies of the terms in the corpus. Measures commonly applied include [Mutual Information](https://en.wikipedia.org/wiki/Mutual_information) scores and [Log-Likelihood](https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood) scores.\nCollocations can also tell us about the meanings of words. If a word has collocates which fall into semantically distinct groups, this can indicate ambiguity or polysemy. And if different words share patterns of collocation, this can be evidence that the words are at least partial synonyms. \n\nThis graphic shows collocation relations in Darwin‚Äôs Origin of Species visualised as a network - the likelihood of a pair of words occurring in close proximity in the text is indicated by the weight of the line linking them:\n\n![Collocation patterns in *Origin of Species* as a network](/collocation_network.png)\n\nThis article uses bigram frequencies as part of an analysis of language change:\n\n<div class=\"reference\"><font size=\"3\">Schweinberger, Martin. 2021. Ongoing change in the Australian English amplifier system. <i>Australian Journal of Linguistics</i> 41(2). 166‚Äì194. <a href=\"https://doi.org/10.1080/07268602.2021.1931028\" target=\"_blank\">https://doi.org/10.1080/07268602.2021.1931028</a>.</font></div>\nAn article which uses concordances and collocation analysis:\n<div class=\"reference\"><font size=\"3\">Baker, Paul & Tony McEnery. 2005. A corpus-based approach to discourses of refugees and asylum seekers in UN and newspaper texts. <i>Journal of Language and Politics</i> 4(2). 197‚Äì226.</font></div>\n\nThis research uses the discovery of shared patterns of collocation as evidence that the words are at least partial synonyms:\n\n<div class=\"reference\"><font size=\"3\">McEnery, Tony & Helen Baker. 2017. <i>Corpus linguistics and 17th-century prostitution: computational linguistics and history</i> (Corpus and Discourse. Research in Corpus and Discourse). London; New York, NY: Bloomsbury Academic. (especially chapters 4 and 5)</font></div>\n\nThis [tutorial](https://slcladal.github.io/coll.html) from LADAL on analysing co-occurences and collocations uses a notebook containing R code as a method to extract and visualise semantic links between words.\n\n\n\n#### Keywords\nKeyword analysis is a statistically robust method of comparing frequencies of words in corpora. It tells us which words are more frequent (or less frequent) than would be expected in one text compared to another text and gives an estimate of the probability of the result. Keyword analysis uses two corpora: a **target** corpus, which is the material of interest, and a **comparison** corpus. Frequency lists are made for each corpus and then frequency of individual types in each corpus are compared. Keywords are ones which occur more ( or less) frequently in the target corpus than expected given the reference corpus. \nThe **keyness** of individual items is a quantitative measure of how unexpected the frequency is; chi-square is a one possible measure of this, but a log-likelihood measure is more commonly used. Positive keywords are words which occur more commonly than expected; negative keywords are words which occur less commonly than expected.\n\nThis visualisation shows a comparison of positive distinguishing words for three texts (Charles Darwin‚Äôs *Origin*, Herman Melville‚Äôs *Moby Dick*, and George Orwell‚Äôs *1984*), words that occur more commonly than we expect in one text when taking the other two texts as a comparison: \n\n![Keywords from three texts](/keywords.png)\n\nThis paper applies keyword analysis to Australian text data sourced from a television series script:\n\n<div class=\"reference\"><font size=\"3\">Bednarek, Monika. 2020. Keyword analysis and the indexing of Aboriginal and Torres Strait Islander identity: A corpus linguistic analysis of the Australian Indigenous TV drama Redfern Now. <i>International Journal of Corpus Linguistics</i> 25/4: 369-99. <a href=\"http://doi.org/10.1075/ijcl.00031.bed\" target=\"_blank\">http://doi.org/10.1075/ijcl.00031.bed</a></font></div>\n\nTony McEnery describes using the keyword analysis method to compare four varieties of English in this chapter:\n\n<div class=\"reference\"><font size=\"3\">McEnery, Tony. 2016. Keywords. In Paul Baker & Jesse Egbert (eds.), *Triangulating methodological approaches in corpus-linguistic research* (Routledge Advances in Corpus Linguistics 17), 20‚Äì32. New York: Routledge.</font></div>\n\nThis article explores how to assess Shakespeare‚Äôs use of words to build characters by using keyword analysis of the characters' dialog:\n\n<div class=\"reference\"><font size=\"3\">Culpeper, Jonathan. 2002. Computers, language and characterisation: An analysis of six characters in Romeo and Juliet. In <i>Conversation in Life and in Literature: Papers from the ASLA Symposium</i> (Association Suedoise de Linguistique Appliquee (ASLA) 15), 11‚Äì30. Uppsala: Universitetstryckeriet. (<a href=\"https://lexically.net/wordsmith/corpus_linguistics_links/Keywords-Culpeper.pdf\" target=\"_blank\">pdf</a>)</font></div>\n\n\n\n### More complex methods ‚Äì Classification {#classification}\n\nClassification methods aim to assign some unit of analysis, such as a word or a document, to a class. For example, a document (or a portion of a document) can be classified as having positive or negative sentiment. These methods are all examples of supervised machine learning. An algorithm is trained on the basis of annotated data to identify classifiers in the data ‚Äì features which correlate in some way with the annotated classifications. If the algorithm achieves good results on testing data (classified by human judgment), then it can be used to classify unannotated data.\n\n\n\n#### Document Classification\nThe task here is to assign documents to categories automatically. An everyday example of this procedure is spam filtering of email that may be applied by internet service providers and also within email applications. An example of this technique being used in research would be automatically identifying historical court records as referring either to violent crimes, property offences, or other crimes. \n\nThe following two articles taken together give an account of a technique for partially automating the training phase of classification and then of how the classifiers allowed researchers to access new information in a large and complex text.\n\n<div class=\"reference\"><font size=\"3\">Leavy, Susan, Mark T Keane & Emilie Pine. 2019. Patterns in language: Text analysis of government reports on the Irish industrial school system with word embedding. <i>Digital Scholarship in the Humanities</i> 34(Supplement_1). i110‚Äìi122. <a href=\"https://doi.org/10.1093/llc/fqz012\" target=\"_blank\">https://doi.org/10.1093/llc/fqz012</a>.</font></div>\n\n<div class=\"reference\"><font size=\"3\">Pine, Emilie, Susan Leavy & Mark T. Keane. 2017. Re-reading the Ryan Report: Witnessing via Close and Distant Reading. <i>√âire-Ireland</i> 52(1‚Äì2). 198‚Äì215. https://doi.org/10.1353/eir.2017.0009. (<a href=\"https://researchrepository.ucd.ie/handle/10197/10287\" target=\"_blank\">available online</a>)</font></div>\n\n[Wikipedia](https://en.wikipedia.org/wiki/Document_classification)\n\n\n\n#### Sentiment analysis\nSentiment analysis assigns documents according to the affect which they express. In simple cases, this can mean sorting documents into those which a express a positive view and those which express a negative view (with a neutral position sometimes also included). Such classifications are the basis for aggregated ratings - for example, online listings of movies and restaurants. A sentiment value is assigned to individual reviews then an aggregate score is calculated based on those values and that aggregate score is the rating presented to the user. More sophisticated sentiment analysis can assign values on a scale. Some sentiment analysis tools use dictionaries of terms with sentiment values assigned to those terms; these are known as pre-trained or pre-determined classifiers.\n\nThe following figure shows the results of the sentiment analysis of four texts (*The Adventures of Huckleberry Finn* by Mark Twain, *1984* by George Orwell, *The Colour out of Space* by H.P.Lovecraft, and *On the Origin of Species* by Charles Darwin) using the Word-Emotion Association Lexicon (Mohammad and Turney 2013). The graphic shows what percentage of each text can be assigned to each of eight categories of sentiment:\n \n![Sentiment analysis of four texts](/sentiment_analysis.png)\n\nThe Wikipedia entry for [Sentiment Analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) gives more information and examples particularly in relation to the use of sentiment analysis as a tool used in online settings. \n\nLADAL‚Äôs [Sentiment Analysis tutorial](https://slcladal.github.io/sentiment.html) uses a notebook containing R code as a method of performing sentiment analysis.\n\nThis article discusses problems in assembling training data for complex sentiment analysis tasks and then applies the results to oral history interviews with Holocaust survivors:\n<div class=\"reference\"><font size=\"3\">Blanke, Tobias, Michael Bryant & Mark Hedges. 2020. Understanding memories of the Holocaust‚ÄîA new approach to neural networks in the digital humanities. <i>Digital Scholarship in the Humanities</i> 35(1). 17‚Äì33. <a href=\"https://doi.org/10.1093/llc/fqy082\" target=\"_blank\">https://doi.org/10.1093/llc/fqy082</a>.</font></div>\n\n\n#### Named Entity Recognition\nNamed Entity Recognition involves two levels of classification. First, segments of text are classified as either denoting or not denoting an entity: for example, a person, a place or an organization. The identified entities can then be classified as belonging to one of the types of entity.\n\nThe Wikipedia entry explaining [named-entity recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) gives further detail about the technique.\n\nThis article looks at the problems encountered when applying a well-known entity recognition package ([Stanford](https://nlp.stanford.edu/software/CRF-NER.html)) to historical newspapers in the National Library of Australia‚Äôs Trove collection:\n<div class=\"reference\"><font size=\"3\">Mac Kim, Sunghwan & Steve Cassidy. 2015. Finding names in Trove: Named Entity Recognition for Australian historical newspapers. In <i>Proceedings of the Australasian Language Technology Association Workshop 2015</i>, 57‚Äì65. (<a href=\"https://aclanthology.org/U15-1007.pdf\" target=\"_blank\">pdf</a>)</font></div>\n\nThis article (section 6.3) discusses why entity recognition is not as useful as might be expected when studying names in novels:\n\n<div class=\"reference\"><font size=\"3\">Dalen-Oskam, K. van. 2013. Names in novels: An experiment in computational stylistics. <i>Literary and Linguistic Computing</i> 28(2). 359‚Äì370. <a href=\"https://doi.org/10.1093/llc/fqs007\" target=\"_blank\">https://doi.org/10.1093/llc/fqs007</a>.</font></div>\n\n\n\n#### Computational Stylistics (Stylometry) \nThis method is also referred to as authorship attribution as the classification task is to assess patterns of language use in order to decide whether to attribute a piece of text to a particular author (and with what degree of confidence). Seemingly simple classifiers are used for this task as they are assumed to be less open to conscious manipulation by writers. For example, comparative patterns of occurrence of function words such as *the* and *a/an* are considered a better classifier than occurrences of content words. Character n-grams, that is sequences of characters of a specified length, have also proven to be a good classifier for use in this task. A recent example of these techniques being applied in a case which received a good deal of public attention was the controversy about whether [Robert Galbraith was really J.K Rowling](https://languagelog.ldc.upenn.edu/nll/?p=5315).\n\nThe Wikipedia entry on [stylometry](https://en.wikipedia.org/wiki/Stylometry) gives further information on the methodology. \n\n\n\nThis article applies stylometric techniques to a classic of Chinese literature:\n<div class=\"reference\"><font size=\"3\">Zhu, Haoran, Lei Lei & Hugh Craig. 2021. Prose, Verse and Authorship in Dream of the Red Chamber: A Stylometric Analysis. <i>Journal of Quantitative Linguistics</i> 28(4). 289‚Äì305. <a href=\"https://doi.org/10.1080/09296174.2020.1724677\" target=\"_blank\">https://doi.org/10.1080/09296174.2020.1724677</a>.</font></div>\nAn overview of the use of function words in stylometry:\n\n<div class=\"reference\"><font size=\"3\">Garcia, A. M. & J. C. Martin. 2007. Function Words in Authorship Attribution Studies. <i>Literary and Linguistic Computing</i> 22(1). 49‚Äì66. <a href=\"https://doi.org/10.1093/llc/fql048\" target=\"_blank\">https://doi.org/10.1093/llc/fql048</a>.</font></div>\n\nA classic stylometric study using Bayesian statistics rather than machine learning is:\n<div class=\"reference\"><font size=\"3\">Mosteller, Frederick & David Lee Wallace. 1984. <i>Applied Bayesian and classical inference: the case of the Federalist papers</i>. New York: Springer-Verlag.</font></div>\n\n\n### More complex methods ‚Äì Others {#others}\n\n#### Topic models\nTopic modeling is a method which tries to recover abstract ‚Äòtopics‚Äô which occur in a collection of documents. The underlying assumption is that different topics will tend to be associated with different words, different documents will tend to be associated with different topics, and therefore the distribution of words across documents allows us to find topics. The complete model includes the strength of association (or probability) between each word and each topic, and between each topic and each document. A topic consists of a group of words and it is up to the researcher to decide if a semantically coherent interpretation can be given to any of the topics recovered. The number of topics to be recovered is specified in advance.\n\nThe example visualisation below is based on topic modeling of the State of the Union addresses given by US presidents, and shows the relative importance of different topics over time. In the right hand part of the figure, the words most closely linked to each topic are listed; the researcher has not attempted to give labels to these (although in some cases, it is not too hard to imagine what labels we might use). Note also that words are not uniquely linked to topics - for example, the word *state* is closely linked to seven of the topics in this model.\n \n![Topics in the State of the Union Address over time](/topic_models.png)\n\nThe Wikipedia entry for [topic models](https://en.wikipedia.org/wiki/Topic_model) gives a more detailed explanation of the process.\n\nThis [topic modeling tutorial](https://slcladal.github.io/topicmodels.html) from LADAL uses R coding to process textual data and generate a topic model from that data.\n\n<i>Poetics</i> 41(6) is a journal issue devoted to the use of topic models in literary studies: the introduction to the journal (by Mohr and Bogdanov: <a href=\"https://doi.org/10.1016/j.poetic.2013.10.001\" target=\"_blank\">https://doi.org/10.1016/j.poetic.2013.10.001</a>) provides a useful overview of the method.\n\nAnd this paper uses topic modeling as one tool in trying to improve access to a huge collection of scholarly literature:\n\n<div class=\"reference\"><font size=\"3\">Mimno, David. 2012. Computational historiography: Data mining in a century of classics journals. <i>Journal on Computing and Cultural Heritage</i> 5(1). 1‚Äì19. <a href=\"https://doi.org/10.1145/2160165.2160168\" target=\"_blank\">https://doi.org/10.1145/2160165.2160168</a>.</font></div>\n\n\n\n#### Network Analysis\nNetwork analysis allows us to produce visualisations of the relationships between entities within a dataset. Analysis of social networks is a classic application of the method, but words and documents can also be thought of as entities and the relationships between them can then be analysed with this method. (see example visualisation of Darwin‚Äôs *Origin of Species* above)\nHere is another example of a network graph illustrating the relationships between the characters of Shakespeare‚Äôs *Romeo and Juliet*:\n\n![Network of characters in Romeo and Juliet](/network_RandJ-1.png)\n\nThis article gives several examples of how representing collocational links between words as a network can lead to insight into meaning relations:\n<div class=\"reference\"><font size=\"3\">Brezina, Vaclav, Tony McEnery & Stephen Wattam. 2015. Collocations in context: A new perspective on collocation networks. <i>International Journal of Corpus Linguistics</i> 20(2). 139‚Äì173. <a href=\"https://doi.org/10.1075/ijcl.20.2.01bre\" target=\"_blank\">https://doi.org/10.1075/ijcl.20.2.01bre</a>. (pdf)</font></div>\n\nWikipedia has articles on network theory in [general](https://en.wikipedia.org/wiki/Network_theory) and on [social network analysis](https://en.wikipedia.org/wiki/Social_network_analysis).\nin particular.\n\nLADAL‚Äôs tutorial on [Network Analysis](https://slcladal.github.io/net.html) introduces this method using R coding.\n\n\n\n### Visualisation {#visualisation}\n\nVisualisation is an important technique for exploring data, allowing us to see patterns easily, and also for presenting results. \nThere are many methods for creating visualisations and this article gives an overview of some possibilities for visualising corpus data:\n<div class=\"reference\"><font size=\"3\">Siirtola, Harri, Terttu Nevalainen, Tanja S√§ily & Kari-Jouko R√§ih√§. 2011. Visualisation of text corpora: A case study of the PCEEC. <i>How to Deal with Data: Problems and Approaches to the Investigation of the English Language over Time and Space</i>. Helsinki: VARIENG 7. <a href=\"https://varieng.helsinki.fi/series/volumes/07/siirtola_et_al/index.html\" target=\"_blank\">[html]</a></font></div>\n\n\nIf you would like to see something more complex, this article includes animations showing change in use of semantic space over time ‚Äì but you need to have full access to the online publication to see it.\n\n\n<div class=\"reference\"><font size=\"3\">Hilpert, Martin & Florent Perek. 2015. Meaning change in a petri dish: constructions, semantic vector spaces, and motion charts. <i>Linguistics Vanguard</i> 1(1). <a href=\"https://doi.org/10.1515/lingvan-2015-0013\" target=\"_blank\">https://doi.org/10.1515/lingvan-2015-0013</a>.</font></div> \n\nThis LADAL tutorial on [data visualisation](https://slcladal.github.io/introviz.html) in R makes use of the [ggplot2](https://ggplot2.tidyverse.org/) package to create some common data visualisations using code.\n\n"},{"title":"Organisation","slug":"organisation","content":"\n# Organisation\n\nATAP is one strand of the partnership between the Australian Research Data Commons ([ARDC](https://ardc.edu.au/)) and the [School of Languages and Cultures](https://languages-cultures.uq.edu.au/) at the [University of Queensland](https://www.uq.edu.au/). This partnership includes a number of projects that explore language-related technologies, data collection infrastructure and Indigenous capability programs. These projects are being led out of the Language Technology and Data Analytics Lab ([LADAL](https://slcladal.github.io/index.html)), which is overseen by [Professor Michael Haugh](https://languages-cultures.uq.edu.au/profile/1498/michael-haugh) and [Dr Martin Schweinberger](https://languages-cultures.uq.edu.au/profile/4295/martin-schweinberger).\n\n<hr />\n\n## Partner Institutions:\n\n**University of Queensland:**\n\n- Professor Michael Haugh\n- Dr Martin Schweinberger\n\n**University of Sydney:**\n\n- Professor Monika Bednarek (Sydney Corpus Lab)\n\n**AARNet**\n\n- Dr Sara King\n- Ryan Fraser\n"},{"title":"Research Objects","slug":"research_objects","content":"\n\nThe ATAP workbench will enable researchers to work in ways which improve the reproducibility and accountability of their research. To do this, the output of your work in ATAP will be a fully documented research object.\n\n![Illustration of an RO-Crate and its contents](/ro-crate.png)\n\nResearch objects are a single package that collects the data, code and other resources used in a piece of research and allows it to be a cite-able research output in its own right. At the end of your work in ATAP, there will be the automated option to package together the components of your workflow, such as:\n- Raw data\n- Transformed data\n- A record of the notebooks which you have used\n- Additional scripts and codes\n- Results\n- Visualisations\n- High quality metadata\n\nATAP will output an [RO-Crate](https://www.researchobject.org/ro-crate/) that contains these objects. An output of this kind can be assigned a unique identifier (such as a doi), and it can be published via services such as Zenodo or figshare so it can be cited in publications. This will make it easy to fulfil the increasingly common requirement of journals to make available the data that supports a publication.\n"},{"title":"Resources","slug":"resources","content":"\n# Resources\n\n[Language Technology and Data Analysis Laboratory (LADAL)](https://slcladal.github.io/)\n\nLADAL aims to help develop computational and digital skills by providing information and practical, hands-on tutorials on data and text analytics as well as on statistical methods relevant for language research.\n<br />\n\n[Our Vimeo Channel](https://vimeo.com/user172843579)\n\nRecorded presentations and interviews - take a look!\n<br>\n\n[GLAM Workbench](https://glam-workbench.net/)\nThe GLAM workbench is a collection of tools, tutorials, examples, and hacks created by Tim Sherratt to help you work with data from galleries, libraries, archives, and museums (the GLAM sector). The primary focus is Australia and New Zealand, but new collections are being added all the time.\n<br />\n\nThe [Sydney Corpus Lab](https://sydneycorpuslab.com/) aims to promote corpus linguistics in Australia, hosts a growing list of English-language corpora, and features regular blogs about corpus linguistic analysis.\n<br />\n\n[CONSTELLATE](https://constellate.org/)\nConstellate is the text analytics service from the not-for-profit ITHAKA - the same people who brought you JSTOR and Portico. It is a platform for teaching, learning, and performing text analysis using the world‚Äôs leading archival repositories of scholarly and primary source content.\n<br />\n\n[The Art of Literary Text Analysis](https://github.com/sgsinclair/alta/blob/master/ipynb/ArtOfLiteraryTextAnalysis.ipynb)\n\nThe Art of Literary Text Analysis (ALTA) has three objectives.\n\n- First, to introduce concepts and methodologies for literary text analysis programming. It doesn't assume you know how to program or how to use digital tools for analyzing texts.\n- Second, to show a range of analytical techniques for the study of texts. While it cannot explain and demonstrate everything, it provides a starting point for humanists with links to other materials.\n- Third, to provide utility notebooks you can use for operating on different texts. These are less well documented and combine ideas from the introductory notebooks.\n  <br />\n\n[Introduction to Cultural Analytics & Python](https://melaniewalsh.github.io/Intro-Cultural-Analytics/welcome.html) is an online textbook by Melanie Walsh, which offers an introduction to the programming language Python that is specifically designed for people interested in the humanities and social sciences. This book demonstrates how Python can be used to study cultural materials such as song lyrics, short stories, newspaper articles, tweets, Reddit posts, and film screenplays. It also introduces computational methods such as web scraping, APIs, topic modeling, Named Entity Recognition (NER), network analysis, and mapping.\n<br />\n\n[Text Analysis Pedagogy Institute](https://labs.jstor.org/tapi/) is an open educational institute for the benefit of teachers (and aspiring teachers) of text analysis in the digital humanities.\n<br />\n\n[The Programming Historian](https://programminghistorian.org/) publishes novice-friendly, peer-reviewed tutorials that help humanists learn a wide range of digital tools, techniques, and workflows to facilitate research and teaching.\n<br />\n\n[Quinn Dombrowski's list of relevant courses and tutorials](https://github.com/quinnanya/dh-jupyter)\nA collection of Jupyter notebooks in many human and computer languages for doing digital humanities.\n"},{"title":"Text Analysis Overview","slug":"text_analysis","content":"# Text Analysis Overview\n\n### Understanding Text as Data\n\nMany definitions of *data* include an element such as *individual items of information*. If we consider *text* to include any sort of language in use, covering different modalities (spoken, written signed) and different extents (from individual sounds to multi-volume books), then fitting text to this definition requires some abstraction. We have to define some unit or units of analysis and we can then treat each of those units as an individual item of information. Examples of such units include documents, sentences and words. But *word* is not as simple a unit as you may think.\n\n### What's in a word?\n\nThe term *word* is problematic. It is well-known to linguists that phonological words (defined by sound patterns), syntactic words (defined by combinatorial possibilities) and orthographic words (defined by the conventions of a writing system) do not always coincide. And even when we are only looking at written material, there are problems. How many words are there in this sentence?\n<center><em>The cat sat on the mat</em></center>\n\nOne answer is that there are six words; that is, there are six groups of characters which are separated according to typographical convention. But there is another answer: There are five words, that is five distinct sequences of characters and one of those sequences (*the*) occurs twice. The terms standardly used to make this distinction are **type** and **token**. **Tokens** are instances of **types**, therefore if we count tokens, we count without considering repetition, while if we count types, we do consider repetition. In our example, there are five types (*the, cat, sat, on, mat*) but six tokens, because there are two tokens of one of the types (*the*).\n\nThere is a further distinction we may need to make which we can see if we consider another question:\n\n<center>Are <i>cat</i> and <i>cats</i> the same word?</center>\n\nThey are distinct types, and therefore must also be distinct as tokens. But we have an intuition that at some level they are related, that there is some more abstract item which underlies both of them. This concept is usually referred to as a **lemma** (there is more about this concept on the [Data Preparation](../data_prep) page).\n\n### Text Analysis Workflow\nThis brief introduction to text analysis divides the process into three parts. In the first stage, the text is made into data. It is divided into the units appropriate for the analysis to be carried out and shaped to a format which our analytic tools can work with. The second stage is the analysis proper, including its interpretation. A wide range of analytic methods can be used, and we give a survey of some of the commonly used possibilities. Finally, our data, methods and results can be documented and packaged as a **research object** which can be stored and reused.\n\n[Data Preparation](../data_prep) &emsp;&emsp; [Useful Methods](../methods) &emsp;&emsp; [Research Objects](../research_objects)\n"},{"title":"Resources","slug":"tools","content":"\n# Tools\n\n### [ATAP Portal](https://data.atap.edu.au/main)\n\nThis first version of our user portal provides access to two datasets with notebooks which enable exploration of the data.\n\n- **Farms to Freeways** - Data collected in 1991-1992 in a project titled Western Sydney Women's Oral History Project 'From farms to freeways: Women's memories of Western Sydney', which sought to analyse the experiences of women who had lived in the Blacktown and Penrith areas since the early 1950s, including their responses to social changes brought about by rapid suburbanisation in the Western Sydney region in the post-war period.\n- **Corpus of Oz Early English** (CoOEE) - Approximately 2 million tokens of material produced in Australia between 1788 and 1900, divided into four time periods and four registers.\n\n### [Discursis](https://github.com/Australian-Text-Analytics-Platform/discursis)\n\nDiscursis is communication analytics technology that allows a user to analyse text based communication data, such as conversations, web forums and training scenarios. It uses natural language processing algorithms to automatically process transcribed text to highlight participant interactions around specific topics and over the time-course of the conversation. Discursis can assist practitioners in understanding the structure, information content, and inter-speaker relationships that are present within input data. Discursis also provides quantitative measures of key metrics, such as topic introduction; topic consistency; and topic novelty.\n\n[Discursis](https://itee.uq.edu.au/project/discursis) was developed by [Dan Angus](https://www.qut.edu.au/about/our-people/academic-profiles/daniel.angus), [Janet Wiles](https://itee.uq.edu.au/profile/2444/janet-wiles) and Andrew Smith and has been reworked as an open source tool by staff of [Sydney Informatics Hub](https://www.sydney.edu.au/research/facilities/sydney-informatics-hub.html).\n\n### [Quotation tool](https://github.com/Australian-Text-Analytics-Platform/quotation-tool)\n\nThis Quotation Tool can be used to extract quotes from a text. In addition to extracting the quotes, the tool also provides information about who the speakers are, the location of the quotes (and the speakers) within the text, the identified named entities, and other information which can be useful for text analysis.\n\n### [Semantic Tagger](https://github.com/Australian-Text-Analytics-Platform/semantic-tagger)\n\nThis Semantic Tagger uses the Python Multilingual Ucrel Semantic Analysis System ([PyMUSAS](https://ucrel.github.io/pymusas/)) to tag text so that you can extract token level semantic tags from the tagged text. PyMUSAS, is a rule based token and Multi Word Expression (MWE) semantic tagger. The tagger can support any semantic tagset, however the currently released tagset is for the UCREL Semantic Analysis System ([USAS](https://ucrel.lancs.ac.uk/usas/)) semantic tags. In addition to the USAS tags, you will also see the lemmas and Part-ofSpeech (POS) tags in the text. For English, the tagger also identifies and tags Multi Word Expressions (MWE), i.e., expressions formed by two or more words that behave like a unit such as 'South Australia'.\n\n### [Geolocation tools](https://github.com/Australian-Text-Analytics-Platform/geolocation-tools-workshop)\n\nThese tools assist the processes of recognising placenames in historical documents and then using online gazetteers to determine what known locations the placenames correspond to, and then to gather related geolocation data such as coordinates. The tools accelerate the workflow for these processes but leave scope for the user to have input in disambiguation.\n\n### [Keyword analysis](https://github.com/Australian-Text-Analytics-Platform/keyword-analysis)\n\nThis notebook presents a Keyword Analysis tool to analyse words in a collection of corpora and identify whether certain words are over or under-represented in a particular corpus compared to their representation in the other corpora.\n\n### [Document Similarity](https://github.com/Australian-Text-Analytics-Platform/document-similarity)\n\nThis notebook presents a tool to identify similar documents in your corpus and decide whether to keep them in the corpus or to remove them.\n\n### Language Technology and Data Analysis Laboratory ([LADAL](https://ladal.edu.au/index.html))\n\nThe Language Technology and Data Analysis Laboratory offers a range of tools for text analysis (and more).\n"},{"title":"Previous Workshops","slug":"workshops","content":"\n# Previous Workshops\n\nIf your university or organisation would like to host a workshop, please [contact us](mailto:info@atap.edu.au).\n\n#### Geolocating Australian Historical Resources\n\nThis workshop was part of the Australian Society of Archivists 2022 Conference\n\n**Date**: October 20 2022 <br>\n**Length**: 3 hours <br>\n**Facilitators**: Michael Niemann, Fiannuala Morgan (ANU), Simon Musgrave\n\n#### Learn how to collect and analyse comments on YouTube videos using the open-source tools Youte and Discursis\n\n**Date**: September 21 2022 <br>\n**Length**: 3 hours <br>\n**Facilitators**: Boyd Nguyen (ADO), Sam Hames (ATAP)\n\n#### Finding quotes and speakers in text using the ATAP quotation tools\n\n**Date**: September 8 2022 <br>\n**Length**: 1 hour <br>\n**Facilitators**: Sony Jufri\n\n#### Advance care planning for your research data\n\n**Date**: September 7 2022 <br>\n**Event**: FAVeR Showcasing Approaches to Digital Humanities for Researchers <br>\n**Length**: 1 hour <br>\n**Facilitators**: Sam Hames, Ben Foley\n\n**Date**: June 21 2022 <br>\n**Event**: [SICSS-Sydney](https://sicss.io/2022/sydney/) <br>\n**Length**: 1 hour <br>\n**Facilitators**: Sam Hames, Ben Foley\n\n#### Computational Thinking in the Humanities\n\nThe workshop Computational Thinking in the Humanities was a 3-hour online workshop featuring two plenary talks, lightning presentations, as well as a panel discussion. The workshop was co-organized by the Australian Text Analytics Platform ([ATAP](https://www.atap.edu.au/)), [FIN-CLARIAH](https://www.kielipankki.fi/organization/fin-clariah/) and its UEF representatives, and the [Australian Digital Observatory](https://www.digitalobservatory.net.au/).\n\n**Date**: September 1 2022 <br>\n**Length**: 3 hours <br>\n[Further details](https://ladal.edu.au/compthink.html)\n\n#### Network analysis and Topic Modeling on Twitter data using R\n\n**Date**: May 18 2022 <br>\n**Event**: Joint event ADO and ATAP <br>\n**Length**: 3 hours <br>\n**Facilitators**: Alice Miller, Simon Musgrave\n\n#### Monotreme Mania! Comparative text analytics on Twitter data\n\n**Date**: 16 March 2022 <br>\n**Event**: Joint event ADO and ATAP <br>\n**Length**: 3 hours <br>\n**Facilitators**: Sam Hames, Simon Musgrave\n\n#### An introduction to Jupyter notebooks for text analysis: Virtual workshop for absolute beginners\n\n**Date**: August 24 2022 <br>\n**Event**: FAVeR Showcasing Approaches to Digital Humanities for Researchers<br>\n**Length**: 2 hours <br>\n**Facilitators**: Sara King, Simon Musgrave\n\n**Date**: 27 July 2022 <br>\n**Event**: Workshop for Sydney Corpus Lab <br>\n**Length**: 3 hours <br>\n**Facilitators**: Sara King, Simon Musgrave\n\n**Date**: 24 November 2021 <br>\n**Event**: Digital Humanities Australasia 2021 Conference <br>\n**Length**: 3 hours <br>\n**Facilitators**: Sara King, Simon Musgrave\n"}]}},"__N_SSG":true}