<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>ATAP</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/7c9960b738380dfb.css" as="style"/><link rel="stylesheet" href="/_next/static/css/7c9960b738380dfb.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-69bfa6990bb9e155.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-f4ae3437c92c1efc.js" defer=""></script><script src="/_next/static/chunks/pages/_app-16676ab8b2ecbfd3.js" defer=""></script><script src="/_next/static/chunks/pages/%5Bpage%5D-d2184b8ab241f292.js" defer=""></script><script src="/_next/static/4R3jnWJKeJMM9F6PmJ3D9/_buildManifest.js" defer=""></script><script src="/_next/static/4R3jnWJKeJMM9F6PmJ3D9/_ssgManifest.js" defer=""></script><script src="/_next/static/4R3jnWJKeJMM9F6PmJ3D9/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div><div class="flex min-h-screen flex-col"><div class="sticky top-0 h-20 bg-white shadow-sm md:static md:shadow-none"><nav class="container flex h-full items-center justify-between text-gray-700"><a href="/"><img src="/ATAP_logo-sm.png" class="h-16"/></a><div class="hidden items-center space-x-2 md:flex"><ul class="flex divide-x divide-slate-400 text-sm"><a class="cursor-pointer px-4 font-semibold first:pl-0 hover:underline" href="/"><li>Home</li></a><a class="cursor-pointer px-4 font-semibold first:pl-0 hover:underline" href="/posts"><li>Blog</li></a><a class="cursor-pointer px-4 font-semibold first:pl-0 hover:underline" href="/text_analysis"><li>Text Analysis</li></a><a class="cursor-pointer px-4 font-semibold first:pl-0 hover:underline" href="/events"><li>Events</li></a><a class="cursor-pointer px-4 font-semibold first:pl-0 hover:underline" href="/resources"><li>Resources</li></a><a class="cursor-pointer px-4 font-semibold first:pl-0 hover:underline" href="/organisation"><li>Organisation</li></a></ul><div class="relative flex"><input placeholder="Page name" class="w-0 mr-2 border transition-all" value=""/><button><svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"></path></svg></button></div></div><div class="md:hidden"><button class="relative h-10 w-10 text-gray-500 focus:outline-none"><span class="sr-only">Open main menu</span><div class="absolute left-1/2 top-1/2 block w-5 -translate-x-1/2 -translate-y-1/2 transform"><span aria-hidden="true" class="absolute block h-0.5 w-5 transform bg-current transition duration-500 ease-in-out rounded-sm -translate-y-1.5"></span><span aria-hidden="true" class="absolute block h-0.5 w-5 transform bg-current transition duration-500 ease-in-out rounded-sm false"></span><span aria-hidden="true" class="absolute block h-0.5 w-5 transform bg-current transition duration-500 ease-in-out rounded-sm translate-y-1.5"></span></div></button></div></nav><div></div></div><main class="container flex-1 py-8"><article class="prose max-w-none lg:prose-xl"><p><a href="#counting-words">Counting words</a>   
<a href="#classification">More complex methods - Classification</a>   
<a href="#others">More complex methods - Others</a>   
<a href="#visualisation">Visualisation</a></p>
<h3 id="id1">Introduction</h3>
<p>Throughout this page, we have given links to further information in Wikipedia and in the tutorials provided by the Language Technology and Data Analysis Laboratory <a href="https://slcladal.github.io/">LADAL</a> at the University of Queensland. We also have given references to published research using the methods we discuss.</p>
<p>LADAL has an overview of <a href="https://slcladal.github.io/textanalysis.html">text analysis and distant reading</a>.</p>
<h3 id="counting-words">Counting Words</h3>
<h4 id="id2">Word frequency</h4>
<p>Knowing how frequently words occur in a text can already give us information about that text and frequency lists based on large corpora are a useful tool in themselves - you can <a href="https://kilgarriff.co.uk/bnc-readme.html">download</a> such lists for the (original) <a href="http://www.natcorp.ox.ac.uk/">British National Corpus</a>.</p>
<p>Tracking changes in the frequency of use of words across time has become popular since Google’s <a href="https://books.google.com/ngrams">n-gram viewer</a> has been available. However, results from this tool have to treated with caution for reasons set out in this <a href="https://broadstreet.blog/2021/08/11/bad-ngrams/">blog-post</a>.</p>
<p>Comparing patterns of word frequency across texts can be part of authorship attribution. Patrick Juola <a href="https://languagelog.ldc.upenn.edu/nll/?p=5315">describes using this method</a> when he tried to decide whether Robert Galbraith was really J.K Rowling.</p>
<p>This paper uses frequency and concordance analysis, with Australian data:</p>
<div class="reference"><font size="3">Bednarek, Monika. 2020. Invisible or high-risk: Computer-assisted discourse analysis of references to Aboriginal and Torres Strait Islander people(s) and issues in a newspaper corpus about diabetes. <i>PLoS ONE</i> 15/6: e0234486. <a href="https://doi.org/10.1371/journal.pone.0234486" target="_blank">https://doi.org/10.1371/journal.pone.0234486</a> </font></div>
<p>The ratio of types and tokens in a text has been used as a measure of lexical diversity in developmental and clinical studies as well as in stylistics. It has also been applied to theoretical problems in linguistics:</p>
<div class="reference"><font size="3">Kettunen, Kimmo. 2014. Can Type-Token Ratio be Used to Show Morphological Complexity of Languages? <i>Journal of Quantitative Linguistics</i> 21(3). 223–245. <a href="https://doi.org/10.1080/09296174.2014.911506" target="_blank">https://doi.org/10.1080/09296174.2014.911506</a>.</font></div>
<h4 id="id3">Concordance</h4>
<p>A concordance allows the researcher to see all instances of a word or phrase in a text, neatly aligned in a column and with preceding and following context (see image below).
Concordances are often a first step in analysis. The concordance allows a researcher to see how a word is used and in what contexts. Most concordancing tools allow sorting of results by either preceding or following words – the coloured text in the example below shows that in this case the results have been sorted hierarchically on the three following words. This possibility can help in discovering patterns of co-occurrence. Concordances are also very useful when looking for good examples to illustrate a point. (The type of display seen in the example is often referred to as KeyWord In Context – KWIC. There is a possibility of confusion here, as there is a separate analytic method commonly referred to as <a href="#keywords">Keywords</a>.)</p>
<p><img src="/concordance.png" alt="Example of a concordance"></p>
<p>(The example here was produced by <a href="https://www.laurenceanthony.net/software/antconc/">Antconc</a>)</p>
<p>This <a href="https://slcladal.github.io/kwics.html">tutorial</a> from LADAL on concordancing uses a notebook containing R code as a method of extracting concordance data.</p>
<h4 id="id4">Clusters and collocations</h4>
<p>Two methods can be used for counting the co-occurrence of items in text.
Clusters (sometimes known as n-grams) are sequences of adjacent items. A bigram is a sequence of two items, a trigram (3-gram) is a sequence of three items and so on. n-grams are types made up of more than one item, and therefore we can count the number of tokens of each n-gram in texts. n-grams are also the basis for a class of language models. (Google created a very large data set of English n-grams in developing their language-based algorithms and this <a href="https://storage.googleapis.com/books/ngrams/books/datasetsv3.html">data is available</a>.)
Collocations are patterns of co-occurrence in which the items are not necessarily adjacent. An example of why this is important is verbs and their objects in English. The object of a verb is a noun phrase and in many cases the first item in an English noun phrase is a determiner. This means that for many transitive verbs, the bigram <em>verb the</em> will occur quite frequently. But it is much more interesting to know whether there are patterns relating verbs and the entities which are their objects.
Collocation analysis uncovers such patterns by looking at co-occurrences within a window of a certain size, for example three tokens on either side of the target. Collocation analysis gives information about the frequency of the co-occurrence of words and also a statistical measure of how likely that frequency is, given the overall frequencies of the terms in the corpus. Measures commonly applied include <a href="https://en.wikipedia.org/wiki/Mutual_information">Mutual Information</a> scores and <a href="https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood">Log-Likelihood</a> scores.
Collocations can also tell us about the meanings of words. If a word has collocates which fall into semantically distinct groups, this can indicate ambiguity or polysemy. And if different words share patterns of collocation, this can be evidence that the words are at least partial synonyms.</p>
<p>This graphic shows collocation relations in Darwin’s Origin of Species visualised as a network - the likelihood of a pair of words occurring in close proximity in the text is indicated by the weight of the line linking them:</p>
<p><img src="/collocation_network.png" alt="Collocation patterns in Origin of Species as a network"></p>
<p>This article uses bigram frequencies as part of an analysis of language change:</p>
<div class="reference"><font size="3">Schweinberger, Martin. 2021. Ongoing change in the Australian English amplifier system. <i>Australian Journal of Linguistics</i> 41(2). 166–194. <a href="https://doi.org/10.1080/07268602.2021.1931028" target="_blank">https://doi.org/10.1080/07268602.2021.1931028</a>.</font></div>
An article which uses concordances and collocation analysis:
<div class="reference"><font size="3">Baker, Paul & Tony McEnery. 2005. A corpus-based approach to discourses of refugees and asylum seekers in UN and newspaper texts. <i>Journal of Language and Politics</i> 4(2). 197–226.</font></div>
<p>This research uses the discovery of shared patterns of collocation as evidence that the words are at least partial synonyms:</p>
<div class="reference"><font size="3">McEnery, Tony & Helen Baker. 2017. <i>Corpus linguistics and 17th-century prostitution: computational linguistics and history</i> (Corpus and Discourse. Research in Corpus and Discourse). London; New York, NY: Bloomsbury Academic. (especially chapters 4 and 5)</font></div>
<p>This <a href="https://slcladal.github.io/coll.html">tutorial</a> from LADAL on analysing co-occurences and collocations uses a notebook containing R code as a method to extract and visualise semantic links between words.</p>
<h4 id="id5">Keywords</h4>
<p>Keyword analysis is a statistically robust method of comparing frequencies of words in corpora. It tells us which words are more frequent (or less frequent) than would be expected in one text compared to another text and gives an estimate of the probability of the result. Keyword analysis uses two corpora: a <strong>target</strong> corpus, which is the material of interest, and a <strong>comparison</strong> corpus. Frequency lists are made for each corpus and then frequency of individual types in each corpus are compared. Keywords are ones which occur more ( or less) frequently in the target corpus than expected given the reference corpus.
The <strong>keyness</strong> of individual items is a quantitative measure of how unexpected the frequency is; chi-square is a one possible measure of this, but a log-likelihood measure is more commonly used. Positive keywords are words which occur more commonly than expected; negative keywords are words which occur less commonly than expected.</p>
<p>This visualisation shows a comparison of positive distinguishing words for three texts (Charles Darwin’s <em>Origin</em>, Herman Melville’s <em>Moby Dick</em>, and George Orwell’s <em>1984</em>), words that occur more commonly than we expect in one text when taking the other two texts as a comparison:</p>
<p><img src="/keywords.png" alt="Keywords from three texts"></p>
<p>This paper applies keyword analysis to Australian text data sourced from a television series script:</p>
<div class="reference"><font size="3">Bednarek, Monika. 2020. Keyword analysis and the indexing of Aboriginal and Torres Strait Islander identity: A corpus linguistic analysis of the Australian Indigenous TV drama Redfern Now. <i>International Journal of Corpus Linguistics</i> 25/4: 369-99. <a href="http://doi.org/10.1075/ijcl.00031.bed" target="_blank">http://doi.org/10.1075/ijcl.00031.bed</a></font></div>
<p>Tony McEnery describes using the keyword analysis method to compare four varieties of English in this chapter:</p>
<div class="reference"><font size="3">McEnery, Tony. 2016. Keywords. In Paul Baker & Jesse Egbert (eds.), *Triangulating methodological approaches in corpus-linguistic research* (Routledge Advances in Corpus Linguistics 17), 20–32. New York: Routledge.</font></div>
<p>This article explores how to assess Shakespeare’s use of words to build characters by using keyword analysis of the characters' dialog:</p>
<div class="reference"><font size="3">Culpeper, Jonathan. 2002. Computers, language and characterisation: An analysis of six characters in Romeo and Juliet. In <i>Conversation in Life and in Literature: Papers from the ASLA Symposium</i> (Association Suedoise de Linguistique Appliquee (ASLA) 15), 11–30. Uppsala: Universitetstryckeriet. (<a href="https://lexically.net/wordsmith/corpus_linguistics_links/Keywords-Culpeper.pdf" target="_blank">pdf</a>)</font></div>
<h3 id="classification">More complex methods – Classification</h3>
<p>Classification methods aim to assign some unit of analysis, such as a word or a document, to a class. For example, a document (or a portion of a document) can be classified as having positive or negative sentiment. These methods are all examples of supervised machine learning. An algorithm is trained on the basis of annotated data to identify classifiers in the data – features which correlate in some way with the annotated classifications. If the algorithm achieves good results on testing data (classified by human judgment), then it can be used to classify unannotated data.</p>
<h4 id="id6">Document Classification</h4>
<p>The task here is to assign documents to categories automatically. An everyday example of this procedure is spam filtering of email that may be applied by internet service providers and also within email applications. An example of this technique being used in research would be automatically identifying historical court records as referring either to violent crimes, property offences, or other crimes.</p>
<p>The following two articles taken together give an account of a technique for partially automating the training phase of classification and then of how the classifiers allowed researchers to access new information in a large and complex text.</p>
<div class="reference"><font size="3">Leavy, Susan, Mark T Keane & Emilie Pine. 2019. Patterns in language: Text analysis of government reports on the Irish industrial school system with word embedding. <i>Digital Scholarship in the Humanities</i> 34(Supplement_1). i110–i122. <a href="https://doi.org/10.1093/llc/fqz012" target="_blank">https://doi.org/10.1093/llc/fqz012</a>.</font></div>
<div class="reference"><font size="3">Pine, Emilie, Susan Leavy & Mark T. Keane. 2017. Re-reading the Ryan Report: Witnessing via Close and Distant Reading. <i>Éire-Ireland</i> 52(1–2). 198–215. https://doi.org/10.1353/eir.2017.0009. (<a href="https://researchrepository.ucd.ie/handle/10197/10287" target="_blank">available online</a>)</font></div>
<p><a href="https://en.wikipedia.org/wiki/Document_classification">Wikipedia</a></p>
<h4 id="id7">Sentiment analysis</h4>
<p>Sentiment analysis assigns documents according to the affect which they express. In simple cases, this can mean sorting documents into those which a express a positive view and those which express a negative view (with a neutral position sometimes also included). Such classifications are the basis for aggregated ratings - for example, online listings of movies and restaurants. A sentiment value is assigned to individual reviews then an aggregate score is calculated based on those values and that aggregate score is the rating presented to the user. More sophisticated sentiment analysis can assign values on a scale. Some sentiment analysis tools use dictionaries of terms with sentiment values assigned to those terms; these are known as pre-trained or pre-determined classifiers.</p>
<p>The following figure shows the results of the sentiment analysis of four texts (<em>The Adventures of Huckleberry Finn</em> by Mark Twain, <em>1984</em> by George Orwell, <em>The Colour out of Space</em> by H.P.Lovecraft, and <em>On the Origin of Species</em> by Charles Darwin) using the Word-Emotion Association Lexicon (Mohammad and Turney 2013). The graphic shows what percentage of each text can be assigned to each of eight categories of sentiment:</p>
<p><img src="/sentiment_analysis.png" alt="Sentiment analysis of four texts"></p>
<p>The Wikipedia entry for <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">Sentiment Analysis</a> gives more information and examples particularly in relation to the use of sentiment analysis as a tool used in online settings.</p>
<p>LADAL’s <a href="https://slcladal.github.io/sentiment.html">Sentiment Analysis tutorial</a> uses a notebook containing R code as a method of performing sentiment analysis.</p>
<p>This article discusses problems in assembling training data for complex sentiment analysis tasks and then applies the results to oral history interviews with Holocaust survivors:</p>
<div class="reference"><font size="3">Blanke, Tobias, Michael Bryant & Mark Hedges. 2020. Understanding memories of the Holocaust—A new approach to neural networks in the digital humanities. <i>Digital Scholarship in the Humanities</i> 35(1). 17–33. <a href="https://doi.org/10.1093/llc/fqy082" target="_blank">https://doi.org/10.1093/llc/fqy082</a>.</font></div>
<h4 id="id8">Named Entity Recognition</h4>
<p>Named Entity Recognition involves two levels of classification. First, segments of text are classified as either denoting or not denoting an entity: for example, a person, a place or an organization. The identified entities can then be classified as belonging to one of the types of entity.</p>
<p>The Wikipedia entry explaining <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named-entity recognition</a> gives further detail about the technique.</p>
<p>This article looks at the problems encountered when applying a well-known entity recognition package (<a href="https://nlp.stanford.edu/software/CRF-NER.html">Stanford</a>) to historical newspapers in the National Library of Australia’s Trove collection:</p>
<div class="reference"><font size="3">Mac Kim, Sunghwan & Steve Cassidy. 2015. Finding names in Trove: Named Entity Recognition for Australian historical newspapers. In <i>Proceedings of the Australasian Language Technology Association Workshop 2015</i>, 57–65. (<a href="https://aclanthology.org/U15-1007.pdf" target="_blank">pdf</a>)</font></div>
<p>This article (section 6.3) discusses why entity recognition is not as useful as might be expected when studying names in novels:</p>
<div class="reference"><font size="3">Dalen-Oskam, K. van. 2013. Names in novels: An experiment in computational stylistics. <i>Literary and Linguistic Computing</i> 28(2). 359–370. <a href="https://doi.org/10.1093/llc/fqs007" target="_blank">https://doi.org/10.1093/llc/fqs007</a>.</font></div>
<h4 id="id9">Computational Stylistics (Stylometry)</h4>
<p>This method is also referred to as authorship attribution as the classification task is to assess patterns of language use in order to decide whether to attribute a piece of text to a particular author (and with what degree of confidence). Seemingly simple classifiers are used for this task as they are assumed to be less open to conscious manipulation by writers. For example, comparative patterns of occurrence of function words such as <em>the</em> and <em>a/an</em> are considered a better classifier than occurrences of content words. Character n-grams, that is sequences of characters of a specified length, have also proven to be a good classifier for use in this task. A recent example of these techniques being applied in a case which received a good deal of public attention was the controversy about whether <a href="https://languagelog.ldc.upenn.edu/nll/?p=5315">Robert Galbraith was really J.K Rowling</a>.</p>
<p>The Wikipedia entry on <a href="https://en.wikipedia.org/wiki/Stylometry">stylometry</a> gives further information on the methodology.</p>
<p>This article applies stylometric techniques to a classic of Chinese literature:</p>
<div class="reference"><font size="3">Zhu, Haoran, Lei Lei & Hugh Craig. 2021. Prose, Verse and Authorship in Dream of the Red Chamber: A Stylometric Analysis. <i>Journal of Quantitative Linguistics</i> 28(4). 289–305. <a href="https://doi.org/10.1080/09296174.2020.1724677" target="_blank">https://doi.org/10.1080/09296174.2020.1724677</a>.</font></div>
An overview of the use of function words in stylometry:
<div class="reference"><font size="3">Garcia, A. M. & J. C. Martin. 2007. Function Words in Authorship Attribution Studies. <i>Literary and Linguistic Computing</i> 22(1). 49–66. <a href="https://doi.org/10.1093/llc/fql048" target="_blank">https://doi.org/10.1093/llc/fql048</a>.</font></div>
<p>A classic stylometric study using Bayesian statistics rather than machine learning is:</p>
<div class="reference"><font size="3">Mosteller, Frederick & David Lee Wallace. 1984. <i>Applied Bayesian and classical inference: the case of the Federalist papers</i>. New York: Springer-Verlag.</font></div>
<h3 id="others">More complex methods – Others</h3>
<h4 id="id10">Topic models</h4>
<p>Topic modeling is a method which tries to recover abstract ‘topics’ which occur in a collection of documents. The underlying assumption is that different topics will tend to be associated with different words, different documents will tend to be associated with different topics, and therefore the distribution of words across documents allows us to find topics. The complete model includes the strength of association (or probability) between each word and each topic, and between each topic and each document. A topic consists of a group of words and it is up to the researcher to decide if a semantically coherent interpretation can be given to any of the topics recovered. The number of topics to be recovered is specified in advance.</p>
<p>The example visualisation below is based on topic modeling of the State of the Union addresses given by US presidents, and shows the relative importance of different topics over time. In the right hand part of the figure, the words most closely linked to each topic are listed; the researcher has not attempted to give labels to these (although in some cases, it is not too hard to imagine what labels we might use). Note also that words are not uniquely linked to topics - for example, the word <em>state</em> is closely linked to seven of the topics in this model.</p>
<p><img src="/topic_models.png" alt="Topics in the State of the Union Address over time"></p>
<p>The Wikipedia entry for <a href="https://en.wikipedia.org/wiki/Topic_model">topic models</a> gives a more detailed explanation of the process.</p>
<p>This <a href="https://slcladal.github.io/topicmodels.html">topic modeling tutorial</a> from LADAL uses R coding to process textual data and generate a topic model from that data.</p>
<p><i>Poetics</i> 41(6) is a journal issue devoted to the use of topic models in literary studies: the introduction to the journal (by Mohr and Bogdanov: <a href="https://doi.org/10.1016/j.poetic.2013.10.001" target="_blank">https://doi.org/10.1016/j.poetic.2013.10.001</a>) provides a useful overview of the method.</p>
<p>And this paper uses topic modeling as one tool in trying to improve access to a huge collection of scholarly literature:</p>
<div class="reference"><font size="3">Mimno, David. 2012. Computational historiography: Data mining in a century of classics journals. <i>Journal on Computing and Cultural Heritage</i> 5(1). 1–19. <a href="https://doi.org/10.1145/2160165.2160168" target="_blank">https://doi.org/10.1145/2160165.2160168</a>.</font></div>
<h4 id="id11">Network Analysis</h4>
<p>Network analysis allows us to produce visualisations of the relationships between entities within a dataset. Analysis of social networks is a classic application of the method, but words and documents can also be thought of as entities and the relationships between them can then be analysed with this method. (see example visualisation of Darwin’s <em>Origin of Species</em> above)
Here is another example of a network graph illustrating the relationships between the characters of Shakespeare’s <em>Romeo and Juliet</em>:</p>
<p><img src="/network_RandJ-1.png" alt="Network of characters in Romeo and Juliet"></p>
<p>This article gives several examples of how representing collocational links between words as a network can lead to insight into meaning relations:</p>
<div class="reference"><font size="3">Brezina, Vaclav, Tony McEnery & Stephen Wattam. 2015. Collocations in context: A new perspective on collocation networks. <i>International Journal of Corpus Linguistics</i> 20(2). 139–173. <a href="https://doi.org/10.1075/ijcl.20.2.01bre" target="_blank">https://doi.org/10.1075/ijcl.20.2.01bre</a>. (pdf)</font></div>
<p>Wikipedia has articles on network theory in <a href="https://en.wikipedia.org/wiki/Network_theory">general</a> and on <a href="https://en.wikipedia.org/wiki/Social_network_analysis">social network analysis</a>.
in particular.</p>
<p>LADAL’s tutorial on <a href="https://slcladal.github.io/net.html">Network Analysis</a> introduces this method using R coding.</p>
<h3 id="visualisation">Visualisation</h3>
<p>Visualisation is an important technique for exploring data, allowing us to see patterns easily, and also for presenting results.
There are many methods for creating visualisations and this article gives an overview of some possibilities for visualising corpus data:</p>
<div class="reference"><font size="3">Siirtola, Harri, Terttu Nevalainen, Tanja Säily & Kari-Jouko Räihä. 2011. Visualisation of text corpora: A case study of the PCEEC. <i>How to Deal with Data: Problems and Approaches to the Investigation of the English Language over Time and Space</i>. Helsinki: VARIENG 7. <a href="https://varieng.helsinki.fi/series/volumes/07/siirtola_et_al/index.html" target="_blank">[html]</a></font></div>
<p>If you would like to see something more complex, this article includes animations showing change in use of semantic space over time – but you need to have full access to the online publication to see it.</p>
<div class="reference"><font size="3">Hilpert, Martin & Florent Perek. 2015. Meaning change in a petri dish: constructions, semantic vector spaces, and motion charts. <i>Linguistics Vanguard</i> 1(1). <a href="https://doi.org/10.1515/lingvan-2015-0013" target="_blank">https://doi.org/10.1515/lingvan-2015-0013</a>.</font></div> 
<p>This LADAL tutorial on <a href="https://slcladal.github.io/introviz.html">data visualisation</a> in R makes use of the <a href="https://ggplot2.tidyverse.org/">ggplot2</a> package to create some common data visualisations using code.</p>
</article></main><div class="bg-secondary"><div class="container flex h-10 items-center justify-center text-white"><small>2022, <!-- --><a href="/">Australian Text Analytics Program.</a></small></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"content":"\u003cp\u003e\u003ca href=\"#counting-words\"\u003eCounting words\u003c/a\u003e   \n\u003ca href=\"#classification\"\u003eMore complex methods - Classification\u003c/a\u003e   \n\u003ca href=\"#others\"\u003eMore complex methods - Others\u003c/a\u003e   \n\u003ca href=\"#visualisation\"\u003eVisualisation\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"id1\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eThroughout this page, we have given links to further information in Wikipedia and in the tutorials provided by the Language Technology and Data Analysis Laboratory \u003ca href=\"https://slcladal.github.io/\"\u003eLADAL\u003c/a\u003e at the University of Queensland. We also have given references to published research using the methods we discuss.\u003c/p\u003e\n\u003cp\u003eLADAL has an overview of \u003ca href=\"https://slcladal.github.io/textanalysis.html\"\u003etext analysis and distant reading\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"counting-words\"\u003eCounting Words\u003c/h3\u003e\n\u003ch4 id=\"id2\"\u003eWord frequency\u003c/h4\u003e\n\u003cp\u003eKnowing how frequently words occur in a text can already give us information about that text and frequency lists based on large corpora are a useful tool in themselves - you can \u003ca href=\"https://kilgarriff.co.uk/bnc-readme.html\"\u003edownload\u003c/a\u003e such lists for the (original) \u003ca href=\"http://www.natcorp.ox.ac.uk/\"\u003eBritish National Corpus\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eTracking changes in the frequency of use of words across time has become popular since Google’s \u003ca href=\"https://books.google.com/ngrams\"\u003en-gram viewer\u003c/a\u003e has been available. However, results from this tool have to treated with caution for reasons set out in this \u003ca href=\"https://broadstreet.blog/2021/08/11/bad-ngrams/\"\u003eblog-post\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eComparing patterns of word frequency across texts can be part of authorship attribution. Patrick Juola \u003ca href=\"https://languagelog.ldc.upenn.edu/nll/?p=5315\"\u003edescribes using this method\u003c/a\u003e when he tried to decide whether Robert Galbraith was really J.K Rowling.\u003c/p\u003e\n\u003cp\u003eThis paper uses frequency and concordance analysis, with Australian data:\u003c/p\u003e\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eBednarek, Monika. 2020. Invisible or high-risk: Computer-assisted discourse analysis of references to Aboriginal and Torres Strait Islander people(s) and issues in a newspaper corpus about diabetes. \u003ci\u003ePLoS ONE\u003c/i\u003e 15/6: e0234486. \u003ca href=\"https://doi.org/10.1371/journal.pone.0234486\" target=\"_blank\"\u003ehttps://doi.org/10.1371/journal.pone.0234486\u003c/a\u003e \u003c/font\u003e\u003c/div\u003e\n\u003cp\u003eThe ratio of types and tokens in a text has been used as a measure of lexical diversity in developmental and clinical studies as well as in stylistics. It has also been applied to theoretical problems in linguistics:\u003c/p\u003e\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eKettunen, Kimmo. 2014. Can Type-Token Ratio be Used to Show Morphological Complexity of Languages? \u003ci\u003eJournal of Quantitative Linguistics\u003c/i\u003e 21(3). 223–245. \u003ca href=\"https://doi.org/10.1080/09296174.2014.911506\" target=\"_blank\"\u003ehttps://doi.org/10.1080/09296174.2014.911506\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\n\u003ch4 id=\"id3\"\u003eConcordance\u003c/h4\u003e\n\u003cp\u003eA concordance allows the researcher to see all instances of a word or phrase in a text, neatly aligned in a column and with preceding and following context (see image below).\nConcordances are often a first step in analysis. The concordance allows a researcher to see how a word is used and in what contexts. Most concordancing tools allow sorting of results by either preceding or following words – the coloured text in the example below shows that in this case the results have been sorted hierarchically on the three following words. This possibility can help in discovering patterns of co-occurrence. Concordances are also very useful when looking for good examples to illustrate a point. (The type of display seen in the example is often referred to as KeyWord In Context – KWIC. There is a possibility of confusion here, as there is a separate analytic method commonly referred to as \u003ca href=\"#keywords\"\u003eKeywords\u003c/a\u003e.)\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/concordance.png\" alt=\"Example of a concordance\"\u003e\u003c/p\u003e\n\u003cp\u003e(The example here was produced by \u003ca href=\"https://www.laurenceanthony.net/software/antconc/\"\u003eAntconc\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003eThis \u003ca href=\"https://slcladal.github.io/kwics.html\"\u003etutorial\u003c/a\u003e from LADAL on concordancing uses a notebook containing R code as a method of extracting concordance data.\u003c/p\u003e\n\u003ch4 id=\"id4\"\u003eClusters and collocations\u003c/h4\u003e\n\u003cp\u003eTwo methods can be used for counting the co-occurrence of items in text.\nClusters (sometimes known as n-grams) are sequences of adjacent items. A bigram is a sequence of two items, a trigram (3-gram) is a sequence of three items and so on. n-grams are types made up of more than one item, and therefore we can count the number of tokens of each n-gram in texts. n-grams are also the basis for a class of language models. (Google created a very large data set of English n-grams in developing their language-based algorithms and this \u003ca href=\"https://storage.googleapis.com/books/ngrams/books/datasetsv3.html\"\u003edata is available\u003c/a\u003e.)\nCollocations are patterns of co-occurrence in which the items are not necessarily adjacent. An example of why this is important is verbs and their objects in English. The object of a verb is a noun phrase and in many cases the first item in an English noun phrase is a determiner. This means that for many transitive verbs, the bigram \u003cem\u003everb the\u003c/em\u003e will occur quite frequently. But it is much more interesting to know whether there are patterns relating verbs and the entities which are their objects.\nCollocation analysis uncovers such patterns by looking at co-occurrences within a window of a certain size, for example three tokens on either side of the target. Collocation analysis gives information about the frequency of the co-occurrence of words and also a statistical measure of how likely that frequency is, given the overall frequencies of the terms in the corpus. Measures commonly applied include \u003ca href=\"https://en.wikipedia.org/wiki/Mutual_information\"\u003eMutual Information\u003c/a\u003e scores and \u003ca href=\"https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood\"\u003eLog-Likelihood\u003c/a\u003e scores.\nCollocations can also tell us about the meanings of words. If a word has collocates which fall into semantically distinct groups, this can indicate ambiguity or polysemy. And if different words share patterns of collocation, this can be evidence that the words are at least partial synonyms.\u003c/p\u003e\n\u003cp\u003eThis graphic shows collocation relations in Darwin’s Origin of Species visualised as a network - the likelihood of a pair of words occurring in close proximity in the text is indicated by the weight of the line linking them:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/collocation_network.png\" alt=\"Collocation patterns in Origin of Species as a network\"\u003e\u003c/p\u003e\n\u003cp\u003eThis article uses bigram frequencies as part of an analysis of language change:\u003c/p\u003e\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eSchweinberger, Martin. 2021. Ongoing change in the Australian English amplifier system. \u003ci\u003eAustralian Journal of Linguistics\u003c/i\u003e 41(2). 166–194. \u003ca href=\"https://doi.org/10.1080/07268602.2021.1931028\" target=\"_blank\"\u003ehttps://doi.org/10.1080/07268602.2021.1931028\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\nAn article which uses concordances and collocation analysis:\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eBaker, Paul \u0026 Tony McEnery. 2005. A corpus-based approach to discourses of refugees and asylum seekers in UN and newspaper texts. \u003ci\u003eJournal of Language and Politics\u003c/i\u003e 4(2). 197–226.\u003c/font\u003e\u003c/div\u003e\n\u003cp\u003eThis research uses the discovery of shared patterns of collocation as evidence that the words are at least partial synonyms:\u003c/p\u003e\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eMcEnery, Tony \u0026 Helen Baker. 2017. \u003ci\u003eCorpus linguistics and 17th-century prostitution: computational linguistics and history\u003c/i\u003e (Corpus and Discourse. Research in Corpus and Discourse). London; New York, NY: Bloomsbury Academic. (especially chapters 4 and 5)\u003c/font\u003e\u003c/div\u003e\n\u003cp\u003eThis \u003ca href=\"https://slcladal.github.io/coll.html\"\u003etutorial\u003c/a\u003e from LADAL on analysing co-occurences and collocations uses a notebook containing R code as a method to extract and visualise semantic links between words.\u003c/p\u003e\n\u003ch4 id=\"id5\"\u003eKeywords\u003c/h4\u003e\n\u003cp\u003eKeyword analysis is a statistically robust method of comparing frequencies of words in corpora. It tells us which words are more frequent (or less frequent) than would be expected in one text compared to another text and gives an estimate of the probability of the result. Keyword analysis uses two corpora: a \u003cstrong\u003etarget\u003c/strong\u003e corpus, which is the material of interest, and a \u003cstrong\u003ecomparison\u003c/strong\u003e corpus. Frequency lists are made for each corpus and then frequency of individual types in each corpus are compared. Keywords are ones which occur more ( or less) frequently in the target corpus than expected given the reference corpus.\nThe \u003cstrong\u003ekeyness\u003c/strong\u003e of individual items is a quantitative measure of how unexpected the frequency is; chi-square is a one possible measure of this, but a log-likelihood measure is more commonly used. Positive keywords are words which occur more commonly than expected; negative keywords are words which occur less commonly than expected.\u003c/p\u003e\n\u003cp\u003eThis visualisation shows a comparison of positive distinguishing words for three texts (Charles Darwin’s \u003cem\u003eOrigin\u003c/em\u003e, Herman Melville’s \u003cem\u003eMoby Dick\u003c/em\u003e, and George Orwell’s \u003cem\u003e1984\u003c/em\u003e), words that occur more commonly than we expect in one text when taking the other two texts as a comparison:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/keywords.png\" alt=\"Keywords from three texts\"\u003e\u003c/p\u003e\n\u003cp\u003eThis paper applies keyword analysis to Australian text data sourced from a television series script:\u003c/p\u003e\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eBednarek, Monika. 2020. Keyword analysis and the indexing of Aboriginal and Torres Strait Islander identity: A corpus linguistic analysis of the Australian Indigenous TV drama Redfern Now. \u003ci\u003eInternational Journal of Corpus Linguistics\u003c/i\u003e 25/4: 369-99. \u003ca href=\"http://doi.org/10.1075/ijcl.00031.bed\" target=\"_blank\"\u003ehttp://doi.org/10.1075/ijcl.00031.bed\u003c/a\u003e\u003c/font\u003e\u003c/div\u003e\n\u003cp\u003eTony McEnery describes using the keyword analysis method to compare four varieties of English in this chapter:\u003c/p\u003e\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eMcEnery, Tony. 2016. Keywords. In Paul Baker \u0026 Jesse Egbert (eds.), *Triangulating methodological approaches in corpus-linguistic research* (Routledge Advances in Corpus Linguistics 17), 20–32. New York: Routledge.\u003c/font\u003e\u003c/div\u003e\n\u003cp\u003eThis article explores how to assess Shakespeare’s use of words to build characters by using keyword analysis of the characters' dialog:\u003c/p\u003e\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eCulpeper, Jonathan. 2002. Computers, language and characterisation: An analysis of six characters in Romeo and Juliet. In \u003ci\u003eConversation in Life and in Literature: Papers from the ASLA Symposium\u003c/i\u003e (Association Suedoise de Linguistique Appliquee (ASLA) 15), 11–30. Uppsala: Universitetstryckeriet. (\u003ca href=\"https://lexically.net/wordsmith/corpus_linguistics_links/Keywords-Culpeper.pdf\" target=\"_blank\"\u003epdf\u003c/a\u003e)\u003c/font\u003e\u003c/div\u003e\n\u003ch3 id=\"classification\"\u003eMore complex methods – Classification\u003c/h3\u003e\n\u003cp\u003eClassification methods aim to assign some unit of analysis, such as a word or a document, to a class. For example, a document (or a portion of a document) can be classified as having positive or negative sentiment. These methods are all examples of supervised machine learning. An algorithm is trained on the basis of annotated data to identify classifiers in the data – features which correlate in some way with the annotated classifications. If the algorithm achieves good results on testing data (classified by human judgment), then it can be used to classify unannotated data.\u003c/p\u003e\n\u003ch4 id=\"id6\"\u003eDocument Classification\u003c/h4\u003e\n\u003cp\u003eThe task here is to assign documents to categories automatically. An everyday example of this procedure is spam filtering of email that may be applied by internet service providers and also within email applications. An example of this technique being used in research would be automatically identifying historical court records as referring either to violent crimes, property offences, or other crimes.\u003c/p\u003e\n\u003cp\u003eThe following two articles taken together give an account of a technique for partially automating the training phase of classification and then of how the classifiers allowed researchers to access new information in a large and complex text.\u003c/p\u003e\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eLeavy, Susan, Mark T Keane \u0026 Emilie Pine. 2019. Patterns in language: Text analysis of government reports on the Irish industrial school system with word embedding. \u003ci\u003eDigital Scholarship in the Humanities\u003c/i\u003e 34(Supplement_1). i110–i122. \u003ca href=\"https://doi.org/10.1093/llc/fqz012\" target=\"_blank\"\u003ehttps://doi.org/10.1093/llc/fqz012\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003ePine, Emilie, Susan Leavy \u0026 Mark T. Keane. 2017. Re-reading the Ryan Report: Witnessing via Close and Distant Reading. \u003ci\u003eÉire-Ireland\u003c/i\u003e 52(1–2). 198–215. https://doi.org/10.1353/eir.2017.0009. (\u003ca href=\"https://researchrepository.ucd.ie/handle/10197/10287\" target=\"_blank\"\u003eavailable online\u003c/a\u003e)\u003c/font\u003e\u003c/div\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Document_classification\"\u003eWikipedia\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"id7\"\u003eSentiment analysis\u003c/h4\u003e\n\u003cp\u003eSentiment analysis assigns documents according to the affect which they express. In simple cases, this can mean sorting documents into those which a express a positive view and those which express a negative view (with a neutral position sometimes also included). Such classifications are the basis for aggregated ratings - for example, online listings of movies and restaurants. A sentiment value is assigned to individual reviews then an aggregate score is calculated based on those values and that aggregate score is the rating presented to the user. More sophisticated sentiment analysis can assign values on a scale. Some sentiment analysis tools use dictionaries of terms with sentiment values assigned to those terms; these are known as pre-trained or pre-determined classifiers.\u003c/p\u003e\n\u003cp\u003eThe following figure shows the results of the sentiment analysis of four texts (\u003cem\u003eThe Adventures of Huckleberry Finn\u003c/em\u003e by Mark Twain, \u003cem\u003e1984\u003c/em\u003e by George Orwell, \u003cem\u003eThe Colour out of Space\u003c/em\u003e by H.P.Lovecraft, and \u003cem\u003eOn the Origin of Species\u003c/em\u003e by Charles Darwin) using the Word-Emotion Association Lexicon (Mohammad and Turney 2013). The graphic shows what percentage of each text can be assigned to each of eight categories of sentiment:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/sentiment_analysis.png\" alt=\"Sentiment analysis of four texts\"\u003e\u003c/p\u003e\n\u003cp\u003eThe Wikipedia entry for \u003ca href=\"https://en.wikipedia.org/wiki/Sentiment_analysis\"\u003eSentiment Analysis\u003c/a\u003e gives more information and examples particularly in relation to the use of sentiment analysis as a tool used in online settings.\u003c/p\u003e\n\u003cp\u003eLADAL’s \u003ca href=\"https://slcladal.github.io/sentiment.html\"\u003eSentiment Analysis tutorial\u003c/a\u003e uses a notebook containing R code as a method of performing sentiment analysis.\u003c/p\u003e\n\u003cp\u003eThis article discusses problems in assembling training data for complex sentiment analysis tasks and then applies the results to oral history interviews with Holocaust survivors:\u003c/p\u003e\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eBlanke, Tobias, Michael Bryant \u0026 Mark Hedges. 2020. Understanding memories of the Holocaust—A new approach to neural networks in the digital humanities. \u003ci\u003eDigital Scholarship in the Humanities\u003c/i\u003e 35(1). 17–33. \u003ca href=\"https://doi.org/10.1093/llc/fqy082\" target=\"_blank\"\u003ehttps://doi.org/10.1093/llc/fqy082\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\n\u003ch4 id=\"id8\"\u003eNamed Entity Recognition\u003c/h4\u003e\n\u003cp\u003eNamed Entity Recognition involves two levels of classification. First, segments of text are classified as either denoting or not denoting an entity: for example, a person, a place or an organization. The identified entities can then be classified as belonging to one of the types of entity.\u003c/p\u003e\n\u003cp\u003eThe Wikipedia entry explaining \u003ca href=\"https://en.wikipedia.org/wiki/Named-entity_recognition\"\u003enamed-entity recognition\u003c/a\u003e gives further detail about the technique.\u003c/p\u003e\n\u003cp\u003eThis article looks at the problems encountered when applying a well-known entity recognition package (\u003ca href=\"https://nlp.stanford.edu/software/CRF-NER.html\"\u003eStanford\u003c/a\u003e) to historical newspapers in the National Library of Australia’s Trove collection:\u003c/p\u003e\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eMac Kim, Sunghwan \u0026 Steve Cassidy. 2015. Finding names in Trove: Named Entity Recognition for Australian historical newspapers. In \u003ci\u003eProceedings of the Australasian Language Technology Association Workshop 2015\u003c/i\u003e, 57–65. (\u003ca href=\"https://aclanthology.org/U15-1007.pdf\" target=\"_blank\"\u003epdf\u003c/a\u003e)\u003c/font\u003e\u003c/div\u003e\n\u003cp\u003eThis article (section 6.3) discusses why entity recognition is not as useful as might be expected when studying names in novels:\u003c/p\u003e\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eDalen-Oskam, K. van. 2013. Names in novels: An experiment in computational stylistics. \u003ci\u003eLiterary and Linguistic Computing\u003c/i\u003e 28(2). 359–370. \u003ca href=\"https://doi.org/10.1093/llc/fqs007\" target=\"_blank\"\u003ehttps://doi.org/10.1093/llc/fqs007\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\n\u003ch4 id=\"id9\"\u003eComputational Stylistics (Stylometry)\u003c/h4\u003e\n\u003cp\u003eThis method is also referred to as authorship attribution as the classification task is to assess patterns of language use in order to decide whether to attribute a piece of text to a particular author (and with what degree of confidence). Seemingly simple classifiers are used for this task as they are assumed to be less open to conscious manipulation by writers. For example, comparative patterns of occurrence of function words such as \u003cem\u003ethe\u003c/em\u003e and \u003cem\u003ea/an\u003c/em\u003e are considered a better classifier than occurrences of content words. Character n-grams, that is sequences of characters of a specified length, have also proven to be a good classifier for use in this task. A recent example of these techniques being applied in a case which received a good deal of public attention was the controversy about whether \u003ca href=\"https://languagelog.ldc.upenn.edu/nll/?p=5315\"\u003eRobert Galbraith was really J.K Rowling\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe Wikipedia entry on \u003ca href=\"https://en.wikipedia.org/wiki/Stylometry\"\u003estylometry\u003c/a\u003e gives further information on the methodology.\u003c/p\u003e\n\u003cp\u003eThis article applies stylometric techniques to a classic of Chinese literature:\u003c/p\u003e\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eZhu, Haoran, Lei Lei \u0026 Hugh Craig. 2021. Prose, Verse and Authorship in Dream of the Red Chamber: A Stylometric Analysis. \u003ci\u003eJournal of Quantitative Linguistics\u003c/i\u003e 28(4). 289–305. \u003ca href=\"https://doi.org/10.1080/09296174.2020.1724677\" target=\"_blank\"\u003ehttps://doi.org/10.1080/09296174.2020.1724677\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\nAn overview of the use of function words in stylometry:\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eGarcia, A. M. \u0026 J. C. Martin. 2007. Function Words in Authorship Attribution Studies. \u003ci\u003eLiterary and Linguistic Computing\u003c/i\u003e 22(1). 49–66. \u003ca href=\"https://doi.org/10.1093/llc/fql048\" target=\"_blank\"\u003ehttps://doi.org/10.1093/llc/fql048\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\n\u003cp\u003eA classic stylometric study using Bayesian statistics rather than machine learning is:\u003c/p\u003e\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eMosteller, Frederick \u0026 David Lee Wallace. 1984. \u003ci\u003eApplied Bayesian and classical inference: the case of the Federalist papers\u003c/i\u003e. New York: Springer-Verlag.\u003c/font\u003e\u003c/div\u003e\n\u003ch3 id=\"others\"\u003eMore complex methods – Others\u003c/h3\u003e\n\u003ch4 id=\"id10\"\u003eTopic models\u003c/h4\u003e\n\u003cp\u003eTopic modeling is a method which tries to recover abstract ‘topics’ which occur in a collection of documents. The underlying assumption is that different topics will tend to be associated with different words, different documents will tend to be associated with different topics, and therefore the distribution of words across documents allows us to find topics. The complete model includes the strength of association (or probability) between each word and each topic, and between each topic and each document. A topic consists of a group of words and it is up to the researcher to decide if a semantically coherent interpretation can be given to any of the topics recovered. The number of topics to be recovered is specified in advance.\u003c/p\u003e\n\u003cp\u003eThe example visualisation below is based on topic modeling of the State of the Union addresses given by US presidents, and shows the relative importance of different topics over time. In the right hand part of the figure, the words most closely linked to each topic are listed; the researcher has not attempted to give labels to these (although in some cases, it is not too hard to imagine what labels we might use). Note also that words are not uniquely linked to topics - for example, the word \u003cem\u003estate\u003c/em\u003e is closely linked to seven of the topics in this model.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/topic_models.png\" alt=\"Topics in the State of the Union Address over time\"\u003e\u003c/p\u003e\n\u003cp\u003eThe Wikipedia entry for \u003ca href=\"https://en.wikipedia.org/wiki/Topic_model\"\u003etopic models\u003c/a\u003e gives a more detailed explanation of the process.\u003c/p\u003e\n\u003cp\u003eThis \u003ca href=\"https://slcladal.github.io/topicmodels.html\"\u003etopic modeling tutorial\u003c/a\u003e from LADAL uses R coding to process textual data and generate a topic model from that data.\u003c/p\u003e\n\u003cp\u003e\u003ci\u003ePoetics\u003c/i\u003e 41(6) is a journal issue devoted to the use of topic models in literary studies: the introduction to the journal (by Mohr and Bogdanov: \u003ca href=\"https://doi.org/10.1016/j.poetic.2013.10.001\" target=\"_blank\"\u003ehttps://doi.org/10.1016/j.poetic.2013.10.001\u003c/a\u003e) provides a useful overview of the method.\u003c/p\u003e\n\u003cp\u003eAnd this paper uses topic modeling as one tool in trying to improve access to a huge collection of scholarly literature:\u003c/p\u003e\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eMimno, David. 2012. Computational historiography: Data mining in a century of classics journals. \u003ci\u003eJournal on Computing and Cultural Heritage\u003c/i\u003e 5(1). 1–19. \u003ca href=\"https://doi.org/10.1145/2160165.2160168\" target=\"_blank\"\u003ehttps://doi.org/10.1145/2160165.2160168\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\n\u003ch4 id=\"id11\"\u003eNetwork Analysis\u003c/h4\u003e\n\u003cp\u003eNetwork analysis allows us to produce visualisations of the relationships between entities within a dataset. Analysis of social networks is a classic application of the method, but words and documents can also be thought of as entities and the relationships between them can then be analysed with this method. (see example visualisation of Darwin’s \u003cem\u003eOrigin of Species\u003c/em\u003e above)\nHere is another example of a network graph illustrating the relationships between the characters of Shakespeare’s \u003cem\u003eRomeo and Juliet\u003c/em\u003e:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/network_RandJ-1.png\" alt=\"Network of characters in Romeo and Juliet\"\u003e\u003c/p\u003e\n\u003cp\u003eThis article gives several examples of how representing collocational links between words as a network can lead to insight into meaning relations:\u003c/p\u003e\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eBrezina, Vaclav, Tony McEnery \u0026 Stephen Wattam. 2015. Collocations in context: A new perspective on collocation networks. \u003ci\u003eInternational Journal of Corpus Linguistics\u003c/i\u003e 20(2). 139–173. \u003ca href=\"https://doi.org/10.1075/ijcl.20.2.01bre\" target=\"_blank\"\u003ehttps://doi.org/10.1075/ijcl.20.2.01bre\u003c/a\u003e. (pdf)\u003c/font\u003e\u003c/div\u003e\n\u003cp\u003eWikipedia has articles on network theory in \u003ca href=\"https://en.wikipedia.org/wiki/Network_theory\"\u003egeneral\u003c/a\u003e and on \u003ca href=\"https://en.wikipedia.org/wiki/Social_network_analysis\"\u003esocial network analysis\u003c/a\u003e.\nin particular.\u003c/p\u003e\n\u003cp\u003eLADAL’s tutorial on \u003ca href=\"https://slcladal.github.io/net.html\"\u003eNetwork Analysis\u003c/a\u003e introduces this method using R coding.\u003c/p\u003e\n\u003ch3 id=\"visualisation\"\u003eVisualisation\u003c/h3\u003e\n\u003cp\u003eVisualisation is an important technique for exploring data, allowing us to see patterns easily, and also for presenting results.\nThere are many methods for creating visualisations and this article gives an overview of some possibilities for visualising corpus data:\u003c/p\u003e\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eSiirtola, Harri, Terttu Nevalainen, Tanja Säily \u0026 Kari-Jouko Räihä. 2011. Visualisation of text corpora: A case study of the PCEEC. \u003ci\u003eHow to Deal with Data: Problems and Approaches to the Investigation of the English Language over Time and Space\u003c/i\u003e. Helsinki: VARIENG 7. \u003ca href=\"https://varieng.helsinki.fi/series/volumes/07/siirtola_et_al/index.html\" target=\"_blank\"\u003e[html]\u003c/a\u003e\u003c/font\u003e\u003c/div\u003e\n\u003cp\u003eIf you would like to see something more complex, this article includes animations showing change in use of semantic space over time – but you need to have full access to the online publication to see it.\u003c/p\u003e\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eHilpert, Martin \u0026 Florent Perek. 2015. Meaning change in a petri dish: constructions, semantic vector spaces, and motion charts. \u003ci\u003eLinguistics Vanguard\u003c/i\u003e 1(1). \u003ca href=\"https://doi.org/10.1515/lingvan-2015-0013\" target=\"_blank\"\u003ehttps://doi.org/10.1515/lingvan-2015-0013\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e \n\u003cp\u003eThis LADAL tutorial on \u003ca href=\"https://slcladal.github.io/introviz.html\"\u003edata visualisation\u003c/a\u003e in R makes use of the \u003ca href=\"https://ggplot2.tidyverse.org/\"\u003eggplot2\u003c/a\u003e package to create some common data visualisations using code.\u003c/p\u003e\n","searchContent":{"posts":[{"title":"What are the FAIR and CARE principles and why should corpus linguists know about them?","slug":"fair-and-care","tags":["FAIR","CARE"],"content":"# FAIR and CARE\nData is becoming increasingly important in today’s world, so corpus linguists might feel that the rest of the world is finally catching up. But the rest of the world are bringing with them new approaches to how data is handled. This means that fields such as corpus linguistics may need to reassess their practices. Such reassessment includes addressing concerns about how data is stored and who can access it (data stewardship) – concerns that are a part of the [Open Science](https://en.wikipedia.org/wiki/Open_science) movement, ultimately grounded on principles of equity and accountability. \n\nThe most influential approach to data stewardship today is the [FAIR](https://www.go-fair.org/) principles.\nAccording to these principles, data should be:\n- *Findable* \n\u003cbr /\u003e\n\u0026emsp; Metadata and data should be easy to find for both humans and computers. \n- *Accessible*\n\u003cbr /\u003e\n\u0026emsp; Once the user finds the required data, she/he/they need to know how can they be accessed, possibly including authentication and authorisation.\n- *Interoperable*\n\u003cbr /\u003e\n\u0026emsp; The data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing.\n- *Reusable*\n\u003cbr /\u003e\n\u0026emsp; The ultimate goal of FAIR is to optimise the reuse of data. To achieve this, metadata and data should be well-described so that they can be replicated and/or combined in different settings.\n\u003cbr /\u003e\u003cbr /\u003e\n\nIn general, corpus linguists do well on the interoperability criterion. Corpus data is usually stored in non-proprietary formats; even when some structure is imposed on the data, this is almost always in a form which is saved as a simple text file (e.g. csv files or xml annotations). Data stored in such formats is easy to move between applications. But what about the other three criteria? \n\nSome corpus data is easy to discover; it is findable. For example CLARIN, the [portal](https://www.clarin.eu/content/data) to the European Union language resource infrastructure, provides access to many large data collections, as does the [Linguistic Data Consortium](https://www.ldc.upenn.edu/) in the USA. However, some data is never made part of a large collection and often remains under the control of individual researchers or research teams. Such data may be almost impossible to find. Even if we can find such data, it is unlikely to be accompanied by good descriptions of the data and metadata, making reusability problematic. Of course, big corpora such as the [British National Corpus](http://www.natcorp.ox.ac.uk/) will be both findable and accompanied by comprehensive corpus manuals. However, it is worth considering how to make other corpora more findable, including the provision of corpus manuals or corpus descriptions. Corpus resource databases such as [CoRD](https://varieng.helsinki.fi/CoRD/) do aim to work towards this principle.\n\nAccessibility may also be an issue for some data. Copyright law may allow use of material for individual research but prohibit any further distribution of the material. The FAIR approach to such cases is that metadata should be available so that interested parties can know that a data holding exists (F), and the metadata will include information about the conditions under which the data may or may not be shared or reused (A and R). \n\n![FAIR and CARE principles](/fair-care.png)\n\nImage from Global Indigenous Data Alliance (https://www.gida-global.org/)\n\u003cbr /\u003e\u003cbr /\u003e\nFor linguists, there is another very important set of principles concerning data, the CARE principles developed by the Global Indigenous Data Alliance:\n\u003cbr /\u003e\n- *Collective Benefit*\n\u003cbr /\u003e\n\u0026emsp; Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data.\n\u003cbr /\u003e\n- *Authority to control*\n\u003cbr /\u003e\n\u0026emsp;Indigenous Peoples’ rights and interests in Indigenous data must be recognised and their authority to control such data be empowered.\n\u003cbr /\u003e\n- *Responsibility*\n\u003cbr /\u003e\n\u0026emsp;Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit.\n\u003cbr /\u003e\n- *Ethics*\n\u003cbr /\u003e\n\u0026emsp;Indigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem.\n\n\u003cbr /\u003e\nThese principles are presented as applying particularly to Indigenous data, but we believe that researchers should adopt this approach in all cases where the people who participate in our research can be seen to have some moral rights in the information they have contributed. Respecting those moral rights should be demonstrated by recognising the participants’ authority to control how data is used, by seeking to ensure that participants derive benefit from use of the data, and by acting ethically and transparently in our relations with the participants. Deborah Cameron and her colleagues (Cameron et al 1993) raised similar issues almost 20 years ago, arguing that the imbalance of power in the relation between researchers and participants needed to be reduced. The CARE principles continue along this path, but go even further in explicitly returning power to the sources of information. \n\nCorpus data is often written language. We have already mentioned that copyright law is relevant to some such material, and that body of law protects at least some rights for the creators of the material. But corpus linguists also work with other kinds of data such as spoken language (spontaneous or produced as a response to some prompt) or written material produced by research participants according to some protocol. In such cases, ethical research practice should include addressing the issues raised by the CARE principles. Some aspects of this practice will fall under institutional ethics requirements (for example, thinking carefully about what permissions we request on consent forms), but other questions must be part of the relationship between the researcher and the research participants. Corpus linguists working with spoken, computer-mediated, or otherwise particularly sensitive data have been aware of at least some of these issues, but the CARE principles offer an opportunity to go further.\n\nAcquiring data for linguistic research takes effort and often that means money. It is therefore a good use of resources if any data we collect can be used by others. The FAIR principles provide a framework to make sharing and reusing data easier, and applying the CARE principles where relevant helps to ensure that our research has a sound ethical basis.\n\n\u003cbr /\u003e\n\u003chr /\u003e\n\u003cbr /\u003e\n\nNote: This post is based on the presentation ‘Advance Australia FAIR’,  given by Simon Musgrave and Michael Haugh to the 4th Forum on Englishes in Australia (LaTrobe University, August 27, 2021). \n\n\u003cbr /\u003e\n\nThanks to Leah Gustafson and Monika Bednarek for helpful comments on drafts.\n\n\u003cbr /\u003e\n\n**Reference:**\nCameron, Deborah, Elizabeth Frazer, Penelope Harvey, Ben Rampton \u0026 Kay Richardson. 1993. Ethics, advocacy and empowerment: Issues of method in researching language. Language \u0026 Communication 13(2). 81–94. [https://doi.org/10.1016/0271-5309(93)90001-4](https://doi.org/10.1016/0271-5309(93)90001-4)\n\n\n\n"},{"title":"Textbooks, tutorials and tool chests: different ways to learn from notebooks - Part 1","slug":"notebooks-part-1","tags":["Jupyter","notebooks","text analysis"],"content":"# Textbooks, tutorials and tool chests: different ways to learn from notebooks - Part 1\n\n### Written by Joel Nothman\n#### This work was supported by the [Sydney Informatics Hub](https://www.sydney.edu.au/research/facilities/sydney-informatics-hub.html), a Core Research Facility of the University of Sydney\n\nOne objective of the Australian Text Analytics Platform is to provide a library of code notebooks that our users – those interested in applying text analytics to research – can learn from and build upon. As the curators of such a resource, we need to learn from what’s been created before for a similar audience; and we need to understand how collections of notebooks can be used as an accessible resource for teaching, critiquing, and creating research using text analytics. These posts therefore summarise the insights we gained from a preliminary survey of key notebook resources.\n\nThis first part provides an introductory overview of the use of notebooks in text analytics and examines the pedagogically focussed genres of textbooks and tutorials. The second part looks at two other genres, tool chests and research articles, and ends with a summary and a consideration of how the discussion can inform the design of ATAP.\n\n## What are notebooks?\n\nNotebooks as popularised by the [Jupyter project](https://jupyter.org/) have become a key medium for experimentation and presentation in research and development. They mix code with narrative text, as well as tables, figures and interactive tools generated by the code.\n\n![Snippet from a notebook](/notebook.png)\n\n*Figure 1 - A screenshot of a part of a notebook*\n\nFigure 1 shows part of a notebook with some text, a code cell and the results generated by running the code\n\nA notebook can be played with interactively, allowing a user to interrogate and tweak the analysis and datasets under construction; but it can also be run as a prefabricated script that fetches a dataset, applies some processing, or generates a report. For researchers, published notebooks provide an opportunity for experimental reproducibility and reusable research pipelines.\n\nNotebooks collected together become a library of tutorials, tools, or reproducible research publications. We found several such collections covering applied text analytics, primarily targeting humanities and social sciences (HASS) researchers, as well as some resources that mix code and narrative, but in which the code cannot be executed directly. To make full use of such resources, it is necessary to copy the code and paste it into a console or another environment where it can be run.\n\nIn addition to having a mix of content, and different approaches to teaching or guiding readers to use text analytics tools, we noted a few broad genres of notebook and types of collection, such as tutorials, tool(kits), apps and research experiments, In this post, we focus on tutorials; the following post will look at the remaining genres..\nBefore going on, we’d like to note that our survey is by no means complete. We’ve not had time to look in detail at the full diversity of relevant notebook collections, but hope that we have captured useful insights about the state of the art.\n\n## Courses for different horses\n\nWhile the various resources all covered core applied text analytics concepts – including acquiring data from various sources, analysing n-gram frequencies, extracting named entities, building and evaluating a topic model, and so on – notebook resources reflected different genres and hence user needs. Some sought to present a rigorous curriculum constructed of tutorials or lessons, others treat collections of notebooks as tools for the user to employ in their work, and others demonstrate the use of a notebook as a single-purpose reproducible research article.\n\n### Courses and Textbooks\n\nThe vast majority of notebook collections we identified positioned themselves as courses, with each notebook corresponding to a tutorial. Some tutorials provide scaffolding for exercises (with or without solutions), but at a baseline, they guide readers/users through the use of a tool or concept. Frequently, the intention is for these tutorial notebooks to be walked through in a workshop, but being openly published as notebooks allows users to execute and play with them outside of the classroom.\n\n\u003ctable\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eGenre\u003c/b\u003e\u003c/td\u003e\u003ctd\u003eTutorial\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eTypical Structure\u003c/b\u003e\u003c/td\u003e\u003ctd\u003e\u003cul\u003e\u003cli\u003eIntro to method\u003c/li\u003e\n\u003cli\u003ePrepare data\u003c/li\u003e\n\u003cli\u003eApply method\u003c/li\u003e\n\u003cli\u003eExplore parameters\u003c/li\u003e\n\u003cli\u003eAnalyse results\u003c/li\u003e\n\u003cli\u003eExercises / next steps\u003c/li\u003e\u003c/ul\u003e\n\n\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eStrengths\u003c/b\u003e\u003c/td\u003e\u003ctd\u003eAddresses weak skills in methods, tooling, and/or coding ability.\n\nA tutorial in a notebook allows the user to interject with exploratory code or exercises.\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eWeaknesses\u003c/b\u003e\u003c/td\u003e\u003ctd\u003eA tutorial may not show its user how to build the notebook they’re viewing.\n\nNot demonstrative of application to a research goal.\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eExample\u003c/b\u003e\u003c/td\u003e\u003ctd\u003e\u003ca href='https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/09-Topic-Modeling-Without-Mallet.html'\u003eTopic Modeling Without Mallet\u003c/a\u003e\u003c/td\u003e\u003c/tr\u003e\n\u003c/table\u003e\n\nTutorials from different authors vary in how much they surround their code with narrative and background explanations of the techniques they are applying. Sinclair and Rockwell’s [The Art of Literary Text Analysis](https://github.com/sgsinclair/alta/blob/master/ipynb/ArtOfLiteraryTextAnalysis.ipynb) (ALTA) provides extensive narrative and appears more like a textbook than a course, not intended for delivery, but for learning and reference. Melanie Walsh’s [Introduction to Cultural Analytics \u0026 Python](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/04-Sentiment-Analysis.html) explicitly presents itself as a text book constituted of notebooks, relying on the [Jupyterbook](https://jupyterbook.org/intro.html) tool to give a glossy, accessible presentation, while also supporting the user to interact with the notebooks. Not unlike ALTA, this course demonstrates a deep appreciation of the breadth of technologies available and how they might be applied – and where they might not be applicable or reliable – in digital humanities contexts. It often supports the learner by giving multiple examples of how to apply a technique, which incidentally means repeated opportunities to demonstrate data preparation with diverse inputs.\n\nFor the R programming community, Martin Schweinberger’s [LADAL](https://slcladal.github.io/) shows a thoroughness in developing users’ abilities from statistical basics to a breadth of NLP, corpus and computational linguistic methods. (As part of the ATAP project, LADAL tutorials are undergoing conversion from R markdown to Jupyter notebooks.) Technique-oriented introductory tutorials are then complemented with “focus studies” on disciplines of linguistic analysis such as learner language, stylistics or linguistic typology.\nIn comparison to the above sites, several other tutorial resources are much more bare, simply demonstrating how to implement a technique with little use of discussion or repeated examples.\nThe varying levels of narrative, and how they present the code, suggests that authors have a mix of goals that we might query:\n- Is the tutorial introducing a technique, or rather its use through code?\n- Is it teaching the application of a technique, or how it works?\n- Does it aim to teach the reader how to read the code? How to experiment with the code? How to compose that code themselves?\n- Does the notebook itself facilitate those engagements with code, or does the more ephemeral teacher?\n\nAlthough not recently updated, we are indebted to Quinn Dombrowski’s curation of a [list of relevant courses and tutorials](https://github.com/quinnanya/dh-jupyter#course-materials), among other digital humanities notebooks. A recent player in this space is [TAPI (Text Analysis Pedagogy Institute)](https://labs.jstor.org/tapi/), which places open publication of notebook-based courses at the core of their researcher schooling. TAPI incorporates several courses by individual presenters, although there does not currently appear to be a high level of standardisation among the TAPI courses, including in their licensing.\n[The Programming Historian](https://programminghistorian.org/) is also an excellent resource for peer-reviewed digital humanities tutorials, often helpfully grounded in use cases. Not being notebooks (nor R markdown), its tutorials can’t be directly executed, leaving the user to copy-paste its code or not engage interactively with its content. What sets the Programming Historian apart is its design as a journal of tutorials, i.e. a collaborative enterprise in text analytics pedagogy. While resources we surveyed are all essentially open to third party contributors who may offer amendments, most collections of tutorial notebooks have single or few authors and are not designed as primarily collaborative endeavours.\n\n[Part 2](/posts/notebooks-part-2) of this post looks at the remaining genres we have identified, libraries and tool chests and presentations of research results. Part 2 also includes a discussion of how the information collected here might shape our view of what ATAP can be.\n\n\n\n\nNote 1: Unfortunately, we confined our survey to English-language resources; and with a focus on the Notebook medium, our analysis is biassed to (but not exclusively) the Python programming language.\n\n\n\n\n"},{"title":"Textbooks, tutorials and tool chests: different ways to learn from notebooks - Part 2","slug":"notebooks-part-2","tags":["Jupyter","notebooks","text analysis"],"content":"# Textbooks, tutorials and tool chests: different ways to learn from notebooks - Part 2\n### More genres of notebooks and what it all means for ATAP\n\n### Written by Joel Nothman\n#### This work was supported by the [Sydney Informatics Hub](https://www.sydney.edu.au/research/facilities/sydney-informatics-hub.html), a Core Research Facility of the University of Sydney\n\n[Part 1](/posts/notebooks-part-1) of this post introduced notebooks in general and identified various broad genres of notebook which have been used in providing text analytic tools. That part looked at notebooks as tutorials and the grouping of such materials into courses or textbooks. In this part, we look at notebooks which introduce individual tools and notebooks which present research results. The post ends with a consideration of how our analysis of different genres of notebook might influence the design of ATAP.\n\n### Libraries and tool chests\n\nA “course” comes with a sense of order and completeness; a “tutorial” with a sense of guidance and demonstration. Some notebook collections appear more designed for reference or application than teaching, as if each notebook was a tool you could take from a chest and utilise for your needs.\n\n\u003ctable\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eGenre\u003c/b\u003e\u003c/td\u003e\u003ctd\u003eTOOLKIT\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eTypical Structure\u003c/b\u003e\u003c/td\u003e\u003ctd\u003e(Idealised! There are few examples of this done thoroughly.)\u003cul\u003e\u003cli\u003ePointers to background reading\u003c/li\u003e\n\u003cli\u003eHow to load data\u003c/li\u003e\n\u003cli\u003eHow to set parameters\u003c/li\u003e\n\u003cli\u003eApply method\u003c/li\u003e\n\u003cli\u003eValidate results\u003c/li\u003e\n\u003cli\u003eQuestions to consider\u003c/li\u003e\n\u003cli\u003eAnalyse results\u003c/li\u003e\n\u003cli\u003eSave output\u003c/li\u003e\u003c/ul\u003e\n\n\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eStrengths\u003c/b\u003e\u003c/td\u003e\u003ctd\u003eOriented around decision making, critical application, multiple pathways or uses, and iterative improvement of methods.\n\nMeaningful scope for each notebook.\n\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eWeaknesses\u003c/b\u003e\u003c/td\u003e\u003ctd\u003eSome processing steps will be optional, notebooks are not very suitable for.\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eExample\u003c/b\u003e\u003c/td\u003e\u003ctd\u003e\u003ca href='https://nbviewer.org/github/GLAM-Workbench/pm-transcripts/blob/master/harvest_transcripts.ipynb'\u003eGLAM Workbench - Harvest Transcripts\u003c/a\u003e\u003c/td\u003e\u003c/tr\u003e\n\u003c/table\u003e\n\nA notable collection in this space is Tim Sherratt’s [GLAM Workbench](https://glam-workbench.net/). Tim often titles a directory of notebooks as “tools, tips and examples”, which nicely summarises the mix of content that users can retrieve from the Workbench. “Tools” might include a [notebook for acquiring Australian parliament’s Hansard transcripts](https://nbviewer.org/github/GLAM-Workbench/australian-commonwealth-hansard/blob/master/Harvesting-Commonwealth-Hansard.ipynb), offering parameters that the user might change for their own project; or an [app that summarises query results](https://github.com/GLAM-Workbench/trove-newspapers/blob/master/querypic.ipynb) from the National Library of Australia’s [Trove](https://trove.nla.gov.au/). An app, here, is distinguished by providing interactive widgets within a notebook, rather than expecting the user to set parameters in code and hit “run”. “Examples” includes a [case study](https://nbviewer.org/github/GLAM-Workbench/ozglam-workbench-naa-wap/blob/master/RecordSearch/2.%20Analyse%20a%20series.ipynb) of filtering a corpus by its metadata, while a [walkthrough on language detection](https://github.com/GLAM-Workbench/trove-newspapers/blob/master/find-non-english-newspapers.ipynb) applied to Trove incorporates many “tips” which help to identify the author’s thinking as his code takes each turn. Case studies like these lie on a spectrum between the genericity of tutorials often applied to demonstration or toy data, and the hypothesis-driven specificity of a research article. Grounding a topic in research use cases may help to evoke a sense of relevance to the user.\n\n\u003ctable\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eGenre\u003c/b\u003e\u003c/td\u003e\u003ctd\u003eAPP\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eTypical Structure\u003c/b\u003e\u003c/td\u003e\u003ctd\u003e\u003cul\u003e\u003cli\u003eEnter parameters / upload data\u003c/li\u003e\n\u003cli\u003eSee analysis\u003c/li\u003e\n\u003cli\u003eInteract\u003c/li\u003e\n\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eStrengths\u003c/b\u003e\u003c/td\u003e\u003ctd\u003eUser friendly way to analyse text.\n\nNotebooks enable rapid development of tools.\n\n\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eWeaknesses\u003c/b\u003e\u003c/td\u003e\u003ctd\u003eDiscourages understanding of the internal workings. Comes with similar constraints as desktop tools, including limited reproducibility.\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eExample\u003c/b\u003e\u003c/td\u003e\u003ctd\u003e\u003ca href='https://github.com/GLAM-Workbench/trove-newspapers/blob/master/querypic.ipynb'\u003eVisualise searches in Trove's newspapers and gazettes\u003c/a\u003e\u003c/td\u003e\u003c/tr\u003e\n\u003c/table\u003e\n\nNotebooks presented as reusable tools may also be very similar to tutorials. Until recently, Ithaka’s [Constellate.org](https://constellate.org/) – a core part of TAPI – included tutorial notebooks, and corresponding editions of the same notebook “for research”. (Here’s a [tutorial](https://github.com/ithaka/tdm-notebooks/blob/ea3c48e0094303a7d40dec4890c6cfdca275d22b/finding-significant-terms.ipynb) and [research](https://github.com/ithaka/tdm-notebooks/blob/ea3c48e0094303a7d40dec4890c6cfdca275d22b/finding-significant-terms-for-research.ipynb) pair on finding key terms in a corpus.) Each “research” notebook omits some of the guidance included in the tutorial, and was rather intended to be applied to a user-supplied dataset. The removal of these applied notebooks notwithstanding, it appears that the Constellate tutorials should dually function as reusable tools. The Constellate platform allows the user to create a corpus from JSTOR content, and select a tutorial/tool notebook from their collection, which is then applied to the new corpus. An important usability feature of reusable tools, and tutorials with this capability, is how they guide or support the user to apply the notebook to their own data.\n\nAlthough targeting experienced corporate data scientists, rather than HASS researchers, data infrastructure provider Databricks openly publishes another relevant tool chest of notebooks (albeit not designed for the Jupyter platform). Their [Solution Accelerators](https://databricks.com/solutions/accelerators) are designed to support a fairly code-savvy analyst in tackling standard data science problems in industry. (Read “Solution Accelerator” as a less patronising, more businessy, revamp of the term “starter kit”.) A Solution Accelerator notebook mixes narrative and code, inviting the user to bring their own data. It may give them data transformation code, models to apply, questions to ask, and diagnostics to perform. The library of Solution Accelerators shows how even a capable user may appreciate a notebook as a launchpad when working with new techniques. The apparent value of Solution Accelerators to Databricks customers highlights the risk and cost of undertaking analysis without workflow guidance from a notebook.\n\nUnlike tutorials, a notebook tool chest does not make great efforts to build up a user’s skills from scratch, but supports them in selecting a tool that might be appropriate, seeing its relevance, and applying it to their needs.\n\n### Journals and Anthologies\n\nThrough their mix of code, narrative and generated tables and figures, notebooks are a powerful tool for presenting reproducible experimental results. A research paper implemented as a notebook may be rebuilt from its code, reproducing tables, figures and examples as the researcher modifies it. An article-as-notebook is distinguished by its focus on a specific research question, and application of whichever tools are appropriate to draw reliable conclusions. Although individual humanities researchers using digital methods might publish their research as code notebooks (see Quinn Dombrowski’s [list](https://github.com/quinnanya/dh-jupyter#research--projects) for instance), institutional or discipline-specific collections of research notebooks are not yet common in applied text analysis and HASS. For comparison, a move towards reproducibility in other sciences allows one to find a multitude of research notebooks in venues like [paperswithcode.com](https://paperswithcode.com/) and [Code Ocean](https://codeocean.com/explore/capsules).\n\n\u003ctable\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eGenre\u003c/b\u003e\u003c/td\u003e\u003ctd\u003eRESEARCH EXPERIMENT\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eTypical Structure\u003c/b\u003e\u003c/td\u003e\u003ctd\u003e\u003cul\u003e\u003cli\u003eIntroduction\u003c/li\u003e\n\u003cli\u003ePreparation\u003c/li\u003e\n\u003cli\u003eRepeat for each tool or step:\n\nModelling, Analysis, Interpretation\u003c/li\u003e\n\u003cli\u003eDiscussion\u003c/li\u003e\u003c/ul\u003e\n\n\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eStrengths\u003c/b\u003e\u003c/td\u003e\u003ctd\u003eDemonstrates application of tools to answer a question.\n\nDemonstrates combination of tools.\n\nEncourages reuse, reproducibility and open science.\n\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eWeaknesses\u003c/b\u003e\u003c/td\u003e\u003ctd\u003eMay not support the reader to adopt the techniques.\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd\u003e\u003cb\u003eExample\u003c/b\u003e\u003c/td\u003e\u003ctd\u003e\u003ca href='https://journalofdigitalhistory.org/en/article/4yxHGiqXYRbX'\u003eTopic Specific Corpus Building\u003c/a\u003e\u003c/td\u003e\u003c/tr\u003e\n\u003c/table\u003e\n\nWhen driven by a research question, the notebook author is less interested in demonstrating what techniques are *available* to apply, and more selective in using techniques that may helpfully harness their data towards a specific goal or narrative. The research thesis thus provides a way to specifically evaluate the output of an analysis: does it support a hypothesis? does it induce a new hypothesis? does it suggest problems in the modelling? The final research notebook may hide aspects of this iterative experimentation, evaluation, and the response of further investigation, issue mitigation, or discarding a path of investigation. (The ideal reproducible research notebook would keep track of dead ends too!)\nTutorials and tools may emulate these processes of iterative development, or provide a range of possible pathways for analysis; the research notebook, however, must keep its focus on the thesis being explored.\n\nThe [Journal of Digital History](https://journalofdigitalhistory.org/) is a new player in this space, releasing its first issue in October 2021. This project of the Luxembourg Centre for Contemporary and Digital History and De Gruyter ensures some reproducibility by requiring the code which constructs the research article to be executable and its data available to reproduce tables and figures, while allowing the reader to evaluate and reuse the research implementation. It goes further in harnessing the notebook medium to add layers of depth to peer-reviewed publications in digital history: viewing a JDH publication, a reader is able to hide what they call the “hermeneutic layer” of the work, which provides methodological detail, as distinct from the narrative arc of the research. The notebook medium applied in this way is able to shift several paradigms about how research is constructed, presented, and accessed.\n\n### Bringing it back to users\n\nIn positioning the Australian Text Analytics Platform’s role amidst ongoing and past resource creation, we have to consider how our library of notebooks might build outcomes for our users. Here are some outcomes we are considering.\n\n#### Reference and Application\n- User has access to starters’ kits for a wide range of techniques, covering data modelling, validation and analysis, archiving, etc.\n- User can experiment with standard tools applied to demo data or their own.\n- User can adapt code notebooks to their own use cases.\n- User is able to compare alternative methods for similar goals.\n\n#### Growth and Learning\n- User is familiar with what selected research methods look like as code.\n- User is familiar with what end-to-end research looks like as code.\n- User can prepare their own data for existing or standard tools.\n- User understands how their research problem might translate to a computational problem, and the applicable techniques.\n- User is able to combine multiple techniques to achieve their research goals.\n- User understands how to build good research software.\n- User can confidently build a notebook from scratch for their own research.\n\nWe also need to define who our user personas are: is it someone with academic expertise, but little in code? or someone with moderate code literacy, but stuck with how to create notebooks for reproducible research? The latter might be more interested in published starter kits, case studies and experiments, while the former needs the guidance provided by a tutorial.\n\n### Where to from here?\n\nBy surveying several collections of notebooks for text analytics, we have a stronger understanding of the spectrum of materials ATAP’s library can incorporate, both in content and in genre.\n\nOne apparent opportunity is to curate or improve a catalogue of existing resources that will meet the needs of the Australian text analytics research community. It’s also possible to see areas where existing resources may fail to meet the needs of current users of desktop and web tools for corpus analysis from [Voyant Tools](https://voyant-tools.org/) to [AntConc](https://www.laurenceanthony.net/software/antconc/); those tools provide interactivity that carries the user across views of statistics, query matches and texts, but tend to be limited in facilitated reproducible research and in extensibility to arbitrary corpus cleaning and analysis procedures. Tying the resources we’ve looked at to user outcomes also gives a better understanding how to evaluate accessibility of a notebook and the learning opportunities arising from it, such as to what extent a resource:\n- supports the user getting their own data into the notebook.\n- demonstrates the combination of multiple techniques into research.\n- helps a researcher choose techniques relevant to their research question.\n- supports the user to experiment with the code of the notebook\n- demonstrates the processes and challenges around using/applying a technique and the code needed to implement those decisions e.g. decisions to make before and after applying topic models. \n\nOverall, we’ve talked little about how notebooks present their content, and how that presentation might affect user engagement and the sustainability of the resource. Of course, those concerns are also important in looking at principles for quality in notebook resources for digital humanities and other text analytics users.\n\n"}],"pages":[{"title":"Preparing Text Data","slug":"data_prep","content":"\n### Introducing data preparation concepts \n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"data-prep-approach-time.png\" \n    alt=\"What data scientists spend the most time doing - pie chart\"  \n  /\u003e\n  \u003cfigcaption align = \"center\"\u003e\u003cb\u003eWhat data scientists spend the most time doing - pie chart\u003c/b\u003e\u003c/figcaption\u003e\n\u003c/figure\u003e\n \nThis graphic is, sadly, all too true. Data scientists and those who are using \ndata as part of their research spend much of their time preparing their \ndataset and transforming its structure into a format that can be used (often \nreferred to as [data wrangling](https://online.hbs.edu/blog/post/data-wrangling)\nor data munging). The Australian Text Analytics Platform will offer a range \nof tools to assist in cleaning text data and performing other preliminary \noperations which can prepare the data for analysis. ATAP analysis notebooks \nassume a common data structure, however the platform will provide notebooks \ncontaining code for transforming data into the structure that is needed for \nthe procedure(s) in the analysis notebooks.\n\nThere are two main processes that are needed to prepare text data for analysis\n, [cleaning](#common-cleaning-techniques) and [annotation](#annotation), and \nthe ones that you will need to use will depend on the dataset that you are \nusing. \n\n### Common Cleaning Techniques\n- **Making all of the text lower case:**\n  This ensures that e.g. *dog* and *Dog* will not be treated as different items\n  and is important if you are going to use analytic methods which rely on \n  counting items. However, if you are planning to extract entities from your \n  text data, retaining capital letters may be important.\n\n- **Standardising spelling:**\n  At least for English text, there are some well-known spelling variations, \n  some with a geographical context (*colour/color*) and some that are more a \n  matter of personal preference (*recognise/recognize*). As with case, \n  standardising spelling ensures that pairs like the examples are treated as \n  tokens of the same type.\n\n- **Removing stopwords:**\n  Stopwords are function words that are not interesting for many analyses and \n  we can safely remove them from our data using a stoplist. The 20 most \n  frequently occurring words in the \n  [British National Corpus](https://www.english-corpora.org/bnc/) are *the, of\n  , and, a, in, to, it, is, was, to, I, for, you, he, be, with, on, that, by\n  *, and *at*. The equivalent list for the Corpus of Contemporary American \n  English ([COCA](https://www.english-corpora.org/coca/)) is *the, be, and, of\n  , a, in, to, have, to, it, I, that, for, you, he, with, on, do, say* and *\n  this*. \n  Some of the differences between the two are because the COCA counts are of \n  lemmas (see Lemmatization below) which are the base forms of a word (think *\n  dog* and *dogs*). Packages such as [nltk](https://www.nltk.org/) and \n  [spaCy  ](https://spacy.io/) include standard stoplists for various \n  languages, and it is possible to specify other words to be excluded.\n\n- **Removing punctuation:**\n  Punctuation can change how a text analysis package identifies a word. For \n  instance to be sure that *dog* and *dog?* are not treated as different items,\n  removing punctuation is good practice.\n\n- **Removing numbers:** \n  Sometimes the presence of numbers in documents can lead to artefacts in \n  analysis. For example, in a collection of documents with page numbering, \n  the numbers might show up as collocates of words or as part of a topic in a \n  topic model. To avoid this, removing numbers is also good practice. This \n  can present a challenge where numbers might be of interest (e.g. a study of \n  mathematics textbooks).\n\n- **Removing whitespace:** \n  Whitespace can be another possible source of artefacts in analysis, \n  especially if the source material uses a lot of tabs.\n\n### Annotation\nAnnotation is the process of adding information to your base dataset in order \nto make it possible to apply analytic techniques. In some cases, this may be \na manual process. For example, much of the annotation which is described in \nthe Text Encoding Initiative [Guidelines](https://tei-c.org/guidelines/) \nrequires a human making decisions although, in some cases, manual annotation \nprocesses may also be scaled up to large text corpora using text \nclassification or information extraction technologies. \n\nHowever some annotation can be carried out automatically, and there are two \nimportant kinds of annotation for text which fall into this category.\n\n\n- **Part-of-speech tagging (POS-tagging):** \n  For some analytic procedures, knowing the part of speech (or class of words) \n  that an item belongs to is important. For languages where good pre-trained \n  models exist, this annotation can be carried out automatically to a high \n  level of accuracy – for English, we expect an accuracy rate better than 95%. \n  POS-taggers generally provide more information than just whether an item is a \n  noun or a verb, they also distinguish singular and plural forms of nouns and \n  tell us whether a verb’s form is present tense form or a past tense. The tag \n  sets which are used can therefore be quite extensive, and there are various \n  tag sets in use such as the \n  [Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n  tags and the [CLAWS](https://ucrel.lancs.ac.uk/claws5tags.html) tags used by \n  the British National Corpus.\n\n[LADAL](https://slcladal.github.io/) has some excellent resources that \ndiscuss POS tagging in more detail.\n\n- **Lemmatization:** \n  The distinctions between different forms of a single lexeme can be a \n  hindrance in analysis especially if we are interested in lexical semantics in \n  texts. Lemmatization identifies the base forms of words (or lemmas) in a text \n  so that all forms of an item are treated together. For example: *dog* and *dogs* \n  will both be instances of the lemma DOG. *eat, eats, eating* and *ate* \n  will all be treated as tokens of the lemma EAT. \n  As noted above, POS-tags give information about the form of words and are \n  generally part of the annotation in lemmatization. A lemma, along with a POS-\n  tag, can be reconstructed to the original form if \n  necessary.\n"},{"title":"Events","slug":"events","content":"# Events\n\n[Webinars](#webinars) \u0026emsp;\u0026emsp;\n[Forthcoming workshops](#forthcoming-workshops) \u0026emsp;\u0026emsp;\n[Previous workshops](#previous-workshops) \u0026emsp;\u0026emsp;\n[Office Hours](#office-hours)\n\n### Webinars {#webinars}\n\nOur webinar series is a joint initiative with the Language Technology and Data Analysis Laboratory ([LADAL](https://slcladal.github.io/index.html)), (School of Languages and Cultures, University of Queensland). LADAL sponsored [webinars](https://slcladal.github.io/webinars2022.html) take place in the alternate months.\n\nAll webinars take place at 8:00PM Brisbane time which is UTC+10. Zoom links will be available one week prior to the event.\n\n#### 4 April 2022 - Keoni Mahelona: A practical approach to Indigenous data sovereignty\nKeoni Mahelona is the Chief Technical Officer of [Te Hiku Media](https://tehiku.nz/) where he is a part of the team developing the Kaitiakitanga Licence. This licence seeks to balance the importance of publicly accessible data with the reality that indigenous peoples may not have access to the resources that enable them to benefit from public data. By simply opening access to data and knowledge, indigenous people could be further colonised and taken advantage of in a digital, modern world. Therefore Keoni is committed to devising data governance regimes which enable Indigenous people to reclaim and maintain sovereignty over indigenous data.\n\n#### June 6 2022 - Barbara McGillivray: The *Journal of Open Humanities Data*\nBarbara McGillivray is a Turing Research Fellow at [The Alan Turing Institute](https://www.turing.ac.uk/), and Editor in Chief of the [Journal of Open Humanities Data](https://openhumanitiesdata.metajnl.com/). Since September 2021 she is also a lecturer in Digital Humanities and Cultural Computation at the [Department of Digital Humanities of King's College London](https://www.kcl.ac.uk/ddh). Before joining the Turing, she was language technologist in the Dictionary division of Oxford University Press and data scientist in the Open Research Group of Springer Nature. Her research at the Turing is on how words change meaning over time and how to model this change in computational ways. She works on machine-learning models for the change in meaning of words in historical times (Ancient Greek, Latin, eighteen-century English) and in contemporary texts (Twitter, web archives, emoji). Her interdisciplinary contribution covers Data Science, Natural Language Processing, Historical Linguistics and other humanistic fields, to push the boundaries of what academic disciplines separately have achieved so far on this topic.\n\n[Zoom link](https://uqz.zoom.us/j/83999047730?from=addon)\n\n#### August 1 2022 - Václav Cvrček: The Czech national Corpus\n[Václav Cvrček](https://ucnk.ff.cuni.cz/en/institute/people/vaclav-cvrcek-2/) is a linguist who deals with the description of the Czech language, especially with the use of large electronic corpora and quantitative methods. In 2013-2016 he worked as the director of the [Czech National Corpus](https://ucnk.ff.cuni.cz/en/) project, since 2016 he has been the deputy director. Recently, he has been focusing on research on textual variability and corpus-based discourse analysis with a focus on online media.\n\n[Zoom link](https://uqz.zoom.us/j/81439620559?from=addon)\n\n\n#### October 3 2022 - Paweł Kamocki: [topic tba]\nPaweł Kamocki is a legal expert in Leibniz-Institut für Deutsche Sprache, Mannheim. He studied linguistics and law, and in 2017 obtained his doctorate in law from the universities of Paris and Münster for a thesis on legal aspects of data-intensive university research, with a focus on Knowledge Commons. He worked as a research and teaching assistant at the Paris Descartes university (now: Université de Paris), then also in the private sector. He is certified to work as an attorney in France. An active member of the [CLARIN](https://www.clarin.eu/) community since 2012, he currently chairs the CLARIN Legal and Ethical Issues Committee. He also worked with other projects and initiatives in the field of research data policy (RDA, EUDAT) and co-created several LegalTech tools for researchers. One of his main research interests are legal issues in Machine Translation.\n\n[Zoom link](https://uqz.zoom.us/j/82090438697?from=addon)\n\n\n### Forthcoming workshops {#forthcoming-workshops}\n\n\n#### Showcasing Approaches to Digital Humanities for Researchers: Introduction to Jupyter Notebooks\n\nThis workshop will introduce you to Jupyter Notebooks, a digital tool that has exploded in popularity in recent years for those working with data – from STEM to HASS disciplines.\n\nYou will learn what they are, what they do and why you might like to use them. It is an introductory set of lessons for those who are brand new, have little or no knowledge of coding and computational methods in research. By the end of the workshop, you will have a good understanding of what Notebooks can do, how to open one up, perform some basic tasks and save it for later. If you are really into it, you will also be able to continue to experiment after the workshop by using other people’s notebooks as springboards for your own adventures!\n\nThis workshop is targeted at those who are absolute beginners or ‘tech-curious’, especially those in the humanities, arts and social science disciplines. It includes a hands-on component, using basic programming commands, but requires no previous knowledge of programming.\n\nThe workshop is presented by the [FAVeR](https://faver.edu.au/).\n\n**Date**: August 24 2022 \u003cbr\u003e\n**Time**: 14:00 - 16:00 Australian Eastern Standard Time \u003cbr\u003e\n**[Details and Registration](https://faver.edu.au/event/showcasing-approaches-to-digital-humanities-for-researchers-introduction-to-jupyter-notebooks/)** \n\n#### Computational Thinking in the Humanities\n\nThe workshop Computational Thinking in the Humanities is a 3-hour online workshop featuring two plenary talks, lightning presentations, as well as a panel discussion. The workshop is co-organized by the Australian Text Analytics Platform ([ATAP](https://www.atap.edu.au/)), [FIN-CLARIAH](https://www.kielipankki.fi/organization/fin-clariah/) and its UEF representatives, and the [Australian Digital Observatory](https://www.digitalobservatory.net.au/).\n\nThe workshop has received financial supported from the [Digital Cultures and Societies Hub](https://hass.uq.edu.au/Digital-Cultures-and-Societies) at the University of Queensland.\n\n**Date**: September 1 2022 \u003cbr\u003e\n**Time**: 17:00 - 20:00 Australian Eastern Standard Time \u003cbr\u003e\n**Zoom**: https://uqz.zoom.us/j/86311263161\n\n\n[Further details](https://ladal.edu.au/compthink.html)\n\n\n### Previous workshops {#previous-workshops}\n\n#### Network analysis and Topic Modeling on Twitter data using R\n\n**Date**: May 18 2022 \u003cbr\u003e\n**Event**: Joint event ADO and ATAP \u003cbr\u003e\n**Length**: 3 hours \u003cbr\u003e\n**Facilitators**: Alice Miller, Simon Musgrave\n\n#### Monotreme Mania! Comparative text analytics on Twitter data\n**Date**: 16 March 2022 \u003cbr\u003e\n**Event**: Joint event ADO and ATAP \u003cbr\u003e\n**Length**: 3 hours \u003cbr\u003e\n**Facilitators**: Sam Hames, Simon Musgrave\n\n#### An introduction to Jupyter notebooks for text analysis: Virtual workshop for absolute beginners\n\n**Date**: 27 July 2022 \u003cbr\u003e\n**Event**: Workshop for Sydney Corpus Lab \u003cbr\u003e\n**Length**: 3 hours \u003cbr\u003e\n**Facilitators**: Sara King, Simon Musgrave\n\n**Date**: 24 November 2021 \u003cbr\u003e\n**Event**: Digital Humanities Australasia 2021 Conference \u003cbr\u003e\n**Length**: 3 hours \u003cbr\u003e\n**Facilitators**: Sara King, Simon Musgrave\n\n\n### Office Hours {#office-hours}\n\nWe invite Australian researchers working with linguistics, text analytics, digital and computational methods, social media and web archives, and much more to attend our regular online office hours, jointly hosted with the [Digital Observatory](https://research.qut.edu.au/digitalobservatory/). Bring your technical questions, research problems and rough ideas and get advice and feedback from the combined expertise of our ARDC research infrastructure projects. No question is too small, and even if we don’t know the answer we are likely to be able to point you to someone who does.\n\nThese sessions run over Zoom from 2-3pm (Australia/Sydney time) every second Tuesday - [details](https://research.qut.edu.au/digitalobservatory/office-hours/).\n\n"},{"slug":"home","content":"\n__The Australian Text Analytics Platform is an open source environment \nthat provides researchers with tools and training for analysing, processing,\nand exploring text.__\n\nText analytics is a suite of methods which enable data-driven research \nby extracting and analysing machine-readable information from within \nunstructured text. Due to the increasing availability of large amounts of\nunstructured text, such techniques are becoming more and more important across\ndiverse research disciplines.\n\nText analysis tends to happen at either a basic, generic level (handled with \nexisting software packages) or with custom code specifically developed by \nprogrammers for a particular project. ATAP will support researchers \ntransitioning to code-based text analysis, with the resultant benefits of \nflexibility, reproducibility and reuse, and the possibility of exporting their \nresults and workflows as a fully documented research object.\n\nATAP will be a collaborative, cloud-based workbench environment, bringing \ntogether users and providers of data and text analytics tools. It will \nencourage researchers to adopt new methods, leading to greater flexibility and \ntransparency in research workflows. The platform will be accessible to \nresearchers with a broad range of experience and skills (including beginners) \nand across a range of disciplines. Support provided by the platform will \ninclude hands-on workshops, online training modules and online office hours, \nas well as advice and collaboration in selected partnerships.\n\n![ARDC logos](/AcknowledgeARDC.png)\n\nThe Australian Text Analytics Platform (ATAP) projects \n[received investment](https://doi.org/10.47486/PL074) from the Australian \nResearch Data Commons (ARDC). The ARDC is funded by the National Collaborative \nResearch Infrastructure Strategy (NCRIS).\n\nATAP acknowledges and pays respects to the Elders and Traditional Owners of the \nlands on which we live and work.\n"},{"title":"Useful Methods","slug":"methods","content":"\n\n[Counting words](#counting-words) \u0026emsp;\u0026emsp; \n[More complex methods - Classification](#classification) \u0026emsp;\u0026emsp; \n[More complex methods - Others](#others) \u0026emsp;\u0026emsp; \n[Visualisation](#visualisation)\n\n### Introduction\nThroughout this page, we have given links to further information in Wikipedia and in the tutorials provided by the Language Technology and Data Analysis Laboratory [LADAL](https://slcladal.github.io/) at the University of Queensland. We also have given references to published research using the methods we discuss.\n\nLADAL has an overview of [text analysis and distant reading](https://slcladal.github.io/textanalysis.html). \n\n\n\n### Counting Words {#counting-words}\n\n#### Word frequency\n\nKnowing how frequently words occur in a text can already give us information about that text and frequency lists based on large corpora are a useful tool in themselves - you can [download](https://kilgarriff.co.uk/bnc-readme.html) such lists for the (original) [British National Corpus](http://www.natcorp.ox.ac.uk/).\n\nTracking changes in the frequency of use of words across time has become popular since Google’s [n-gram viewer](https://books.google.com/ngrams) has been available. However, results from this tool have to treated with caution for reasons set out in this [blog-post](https://broadstreet.blog/2021/08/11/bad-ngrams/). \n\nComparing patterns of word frequency across texts can be part of authorship attribution. Patrick Juola [describes using this method](https://languagelog.ldc.upenn.edu/nll/?p=5315) when he tried to decide whether Robert Galbraith was really J.K Rowling.\n\n\nThis paper uses frequency and concordance analysis, with Australian data:\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eBednarek, Monika. 2020. Invisible or high-risk: Computer-assisted discourse analysis of references to Aboriginal and Torres Strait Islander people(s) and issues in a newspaper corpus about diabetes. \u003ci\u003ePLoS ONE\u003c/i\u003e 15/6: e0234486. \u003ca href=\"https://doi.org/10.1371/journal.pone.0234486\" target=\"_blank\"\u003ehttps://doi.org/10.1371/journal.pone.0234486\u003c/a\u003e \u003c/font\u003e\u003c/div\u003e\n\n\nThe ratio of types and tokens in a text has been used as a measure of lexical diversity in developmental and clinical studies as well as in stylistics. It has also been applied to theoretical problems in linguistics:\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eKettunen, Kimmo. 2014. Can Type-Token Ratio be Used to Show Morphological Complexity of Languages? \u003ci\u003eJournal of Quantitative Linguistics\u003c/i\u003e 21(3). 223–245. \u003ca href=\"https://doi.org/10.1080/09296174.2014.911506\" target=\"_blank\"\u003ehttps://doi.org/10.1080/09296174.2014.911506\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\n\n\n#### Concordance\nA concordance allows the researcher to see all instances of a word or phrase in a text, neatly aligned in a column and with preceding and following context (see image below).\nConcordances are often a first step in analysis. The concordance allows a researcher to see how a word is used and in what contexts. Most concordancing tools allow sorting of results by either preceding or following words – the coloured text in the example below shows that in this case the results have been sorted hierarchically on the three following words. This possibility can help in discovering patterns of co-occurrence. Concordances are also very useful when looking for good examples to illustrate a point. (The type of display seen in the example is often referred to as KeyWord In Context – KWIC. There is a possibility of confusion here, as there is a separate analytic method commonly referred to as [Keywords](#keywords).)\n\n![Example of a concordance](/concordance.png)\n\n(The example here was produced by [Antconc](https://www.laurenceanthony.net/software/antconc/)) \n\nThis [tutorial](https://slcladal.github.io/kwics.html) from LADAL on concordancing uses a notebook containing R code as a method of extracting concordance data.\n\n\n\n\n#### Clusters and collocations\nTwo methods can be used for counting the co-occurrence of items in text. \nClusters (sometimes known as n-grams) are sequences of adjacent items. A bigram is a sequence of two items, a trigram (3-gram) is a sequence of three items and so on. n-grams are types made up of more than one item, and therefore we can count the number of tokens of each n-gram in texts. n-grams are also the basis for a class of language models. (Google created a very large data set of English n-grams in developing their language-based algorithms and this [data is available](https://storage.googleapis.com/books/ngrams/books/datasetsv3.html).)\nCollocations are patterns of co-occurrence in which the items are not necessarily adjacent. An example of why this is important is verbs and their objects in English. The object of a verb is a noun phrase and in many cases the first item in an English noun phrase is a determiner. This means that for many transitive verbs, the bigram *verb the* will occur quite frequently. But it is much more interesting to know whether there are patterns relating verbs and the entities which are their objects. \nCollocation analysis uncovers such patterns by looking at co-occurrences within a window of a certain size, for example three tokens on either side of the target. Collocation analysis gives information about the frequency of the co-occurrence of words and also a statistical measure of how likely that frequency is, given the overall frequencies of the terms in the corpus. Measures commonly applied include [Mutual Information](https://en.wikipedia.org/wiki/Mutual_information) scores and [Log-Likelihood](https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood) scores.\nCollocations can also tell us about the meanings of words. If a word has collocates which fall into semantically distinct groups, this can indicate ambiguity or polysemy. And if different words share patterns of collocation, this can be evidence that the words are at least partial synonyms. \n\nThis graphic shows collocation relations in Darwin’s Origin of Species visualised as a network - the likelihood of a pair of words occurring in close proximity in the text is indicated by the weight of the line linking them:\n\n![Collocation patterns in *Origin of Species* as a network](/collocation_network.png)\n\nThis article uses bigram frequencies as part of an analysis of language change:\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eSchweinberger, Martin. 2021. Ongoing change in the Australian English amplifier system. \u003ci\u003eAustralian Journal of Linguistics\u003c/i\u003e 41(2). 166–194. \u003ca href=\"https://doi.org/10.1080/07268602.2021.1931028\" target=\"_blank\"\u003ehttps://doi.org/10.1080/07268602.2021.1931028\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\nAn article which uses concordances and collocation analysis:\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eBaker, Paul \u0026 Tony McEnery. 2005. A corpus-based approach to discourses of refugees and asylum seekers in UN and newspaper texts. \u003ci\u003eJournal of Language and Politics\u003c/i\u003e 4(2). 197–226.\u003c/font\u003e\u003c/div\u003e\n\nThis research uses the discovery of shared patterns of collocation as evidence that the words are at least partial synonyms:\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eMcEnery, Tony \u0026 Helen Baker. 2017. \u003ci\u003eCorpus linguistics and 17th-century prostitution: computational linguistics and history\u003c/i\u003e (Corpus and Discourse. Research in Corpus and Discourse). London; New York, NY: Bloomsbury Academic. (especially chapters 4 and 5)\u003c/font\u003e\u003c/div\u003e\n\nThis [tutorial](https://slcladal.github.io/coll.html) from LADAL on analysing co-occurences and collocations uses a notebook containing R code as a method to extract and visualise semantic links between words.\n\n\n\n#### Keywords\nKeyword analysis is a statistically robust method of comparing frequencies of words in corpora. It tells us which words are more frequent (or less frequent) than would be expected in one text compared to another text and gives an estimate of the probability of the result. Keyword analysis uses two corpora: a **target** corpus, which is the material of interest, and a **comparison** corpus. Frequency lists are made for each corpus and then frequency of individual types in each corpus are compared. Keywords are ones which occur more ( or less) frequently in the target corpus than expected given the reference corpus. \nThe **keyness** of individual items is a quantitative measure of how unexpected the frequency is; chi-square is a one possible measure of this, but a log-likelihood measure is more commonly used. Positive keywords are words which occur more commonly than expected; negative keywords are words which occur less commonly than expected.\n\nThis visualisation shows a comparison of positive distinguishing words for three texts (Charles Darwin’s *Origin*, Herman Melville’s *Moby Dick*, and George Orwell’s *1984*), words that occur more commonly than we expect in one text when taking the other two texts as a comparison: \n\n![Keywords from three texts](/keywords.png)\n\nThis paper applies keyword analysis to Australian text data sourced from a television series script:\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eBednarek, Monika. 2020. Keyword analysis and the indexing of Aboriginal and Torres Strait Islander identity: A corpus linguistic analysis of the Australian Indigenous TV drama Redfern Now. \u003ci\u003eInternational Journal of Corpus Linguistics\u003c/i\u003e 25/4: 369-99. \u003ca href=\"http://doi.org/10.1075/ijcl.00031.bed\" target=\"_blank\"\u003ehttp://doi.org/10.1075/ijcl.00031.bed\u003c/a\u003e\u003c/font\u003e\u003c/div\u003e\n\nTony McEnery describes using the keyword analysis method to compare four varieties of English in this chapter:\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eMcEnery, Tony. 2016. Keywords. In Paul Baker \u0026 Jesse Egbert (eds.), *Triangulating methodological approaches in corpus-linguistic research* (Routledge Advances in Corpus Linguistics 17), 20–32. New York: Routledge.\u003c/font\u003e\u003c/div\u003e\n\nThis article explores how to assess Shakespeare’s use of words to build characters by using keyword analysis of the characters' dialog:\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eCulpeper, Jonathan. 2002. Computers, language and characterisation: An analysis of six characters in Romeo and Juliet. In \u003ci\u003eConversation in Life and in Literature: Papers from the ASLA Symposium\u003c/i\u003e (Association Suedoise de Linguistique Appliquee (ASLA) 15), 11–30. Uppsala: Universitetstryckeriet. (\u003ca href=\"https://lexically.net/wordsmith/corpus_linguistics_links/Keywords-Culpeper.pdf\" target=\"_blank\"\u003epdf\u003c/a\u003e)\u003c/font\u003e\u003c/div\u003e\n\n\n\n### More complex methods – Classification {#classification}\n\nClassification methods aim to assign some unit of analysis, such as a word or a document, to a class. For example, a document (or a portion of a document) can be classified as having positive or negative sentiment. These methods are all examples of supervised machine learning. An algorithm is trained on the basis of annotated data to identify classifiers in the data – features which correlate in some way with the annotated classifications. If the algorithm achieves good results on testing data (classified by human judgment), then it can be used to classify unannotated data.\n\n\n\n#### Document Classification\nThe task here is to assign documents to categories automatically. An everyday example of this procedure is spam filtering of email that may be applied by internet service providers and also within email applications. An example of this technique being used in research would be automatically identifying historical court records as referring either to violent crimes, property offences, or other crimes. \n\nThe following two articles taken together give an account of a technique for partially automating the training phase of classification and then of how the classifiers allowed researchers to access new information in a large and complex text.\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eLeavy, Susan, Mark T Keane \u0026 Emilie Pine. 2019. Patterns in language: Text analysis of government reports on the Irish industrial school system with word embedding. \u003ci\u003eDigital Scholarship in the Humanities\u003c/i\u003e 34(Supplement_1). i110–i122. \u003ca href=\"https://doi.org/10.1093/llc/fqz012\" target=\"_blank\"\u003ehttps://doi.org/10.1093/llc/fqz012\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003ePine, Emilie, Susan Leavy \u0026 Mark T. Keane. 2017. Re-reading the Ryan Report: Witnessing via Close and Distant Reading. \u003ci\u003eÉire-Ireland\u003c/i\u003e 52(1–2). 198–215. https://doi.org/10.1353/eir.2017.0009. (\u003ca href=\"https://researchrepository.ucd.ie/handle/10197/10287\" target=\"_blank\"\u003eavailable online\u003c/a\u003e)\u003c/font\u003e\u003c/div\u003e\n\n[Wikipedia](https://en.wikipedia.org/wiki/Document_classification)\n\n\n\n#### Sentiment analysis\nSentiment analysis assigns documents according to the affect which they express. In simple cases, this can mean sorting documents into those which a express a positive view and those which express a negative view (with a neutral position sometimes also included). Such classifications are the basis for aggregated ratings - for example, online listings of movies and restaurants. A sentiment value is assigned to individual reviews then an aggregate score is calculated based on those values and that aggregate score is the rating presented to the user. More sophisticated sentiment analysis can assign values on a scale. Some sentiment analysis tools use dictionaries of terms with sentiment values assigned to those terms; these are known as pre-trained or pre-determined classifiers.\n\nThe following figure shows the results of the sentiment analysis of four texts (*The Adventures of Huckleberry Finn* by Mark Twain, *1984* by George Orwell, *The Colour out of Space* by H.P.Lovecraft, and *On the Origin of Species* by Charles Darwin) using the Word-Emotion Association Lexicon (Mohammad and Turney 2013). The graphic shows what percentage of each text can be assigned to each of eight categories of sentiment:\n \n![Sentiment analysis of four texts](/sentiment_analysis.png)\n\nThe Wikipedia entry for [Sentiment Analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) gives more information and examples particularly in relation to the use of sentiment analysis as a tool used in online settings. \n\nLADAL’s [Sentiment Analysis tutorial](https://slcladal.github.io/sentiment.html) uses a notebook containing R code as a method of performing sentiment analysis.\n\nThis article discusses problems in assembling training data for complex sentiment analysis tasks and then applies the results to oral history interviews with Holocaust survivors:\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eBlanke, Tobias, Michael Bryant \u0026 Mark Hedges. 2020. Understanding memories of the Holocaust—A new approach to neural networks in the digital humanities. \u003ci\u003eDigital Scholarship in the Humanities\u003c/i\u003e 35(1). 17–33. \u003ca href=\"https://doi.org/10.1093/llc/fqy082\" target=\"_blank\"\u003ehttps://doi.org/10.1093/llc/fqy082\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\n\n\n#### Named Entity Recognition\nNamed Entity Recognition involves two levels of classification. First, segments of text are classified as either denoting or not denoting an entity: for example, a person, a place or an organization. The identified entities can then be classified as belonging to one of the types of entity.\n\nThe Wikipedia entry explaining [named-entity recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) gives further detail about the technique.\n\nThis article looks at the problems encountered when applying a well-known entity recognition package ([Stanford](https://nlp.stanford.edu/software/CRF-NER.html)) to historical newspapers in the National Library of Australia’s Trove collection:\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eMac Kim, Sunghwan \u0026 Steve Cassidy. 2015. Finding names in Trove: Named Entity Recognition for Australian historical newspapers. In \u003ci\u003eProceedings of the Australasian Language Technology Association Workshop 2015\u003c/i\u003e, 57–65. (\u003ca href=\"https://aclanthology.org/U15-1007.pdf\" target=\"_blank\"\u003epdf\u003c/a\u003e)\u003c/font\u003e\u003c/div\u003e\n\nThis article (section 6.3) discusses why entity recognition is not as useful as might be expected when studying names in novels:\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eDalen-Oskam, K. van. 2013. Names in novels: An experiment in computational stylistics. \u003ci\u003eLiterary and Linguistic Computing\u003c/i\u003e 28(2). 359–370. \u003ca href=\"https://doi.org/10.1093/llc/fqs007\" target=\"_blank\"\u003ehttps://doi.org/10.1093/llc/fqs007\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\n\n\n\n#### Computational Stylistics (Stylometry) \nThis method is also referred to as authorship attribution as the classification task is to assess patterns of language use in order to decide whether to attribute a piece of text to a particular author (and with what degree of confidence). Seemingly simple classifiers are used for this task as they are assumed to be less open to conscious manipulation by writers. For example, comparative patterns of occurrence of function words such as *the* and *a/an* are considered a better classifier than occurrences of content words. Character n-grams, that is sequences of characters of a specified length, have also proven to be a good classifier for use in this task. A recent example of these techniques being applied in a case which received a good deal of public attention was the controversy about whether [Robert Galbraith was really J.K Rowling](https://languagelog.ldc.upenn.edu/nll/?p=5315).\n\nThe Wikipedia entry on [stylometry](https://en.wikipedia.org/wiki/Stylometry) gives further information on the methodology. \n\n\n\nThis article applies stylometric techniques to a classic of Chinese literature:\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eZhu, Haoran, Lei Lei \u0026 Hugh Craig. 2021. Prose, Verse and Authorship in Dream of the Red Chamber: A Stylometric Analysis. \u003ci\u003eJournal of Quantitative Linguistics\u003c/i\u003e 28(4). 289–305. \u003ca href=\"https://doi.org/10.1080/09296174.2020.1724677\" target=\"_blank\"\u003ehttps://doi.org/10.1080/09296174.2020.1724677\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\nAn overview of the use of function words in stylometry:\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eGarcia, A. M. \u0026 J. C. Martin. 2007. Function Words in Authorship Attribution Studies. \u003ci\u003eLiterary and Linguistic Computing\u003c/i\u003e 22(1). 49–66. \u003ca href=\"https://doi.org/10.1093/llc/fql048\" target=\"_blank\"\u003ehttps://doi.org/10.1093/llc/fql048\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\n\nA classic stylometric study using Bayesian statistics rather than machine learning is:\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eMosteller, Frederick \u0026 David Lee Wallace. 1984. \u003ci\u003eApplied Bayesian and classical inference: the case of the Federalist papers\u003c/i\u003e. New York: Springer-Verlag.\u003c/font\u003e\u003c/div\u003e\n\n\n### More complex methods – Others {#others}\n\n#### Topic models\nTopic modeling is a method which tries to recover abstract ‘topics’ which occur in a collection of documents. The underlying assumption is that different topics will tend to be associated with different words, different documents will tend to be associated with different topics, and therefore the distribution of words across documents allows us to find topics. The complete model includes the strength of association (or probability) between each word and each topic, and between each topic and each document. A topic consists of a group of words and it is up to the researcher to decide if a semantically coherent interpretation can be given to any of the topics recovered. The number of topics to be recovered is specified in advance.\n\nThe example visualisation below is based on topic modeling of the State of the Union addresses given by US presidents, and shows the relative importance of different topics over time. In the right hand part of the figure, the words most closely linked to each topic are listed; the researcher has not attempted to give labels to these (although in some cases, it is not too hard to imagine what labels we might use). Note also that words are not uniquely linked to topics - for example, the word *state* is closely linked to seven of the topics in this model.\n \n![Topics in the State of the Union Address over time](/topic_models.png)\n\nThe Wikipedia entry for [topic models](https://en.wikipedia.org/wiki/Topic_model) gives a more detailed explanation of the process.\n\nThis [topic modeling tutorial](https://slcladal.github.io/topicmodels.html) from LADAL uses R coding to process textual data and generate a topic model from that data.\n\n\u003ci\u003ePoetics\u003c/i\u003e 41(6) is a journal issue devoted to the use of topic models in literary studies: the introduction to the journal (by Mohr and Bogdanov: \u003ca href=\"https://doi.org/10.1016/j.poetic.2013.10.001\" target=\"_blank\"\u003ehttps://doi.org/10.1016/j.poetic.2013.10.001\u003c/a\u003e) provides a useful overview of the method.\n\nAnd this paper uses topic modeling as one tool in trying to improve access to a huge collection of scholarly literature:\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eMimno, David. 2012. Computational historiography: Data mining in a century of classics journals. \u003ci\u003eJournal on Computing and Cultural Heritage\u003c/i\u003e 5(1). 1–19. \u003ca href=\"https://doi.org/10.1145/2160165.2160168\" target=\"_blank\"\u003ehttps://doi.org/10.1145/2160165.2160168\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\n\n\n\n#### Network Analysis\nNetwork analysis allows us to produce visualisations of the relationships between entities within a dataset. Analysis of social networks is a classic application of the method, but words and documents can also be thought of as entities and the relationships between them can then be analysed with this method. (see example visualisation of Darwin’s *Origin of Species* above)\nHere is another example of a network graph illustrating the relationships between the characters of Shakespeare’s *Romeo and Juliet*:\n\n![Network of characters in Romeo and Juliet](/network_RandJ-1.png)\n\nThis article gives several examples of how representing collocational links between words as a network can lead to insight into meaning relations:\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eBrezina, Vaclav, Tony McEnery \u0026 Stephen Wattam. 2015. Collocations in context: A new perspective on collocation networks. \u003ci\u003eInternational Journal of Corpus Linguistics\u003c/i\u003e 20(2). 139–173. \u003ca href=\"https://doi.org/10.1075/ijcl.20.2.01bre\" target=\"_blank\"\u003ehttps://doi.org/10.1075/ijcl.20.2.01bre\u003c/a\u003e. (pdf)\u003c/font\u003e\u003c/div\u003e\n\nWikipedia has articles on network theory in [general](https://en.wikipedia.org/wiki/Network_theory) and on [social network analysis](https://en.wikipedia.org/wiki/Social_network_analysis).\nin particular.\n\nLADAL’s tutorial on [Network Analysis](https://slcladal.github.io/net.html) introduces this method using R coding.\n\n\n\n### Visualisation {#visualisation}\n\nVisualisation is an important technique for exploring data, allowing us to see patterns easily, and also for presenting results. \nThere are many methods for creating visualisations and this article gives an overview of some possibilities for visualising corpus data:\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eSiirtola, Harri, Terttu Nevalainen, Tanja Säily \u0026 Kari-Jouko Räihä. 2011. Visualisation of text corpora: A case study of the PCEEC. \u003ci\u003eHow to Deal with Data: Problems and Approaches to the Investigation of the English Language over Time and Space\u003c/i\u003e. Helsinki: VARIENG 7. \u003ca href=\"https://varieng.helsinki.fi/series/volumes/07/siirtola_et_al/index.html\" target=\"_blank\"\u003e[html]\u003c/a\u003e\u003c/font\u003e\u003c/div\u003e\n\n\nIf you would like to see something more complex, this article includes animations showing change in use of semantic space over time – but you need to have full access to the online publication to see it.\n\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eHilpert, Martin \u0026 Florent Perek. 2015. Meaning change in a petri dish: constructions, semantic vector spaces, and motion charts. \u003ci\u003eLinguistics Vanguard\u003c/i\u003e 1(1). \u003ca href=\"https://doi.org/10.1515/lingvan-2015-0013\" target=\"_blank\"\u003ehttps://doi.org/10.1515/lingvan-2015-0013\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e \n\nThis LADAL tutorial on [data visualisation](https://slcladal.github.io/introviz.html) in R makes use of the [ggplot2](https://ggplot2.tidyverse.org/) package to create some common data visualisations using code.\n\n"},{"title":"Organisation","slug":"organisation","content":"# Organisation\n\nATAP is one strand of the partnership between the Australian Research Data Commons ([ARDC](https://ardc.edu.au/)) and the [School of Languages and Cultures](https://languages-cultures.uq.edu.au/) at the [University of Queensland](https://www.uq.edu.au/). This partnership includes a number of  projects that explore language-related technologies, data collection infrastructure and Indigenous capability programs. These projects are being led out of the Language Technology and Data Analytics Lab ([LADAL](https://slcladal.github.io/index.html)), which is overseen by [Professor Michael Haugh](https://languages-cultures.uq.edu.au/profile/1498/michael-haugh) and [Dr Martin Schweinberger](https://languages-cultures.uq.edu.au/profile/4295/martin-schweinberger).\n\n\u003chr /\u003e\n\n## Partner Institutions:\n\n**University of Queensland:** \n\n- Professor Michael Haugh\n- Dr Martin Schweinberger\n\n**University of Sydney:**\n\n- Professor Monika Bednarek (Sydney Corpus Lab)\n\n**AARNet**\n\n- Dr Sara King\n- Ryan Fraser\n\u003cbr /\u003e\n\u003cbr /\u003e\n\u003chr /\u003e\n\u003cbr /\u003e\n\n## Project Team\n\u003cbr /\u003e\n\n### Technology and Interoperability Team\n\n- **Lead: Peter Sefton**\n\n- Moises Sacal\n- Marco La Rosa\n- Michael D’Silva (AARNet)\n- River Smith\n- Alvin Sebastian\n\n### Applications and Training Team\n\n- **Lead: Ben Foley**\n\n- Michael Niemann\n- Marius Mathers (Sydney Informatics Hub)\n\n### Data and Policy Team\n\n- **Lead: Kathrin Kaiser**\n\n- Cale Johnstone\n- Maria Weaver\n- Adam Bell (AARNet)\n\n### Engagement and Outreach Team\n\n- **Lead: Simon Musgrave**\n\n- Sara King (AARNet)\n- Leah Gustafson\n- Harriet Sheppard\n\n\u003cbr /\u003e\n\n### Project Manager: Marco Fahmi\n\n### Project Coordinator: Leah Gustafson\n\n"},{"title":"Research Objects","slug":"research_objects","content":"\n\nThe ATAP workbench will enable researchers to work in ways which improve the reproducibility and accountability of their research. To do this, the output of your work in ATAP will be a fully documented research object.\n\n![Illustration of an RO-Crate and its contents](/ro-crate.png)\n\nResearch objects are a single package that collects the data, code and other resources used in a piece of research and allows it to be a cite-able research output in its own right. At the end of your work in ATAP, there will be the automated option to package together the components of your workflow, such as:\n- Raw data\n- Transformed data\n- A record of the notebooks which you have used\n- Additional scripts and codes\n- Results\n- Visualisations\n- High quality metadata\n\nATAP will output an [RO-Crate](https://www.researchobject.org/ro-crate/) that contains these objects. An output of this kind can be assigned a unique identifier (such as a doi), and it can be published via services such as Zenodo or figshare so it can be cited in publications. This will make it easy to fulfil the increasingly common requirement of journals to make available the data that supports a publication.\n"},{"title":"Resources","slug":"resources","content":"# Resources\n\n[Language Technology and Data Analysis Laboratory (LADAL)](https://slcladal.github.io/)\n\nLADAL aims to help develop computational and digital skills by providing information and practical, hands-on tutorials on data and text analytics as well as on statistical methods relevant for language research.\n\n\u003cbr /\u003e\n\n[GLAM Workbench](https://glam-workbench.net/)\nThe GLAM workbench is a collection of tools, tutorials, examples, and hacks created by Tim Sherratt to help you work with data from galleries, libraries, archives, and museums (the GLAM sector). The primary focus is Australia and New Zealand, but new collections are being added all the time.\n\n\u003cbr /\u003e\n\nThe [Sydney Corpus Lab](https://sydneycorpuslab.com/) aims to promote corpus linguistics in Australia, hosts a growing list of English-language corpora, and features regular blogs about corpus linguistic analysis.\n\n\u003cbr /\u003e\n\n[CONSTELLATE](https://constellate.org/)\nConstellate is the text analytics service from the not-for-profit ITHAKA - the same people who brought you JSTOR and Portico. It is a platform for teaching, learning, and performing text analysis using the world’s leading archival repositories of scholarly and primary source content.\n\n\u003cbr /\u003e\n\n[The Art of Literary Text Analysis](https://github.com/sgsinclair/alta/blob/master/ipynb/ArtOfLiteraryTextAnalysis.ipynb)\n\nThe Art of Literary Text Analysis (ALTA) has three objectives.\n- First, to introduce concepts and methodologies for literary text analysis programming. It doesn't assume you know how to program or how to use digital tools for analyzing texts.\n- Second, to show a range of analytical techniques for the study of texts. While it cannot explain and demonstrate everything, it provides a starting point for humanists with links to other materials.\n- Third, to provide utility notebooks you can use for operating on different texts. These are less well documented and combine ideas from the introductory notebooks.\n\n\u003cbr /\u003e\n\n[Introduction to Cultural Analytics \u0026 Python](https://melaniewalsh.github.io/Intro-Cultural-Analytics/welcome.html) is an online textbook by Melanie Walsh, which offers an introduction to the programming language Python that is specifically designed for people interested in the humanities and social sciences. This book demonstrates how Python can be used to study cultural materials such as song lyrics, short stories, newspaper articles, tweets, Reddit posts, and film screenplays. It also introduces computational methods such as web scraping, APIs, topic modeling, Named Entity Recognition (NER), network analysis, and mapping.\n\n\u003cbr /\u003e\n\n[Text Analysis Pedagogy Institute](https://labs.jstor.org/tapi/) is an open educational institute for the benefit of teachers (and aspiring teachers) of text analysis in the digital humanities.\n\n\u003cbr /\u003e\n\n[The Programming Historian](https://programminghistorian.org/) publishes novice-friendly, peer-reviewed tutorials that help humanists learn a wide range of digital tools, techniques, and workflows to facilitate research and teaching.\n\n\u003cbr /\u003e\n\n[Quinn Dombrowski's list of relevant courses and tutorials](https://github.com/quinnanya/dh-jupyter)\nA collection of Jupyter notebooks in many human and computer languages for doing digital humanities. \n"},{"title":"Text Analysis Overview","slug":"text_analysis","content":"# Text Analysis Overview\n\n### Understanding Text as Data\n\nMany definitions of *data* include an element such as *individual items of information*. If we consider *text* to include any sort of language in use, covering different modalities (spoken, written signed) and different extents (from individual sounds to multi-volume books), then fitting text to this definition requires some abstraction. We have to define some unit or units of analysis and we can then treat each of those units as an individual item of information. Examples of such units include documents, sentences and words. But *word* is not as simple a unit as you may think.\n\n### What's in a word?\n\nThe term *word* is problematic. It is well-known to linguists that phonological words (defined by sound patterns), syntactic words (defined by combinatorial possibilities) and orthographic words (defined by the conventions of a writing system) do not always coincide. And even when we are only looking at written material, there are problems. How many words are there in this sentence?\n\u003ccenter\u003e\u003cem\u003eThe cat sat on the mat\u003c/em\u003e\u003c/center\u003e\n\nOne answer is that there are six words; that is, there are six groups of characters which are separated according to typographical convention. But there is another answer: There are five words, that is five distinct sequences of characters and one of those sequences (*the*) occurs twice. The terms standardly used to make this distinction are **type** and **token**. **Tokens** are instances of **types**, therefore if we count tokens, we count without considering repetition, while if we count types, we do consider repetition. In our example, there are five types (*the, cat, sat, on, mat*) but six tokens, because there are two tokens of one of the types (*the*).\n\nThere is a further distinction we may need to make which we can see if we consider another question:\n\n\u003ccenter\u003eAre \u003ci\u003ecat\u003c/i\u003e and \u003ci\u003ecats\u003c/i\u003e the same word?\u003c/center\u003e\n\nThey are distinct types, and therefore must also be distinct as tokens. But we have an intuition that at some level they are related, that there is some more abstract item which underlies both of them. This concept is usually referred to as a **lemma** (there is more about this concept on the [Data Preparation](../data_prep) page).\n\n### Text Analysis Workflow\nThis brief introduction to text analysis divides the process into three parts. In the first stage, the text is made into data. It is divided into the units appropriate for the analysis to be carried out and shaped to a format which our analytic tools can work with. The second stage is the analysis proper, including its interpretation. A wide range of analytic methods can be used, and we give a survey of some of the commonly used possibilities. Finally, our data, methods and results can be documented and packaged as a **research object** which can be stored and reused.\n\n[Data Preparation](../data_prep) \u0026emsp;\u0026emsp; [Useful Methods](../methods) \u0026emsp;\u0026emsp; [Research Objects](../research_objects)\n"}]}},"__N_SSG":true},"page":"/[page]","query":{"page":"methods"},"buildId":"4R3jnWJKeJMM9F6PmJ3D9","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>