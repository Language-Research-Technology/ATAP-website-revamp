<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>ATAP</title><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/7c9960b738380dfb.css" as="style"/><link rel="stylesheet" href="/_next/static/css/7c9960b738380dfb.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-69bfa6990bb9e155.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-f4ae3437c92c1efc.js" defer=""></script><script src="/_next/static/chunks/pages/_app-460153e333fa776a.js" defer=""></script><script src="/_next/static/chunks/pages/%5Bpage%5D-d2184b8ab241f292.js" defer=""></script><script src="/_next/static/dHCBNQAcQ0TXdzqZzRIFS/_buildManifest.js" defer=""></script><script src="/_next/static/dHCBNQAcQ0TXdzqZzRIFS/_ssgManifest.js" defer=""></script><script src="/_next/static/dHCBNQAcQ0TXdzqZzRIFS/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div><div class="flex min-h-screen flex-col"><div class="sticky top-0 h-20 bg-white shadow-sm md:static md:shadow-none"><nav class="container flex h-full items-center justify-between text-gray-700"><a href="/"><img src="/ATAP_logo-sm.png" class="h-16"/></a><div class="hidden items-center space-x-2 md:flex"><ul class="flex divide-x divide-slate-400 text-sm"><a class="cursor-pointer px-4 font-semibold first:pl-0 hover:underline" href="/"><li>Home</li></a><a class="cursor-pointer px-4 font-semibold first:pl-0 hover:underline" href="/posts"><li>Blog</li></a><a class="cursor-pointer px-4 font-semibold first:pl-0 hover:underline" href="/text_analysis"><li>Text Analysis</li></a><a class="cursor-pointer px-4 font-semibold first:pl-0 hover:underline" href="/events"><li>Events</li></a><a class="cursor-pointer px-4 font-semibold first:pl-0 hover:underline" href="/resources"><li>Resources</li></a><a class="cursor-pointer px-4 font-semibold first:pl-0 hover:underline" href="/organisation"><li>Organisation</li></a></ul><div class="relative flex"><input placeholder="Page name" class="w-0 mr-2 border transition-all" value=""/><button><svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"></path></svg></button></div></div><div class="md:hidden"><button class="relative h-10 w-10 text-gray-500 focus:outline-none"><span class="sr-only">Open main menu</span><div class="absolute left-1/2 top-1/2 block w-5 -translate-x-1/2 -translate-y-1/2 transform"><span aria-hidden="true" class="absolute block h-0.5 w-5 transform bg-current transition duration-500 ease-in-out rounded-sm -translate-y-1.5"></span><span aria-hidden="true" class="absolute block h-0.5 w-5 transform bg-current transition duration-500 ease-in-out rounded-sm false"></span><span aria-hidden="true" class="absolute block h-0.5 w-5 transform bg-current transition duration-500 ease-in-out rounded-sm translate-y-1.5"></span></div></button></div></nav><div></div></div><main class="container flex-1 py-8"><article class="prose max-w-none lg:prose-xl"><p>The ATAP workbench will enable researchers to work in ways which improve the reproducibility and accountability of their research. To do this, the output of your work in ATAP will be a fully documented research object.</p>
<p><img src="/ro-crate.png" alt="Illustration of an RO-Crate and its contents"></p>
<p>Research objects are a single package that collects the data, code and other resources used in a piece of research and allows it to be a cite-able research output in its own right. At the end of your work in ATAP, there will be the automated option to package together the components of your workflow, such as:</p>
<ul>
<li>Raw data</li>
<li>Transformed data</li>
<li>A record of the notebooks which you have used</li>
<li>Additional scripts and codes</li>
<li>Results</li>
<li>Visualisations</li>
<li>High quality metadata</li>
</ul>
<p>ATAP will output an <a href="https://www.researchobject.org/ro-crate/">RO-Crate</a> that contains these objects. An output of this kind can be assigned a unique identifier (such as a doi), and it can be published via services such as Zenodo or figshare so it can be cited in publications. This will make it easy to fulfil the increasingly common requirement of journals to make available the data that supports a publication.</p>
</article></main><div class="bg-secondary"><div class="container flex h-10 items-center justify-center text-white"><small>2021,<!-- --> <!-- --><a href="https://www.dynamicsoflanguage.edu.au/">CoEDL, Centre of Excellence for the Dynamics of Language.</a></small></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"content":"\u003cp\u003eThe ATAP workbench will enable researchers to work in ways which improve the reproducibility and accountability of their research. To do this, the output of your work in ATAP will be a fully documented research object.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/ro-crate.png\" alt=\"Illustration of an RO-Crate and its contents\"\u003e\u003c/p\u003e\n\u003cp\u003eResearch objects are a single package that collects the data, code and other resources used in a piece of research and allows it to be a cite-able research output in its own right. At the end of your work in ATAP, there will be the automated option to package together the components of your workflow, such as:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRaw data\u003c/li\u003e\n\u003cli\u003eTransformed data\u003c/li\u003e\n\u003cli\u003eA record of the notebooks which you have used\u003c/li\u003e\n\u003cli\u003eAdditional scripts and codes\u003c/li\u003e\n\u003cli\u003eResults\u003c/li\u003e\n\u003cli\u003eVisualisations\u003c/li\u003e\n\u003cli\u003eHigh quality metadata\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eATAP will output an \u003ca href=\"https://www.researchobject.org/ro-crate/\"\u003eRO-Crate\u003c/a\u003e that contains these objects. An output of this kind can be assigned a unique identifier (such as a doi), and it can be published via services such as Zenodo or figshare so it can be cited in publications. This will make it easy to fulfil the increasingly common requirement of journals to make available the data that supports a publication.\u003c/p\u003e\n","searchContent":{"posts":[{"title":"What are the FAIR and CARE principles and why should corpus linguists know about them?","slug":"fair-and-care","tags":["FAIR","CARE"],"content":"# FAIR and CARE\nData is becoming increasingly important in today’s world, so corpus linguists might feel that the rest of the world is finally catching up. But the rest of the world are bringing with them new approaches to how data is handled. This means that fields such as corpus linguistics may need to reassess their practices. Such reassessment includes addressing concerns about how data is stored and who can access it (data stewardship) – concerns that are a part of the [Open Science](https://en.wikipedia.org/wiki/Open_science) movement, ultimately grounded on principles of equity and accountability. \n\nThe most influential approach to data stewardship today is the [FAIR](https://www.go-fair.org/) principles.\nAccording to these principles, data should be:\n- *Findable* \n\u003cbr /\u003e\n\u0026emsp; Metadata and data should be easy to find for both humans and computers. \n- *Accessible*\n\u003cbr /\u003e\n\u0026emsp; Once the user finds the required data, she/he/they need to know how can they be accessed, possibly including authentication and authorisation.\n- *Interoperable*\n\u003cbr /\u003e\n\u0026emsp; The data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing.\n- *Reusable*\n\u003cbr /\u003e\n\u0026emsp; The ultimate goal of FAIR is to optimise the reuse of data. To achieve this, metadata and data should be well-described so that they can be replicated and/or combined in different settings.\n\u003cbr /\u003e\u003cbr /\u003e\n\nIn general, corpus linguists do well on the interoperability criterion. Corpus data is usually stored in non-proprietary formats; even when some structure is imposed on the data, this is almost always in a form which is saved as a simple text file (e.g. csv files or xml annotations). Data stored in such formats is easy to move between applications. But what about the other three criteria? \n\nSome corpus data is easy to discover; it is findable. For example CLARIN, the [portal](https://www.clarin.eu/content/data) to the European Union language resource infrastructure, provides access to many large data collections, as does the [Linguistic Data Consortium](https://www.ldc.upenn.edu/) in the USA. However, some data is never made part of a large collection and often remains under the control of individual researchers or research teams. Such data may be almost impossible to find. Even if we can find such data, it is unlikely to be accompanied by good descriptions of the data and metadata, making reusability problematic. Of course, big corpora such as the [British National Corpus](http://www.natcorp.ox.ac.uk/) will be both findable and accompanied by comprehensive corpus manuals. However, it is worth considering how to make other corpora more findable, including the provision of corpus manuals or corpus descriptions. Corpus resource databases such as [CoRD](https://varieng.helsinki.fi/CoRD/) do aim to work towards this principle.\n\nAccessibility may also be an issue for some data. Copyright law may allow use of material for individual research but prohibit any further distribution of the material. The FAIR approach to such cases is that metadata should be available so that interested parties can know that a data holding exists (F), and the metadata will include information about the conditions under which the data may or may not be shared or reused (A and R). \n\n![FAIR and CARE principles](/fair-care.png)\n\nImage from Global Indigenous Data Alliance (https://www.gida-global.org/)\n\u003cbr /\u003e\u003cbr /\u003e\nFor linguists, there is another very important set of principles concerning data, the CARE principles developed by the Global Indigenous Data Alliance:\n\u003cbr /\u003e\n- *Collective Benefit*\n\u003cbr /\u003e\n\u0026emsp; Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data.\n\u003cbr /\u003e\n- *Authority to control*\n\u003cbr /\u003e\n\u0026emsp;Indigenous Peoples’ rights and interests in Indigenous data must be recognised and their authority to control such data be empowered.\n\u003cbr /\u003e\n- *Responsibility*\n\u003cbr /\u003e\n\u0026emsp;Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit.\n\u003cbr /\u003e\n- *Ethics*\n\u003cbr /\u003e\n\u0026emsp;Indigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem.\n\n\u003cbr /\u003e\nThese principles are presented as applying particularly to Indigenous data, but we believe that researchers should adopt this approach in all cases where the people who participate in our research can be seen to have some moral rights in the information they have contributed. Respecting those moral rights should be demonstrated by recognising the participants’ authority to control how data is used, by seeking to ensure that participants derive benefit from use of the data, and by acting ethically and transparently in our relations with the participants. Deborah Cameron and her colleagues (Cameron et al 1993) raised similar issues almost 20 years ago, arguing that the imbalance of power in the relation between researchers and participants needed to be reduced. The CARE principles continue along this path, but go even further in explicitly returning power to the sources of information. \n\nCorpus data is often written language. We have already mentioned that copyright law is relevant to some such material, and that body of law protects at least some rights for the creators of the material. But corpus linguists also work with other kinds of data such as spoken language (spontaneous or produced as a response to some prompt) or written material produced by research participants according to some protocol. In such cases, ethical research practice should include addressing the issues raised by the CARE principles. Some aspects of this practice will fall under institutional ethics requirements (for example, thinking carefully about what permissions we request on consent forms), but other questions must be part of the relationship between the researcher and the research participants. Corpus linguists working with spoken, computer-mediated, or otherwise particularly sensitive data have been aware of at least some of these issues, but the CARE principles offer an opportunity to go further.\n\nAcquiring data for linguistic research takes effort and often that means money. It is therefore a good use of resources if any data we collect can be used by others. The FAIR principles provide a framework to make sharing and reusing data easier, and applying the CARE principles where relevant helps to ensure that our research has a sound ethical basis.\n\n\u003cbr /\u003e\n\u003chr /\u003e\n\u003cbr /\u003e\n\nNote: This post is based on the presentation ‘Advance Australia FAIR’,  given by Simon Musgrave and Michael Haugh to the 4th Forum on Englishes in Australia (LaTrobe University, August 27, 2021). \n\n\u003cbr /\u003e\n\nThanks to Leah Gustafson and Monika Bednarek for helpful comments on drafts.\n\n\u003cbr /\u003e\n\n**Reference:**\nCameron, Deborah, Elizabeth Frazer, Penelope Harvey, Ben Rampton \u0026 Kay Richardson. 1993. Ethics, advocacy and empowerment: Issues of method in researching language. Language \u0026 Communication 13(2). 81–94. [https://doi.org/10.1016/0271-5309(93)90001-4](https://doi.org/10.1016/0271-5309(93)90001-4)\n\n\n\n"}],"pages":[{"title":"Preparing Text Data","slug":"data_prep","content":"\n### Introducing data preparation concepts \n\n\u003cfigure\u003e\n  \u003cimg\n    src=\"data-prep-approach-time.png\" \n    alt=\"What data scientists spend the most time doing - pie chart\"  \n  /\u003e\n  \u003cfigcaption align = \"center\"\u003e\u003cb\u003eWhat data scientists spend the most time doing - pie chart\u003c/b\u003e\u003c/figcaption\u003e\n\u003c/figure\u003e\n \nThis graphic is, sadly, all too true. Data scientists and those who are using \ndata as part of their research spend much of their time preparing their \ndataset and transforming its structure into a format that can be used (often \nreferred to as [data wrangling](https://online.hbs.edu/blog/post/data-wrangling)\nor data munging). The Australian Text Analytics Platform will offer a range \nof tools to assist in cleaning text data and performing other preliminary \noperations which can prepare the data for analysis. ATAP analysis notebooks \nassume a common data structure, however the platform will provide notebooks \ncontaining code for transforming data into the structure that is needed for \nthe procedure(s) in the analysis notebooks.\n\nThere are two main processes that are needed to prepare text data for analysis\n, [cleaning](#common-cleaning-techniques) and [annotation](#annotation), and \nthe ones that you will need to use will depend on the dataset that you are \nusing. \n\n### Common Cleaning Techniques\n- **Making all of the text lower case:**\n  This ensures that e.g. *dog* and *Dog* will not be treated as different items\n  and is important if you are going to use analytic methods which rely on \n  counting items. However, if you are planning to extract entities from your \n  text data, retaining capital letters may be important.\n\n- **Standardising spelling:**\n  At least for English text, there are some well-known spelling variations, \n  some with a geographical context (*colour/color*) and some that are more a \n  matter of personal preference (*recognise/recognize*). As with case, \n  standardising spelling ensures that pairs like the examples are treated as \n  tokens of the same type.\n\n- **Removing stopwords:**\n  Stopwords are function words that are not interesting for many analyses and \n  we can safely remove them from our data using a stoplist. The 20 most \n  frequently occurring words in the \n  [British National Corpus](https://www.english-corpora.org/bnc/) are *the, of\n  , and, a, in, to, it, is, was, to, I, for, you, he, be, with, on, that, by\n  *, and *at*. The equivalent list for the Corpus of Contemporary American \n  English ([COCA](https://www.english-corpora.org/coca/)) is *the, be, and, of\n  , a, in, to, have, to, it, I, that, for, you, he, with, on, do, say* and *\n  this*. \n  Some of the differences between the two are because the COCA counts are of \n  lemmas (see Lemmatization below) which are the base forms of a word (think *\n  dog* and *dogs*). Packages such as [nltk](https://www.nltk.org/) and \n  [spaCy  ](https://spacy.io/) include standard stoplists for various \n  languages, and it is possible to specify other words to be excluded.\n\n- **Removing punctuation:**\n  Punctuation can change how a text analysis package identifies a word. For \n  instance to be sure that *dog* and *dog?* are not treated as different items,\n  removing punctuation is good practice.\n\n- **Removing numbers:** \n  Sometimes the presence of numbers in documents can lead to artefacts in \n  analysis. For example, in a collection of documents with page numbering, \n  the numbers might show up as collocates of words or as part of a topic in a \n  topic model. To avoid this, removing numbers is also good practice. This \n  can present a challenge where numbers might be of interest (e.g. a study of \n  mathematics textbooks).\n\n- **Removing whitespace:** \n  Whitespace can be another possible source of artefacts in analysis, \n  especially if the source material uses a lot of tabs.\n\n### Annotation\nAnnotation is the process of adding information to your base dataset in order \nto make it possible to apply analytic techniques. In some cases, this may be \na manual process. For example, much of the annotation which is described in \nthe Text Encoding Initiative [Guidelines](https://tei-c.org/guidelines/) \nrequires a human making decisions although, in some cases, manual annotation \nprocesses may also be scaled up to large text corpora using text \nclassification or information extraction technologies. \n\nHowever some annotation can be carried out automatically, and there are two \nimportant kinds of annotation for text which fall into this category.\n\n\n- **Part-of-speech tagging (POS-tagging):** \n  For some analytic procedures, knowing the part of speech (or class of words) \n  that an item belongs to is important. For languages where good pre-trained \n  models exist, this annotation can be carried out automatically to a high \n  level of accuracy – for English, we expect an accuracy rate better than 95%. \n  POS-taggers generally provide more information than just whether an item is a \n  noun or a verb, they also distinguish singular and plural forms of nouns and \n  tell us whether a verb’s form is present tense form or a past tense. The tag \n  sets which are used can therefore be quite extensive, and there are various \n  tag sets in use such as the \n  [Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n  tags and the [CLAWS](https://ucrel.lancs.ac.uk/claws5tags.html) tags used by \n  the British National Corpus.\n\n[LADAL](https://slcladal.github.io/) has some excellent resources that \ndiscuss POS tagging in more detail.\n\n- **Lemmatization:** \n  The distinctions between different forms of a single lexeme can be a \n  hindrance in analysis especially if we are interested in lexical semantics in \n  texts. Lemmatization identifies the base forms of words (or lemmas) in a text \n  so that all forms of an item are treated together. For example: *dog* and *dogs* \n  will both be instances of the lemma DOG. *eat, eats, eating* and *ate* \n  will all be treated as tokens of the lemma EAT. \n  As noted above, POS-tags give information about the form of words and are \n  generally part of the annotation in lemmatization. A lemma, along with a POS-\n  tag, can be reconstructed to the original form if \n  necessary.\n"},{"title":"Events","slug":"events","content":"# Events\n\n[Webinars](#webinars) \u0026emsp;\u0026emsp;\n[Forthcoming workshops](#forthcoming-workshops) \u0026emsp;\u0026emsp;\n[Previous workshops](#previous-workshops) \u0026emsp;\u0026emsp;\n[Office Hours](#office-hours)\n\n### Webinars {#webinars}\n\nOur webinar series is a joint initiative with the Language Technology and Data Analysis Laboratory ([LADAL](https://slcladal.github.io/index.html)), (School of Languages and Cultures, University of Queensland). LADAL sponsored [webinars](https://slcladal.github.io/webinars2022.html) take place in the alternate months.\n\nAll webinars take place at 8:00PM Brisbane time which is UTC+10. Zoom links will be available one week prior to the event.\n\n#### 4 April 2022 - Keoni Mahelona: A practical approach to Indigenous data sovereignty\nKeoni Mahelona is the Chief Technical Officer of [Te Hiku Media](https://tehiku.nz/) where he is a part of the team developing the Kaitiakitanga Licence. This licence seeks to balance the importance of publicly accessible data with the reality that indigenous peoples may not have access to the resources that enable them to benefit from public data. By simply opening access to data and knowledge, indigenous people could be further colonised and taken advantage of in a digital, modern world. Therefore Keoni is committed to devising data governance regimes which enable Indigenous people to reclaim and maintain sovereignty over indigenous data.\n\n#### June 6 2022 - Barbara McGillivray: The *Journal of Open Humanities Data*\nBarbara McGillivray is a Turing Research Fellow at [The Alan Turing Institute](https://www.turing.ac.uk/), and Editor in Chief of the [Journal of Open Humanities Data](https://openhumanitiesdata.metajnl.com/). Since September 2021 she is also a lecturer in Digital Humanities and Cultural Computation at the [Department of Digital Humanities of King's College London](https://www.kcl.ac.uk/ddh). Before joining the Turing, she was language technologist in the Dictionary division of Oxford University Press and data scientist in the Open Research Group of Springer Nature. Her research at the Turing is on how words change meaning over time and how to model this change in computational ways. She works on machine-learning models for the change in meaning of words in historical times (Ancient Greek, Latin, eighteen-century English) and in contemporary texts (Twitter, web archives, emoji). Her interdisciplinary contribution covers Data Science, Natural Language Processing, Historical Linguistics and other humanistic fields, to push the boundaries of what academic disciplines separately have achieved so far on this topic.\n\n[Zoom link](https://uqz.zoom.us/j/83999047730?from=addon)\n\n#### August 1 2022 - Václav Cvrček: The Czech national Corpus\n[Václav Cvrček](https://ucnk.ff.cuni.cz/en/institute/people/vaclav-cvrcek-2/) is a linguist who deals with the description of the Czech language, especially with the use of large electronic corpora and quantitative methods. In 2013-2016 he worked as the director of the [Czech National Corpus](https://ucnk.ff.cuni.cz/en/) project, since 2016 he has been the deputy director. Recently, he has been focusing on research on textual variability and corpus-based discourse analysis with a focus on online media.\n\n[Zoom link](https://uqz.zoom.us/j/81439620559?from=addon)\n\n\n#### October 3 2022 - Paweł Kamocki: [topic tba]\nPaweł Kamocki is a legal expert in Leibniz-Institut für Deutsche Sprache, Mannheim. He studied linguistics and law, and in 2017 obtained his doctorate in law from the universities of Paris and Münster for a thesis on legal aspects of data-intensive university research, with a focus on Knowledge Commons. He worked as a research and teaching assistant at the Paris Descartes university (now: Université de Paris), then also in the private sector. He is certified to work as an attorney in France. An active member of the [CLARIN](https://www.clarin.eu/) community since 2012, he currently chairs the CLARIN Legal and Ethical Issues Committee. He also worked with other projects and initiatives in the field of research data policy (RDA, EUDAT) and co-created several LegalTech tools for researchers. One of his main research interests are legal issues in Machine Translation.\n\n[Zoom link](https://uqz.zoom.us/j/82090438697?from=addon)\n\n\n### Forthcoming workshops {#forthcoming-workshops}\n\n#### Network analysis and Topic Modeling on Twitter data using R\n\nThis online workshop is offered free to Australian researchers and research students and will cover:\n- Introduction to network theory.\n- Fundamental principles of using R for network analysis.\n- Topic modeling of tweet text using R.\nThe data used for the workshop will be an open source dataset of Twitter data relating to the 2019 Federal Election.\n\n**Date**: May 18 2022\n**Time**: 9:00AM - 12:00PM AEST\n**Venue**: Online\n\n[Registration](https://www.eventbrite.com.au/e/network-analysis-and-topic-modeling-on-twitter-data-using-r-registration-327690108937)\n\nBrought to you by the teams at the Australian Digital Observatory ([ADO](https://www.digitalobservatory.net.au/)) and the Australian Text Analytics Platform (ATAP) via the Australian Research Data Commons ([ARDC](https://ardc.edu.au/)).\n\n\n\n### Previous workshops {#previous-workshops}\n\n#### Monotreme Mania! Comparative text analytics on Twitter data\n**Date**: 16 March 2022\n**Event**: Joint event ADO and ATAP\n**Length**: 3 hours\n**Facilitators**: Sam Hames, Simon Musgrave\n\n#### An introduction to Jupyter notebooks for text analysis: Virtual workshop for absolute beginners\n**Date**: 24 November 2021\n**Event**: Digital Humanities Australasia 2021 Conference\n**Length**: 3 hours\n**Facilitators**: Sara King, Simon Musgrave\n\n\n### Office Hours {#office-hours}\n\nWe invite Australian researchers working with linguistics, text analytics, digital and computational methods, social media and web archives, and much more to attend our regular online office hours, jointly hosted with the [Digital Observatory](https://research.qut.edu.au/digitalobservatory/). Bring your technical questions, research problems and rough ideas and get advice and feedback from the combined expertise of our ARDC research infrastructure projects. No question is too small, and even if we don’t know the answer we are likely to be able to point you to someone who does.\n\nThese sessions run over Zoom from 2-3pm (Australia/Sydney time) every second Tuesday - [details](https://research.qut.edu.au/digitalobservatory/office-hours/).\n\n"},{"slug":"home","content":"\n__The Australian Text Analytics Platform is an open source environment \nthat provides researchers with tools and training for analysing, processing,\nand exploring text.__\n\nText analytics is a suite of methods which enable data-driven research \nby extracting and analysing machine-readable information from within \nunstructured text. Due to the increasing availability of large amounts of\nunstructured text, such techniques are becoming more and more important across\ndiverse research disciplines.\n\nText analysis tends to happen at either a basic, generic level (handled with \nexisting software packages) or with custom code specifically developed by \nprogrammers for a particular project. ATAP will support researchers \ntransitioning to code-based text analysis, with the resultant benefits of \nflexibility, reproducibility and reuse, and the possibility of exporting their \nresults and workflows as a fully documented research object.\n\nATAP will be a collaborative, cloud-based workbench environment, bringing \ntogether users and providers of data and text analytics tools. It will \nencourage researchers to adopt new methods, leading to greater flexibility and \ntransparency in research workflows. The platform will be accessible to \nresearchers with a broad range of experience and skills (including beginners) \nand across a range of disciplines. Support provided by the platform will \ninclude hands-on workshops, online training modules and online office hours, \nas well as advice and collaboration in selected partnerships.\n\n![ARDC logos](/AcknowledgeARDC.png)\n\nThe Australian Text Analytics Platform (ATAP) projects \n[received investment](https://doi.org/10.47486/PL074) from the Australian \nResearch Data Commons (ARDC). The ARDC is funded by the National Collaborative \nResearch Infrastructure Strategy (NCRIS).\n\nATAP acknowledges and pays respects to the Elders and Traditional Owners of the \nlands on which we live and work.\n"},{"title":"Useful Methods","slug":"methods","content":"\n\n[Counting words](#counting-words) \u0026emsp;\u0026emsp; \n[More complex methods - Classification](#classification) \u0026emsp;\u0026emsp; \n[More complex methods - Others](#others) \u0026emsp;\u0026emsp; \n[Visualisation](#visualisation)\n\n### Introduction\nThroughout this page, we have given links to further information in Wikipedia and in the tutorials provided by the Language Technology and Data Analysis Laboratory [LADAL](https://slcladal.github.io/) at the University of Queensland. We also have given references to published research using the methods we discuss.\n\nLADAL has an overview of [text analysis and distant reading](https://slcladal.github.io/textanalysis.html). \n\n\n\n### Counting Words {#counting-words}\n\n#### Word frequency\n\nKnowing how frequently words occur in a text can already give us information about that text and frequency lists based on large corpora are a useful tool in themselves - you can [download](https://kilgarriff.co.uk/bnc-readme.html) such lists for the (original) [British National Corpus](http://www.natcorp.ox.ac.uk/).\n\nTracking changes in the frequency of use of words across time has become popular since Google’s [n-gram viewer](https://books.google.com/ngrams) has been available. However, results from this tool have to treated with caution for reasons set out in this [blog-post](https://broadstreet.blog/2021/08/11/bad-ngrams/). \n\nComparing patterns of word frequency across texts can be part of authorship attribution. Patrick Juola [describes using this method](https://languagelog.ldc.upenn.edu/nll/?p=5315) when he tried to decide whether Robert Galbraith was really J.K Rowling.\n\n\nThis paper uses frequency and concordance analysis, with Australian data:\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eBednarek, Monika. 2020. Invisible or high-risk: Computer-assisted discourse analysis of references to Aboriginal and Torres Strait Islander people(s) and issues in a newspaper corpus about diabetes. \u003ci\u003ePLoS ONE\u003c/i\u003e 15/6: e0234486. \u003ca href=\"https://doi.org/10.1371/journal.pone.0234486\" target=\"_blank\"\u003ehttps://doi.org/10.1371/journal.pone.0234486\u003c/a\u003e \u003c/font\u003e\u003c/div\u003e\n\n\nThe ratio of types and tokens in a text has been used as a measure of lexical diversity in developmental and clinical studies as well as in stylistics. It has also been applied to theoretical problems in linguistics:\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eKettunen, Kimmo. 2014. Can Type-Token Ratio be Used to Show Morphological Complexity of Languages? \u003ci\u003eJournal of Quantitative Linguistics\u003c/i\u003e 21(3). 223–245. \u003ca href=\"https://doi.org/10.1080/09296174.2014.911506\" target=\"_blank\"\u003ehttps://doi.org/10.1080/09296174.2014.911506\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\n\n\n#### Concordance\nA concordance allows the researcher to see all instances of a word or phrase in a text, neatly aligned in a column and with preceding and following context (see image below).\nConcordances are often a first step in analysis. The concordance allows a researcher to see how a word is used and in what contexts. Most concordancing tools allow sorting of results by either preceding or following words – the coloured text in the example below shows that in this case the results have been sorted hierarchically on the three following words. This possibility can help in discovering patterns of co-occurrence. Concordances are also very useful when looking for good examples to illustrate a point. (The type of display seen in the example is often referred to as KeyWord In Context – KWIC. There is a possibility of confusion here, as there is a separate analytic method commonly referred to as [Keywords](#keywords).)\n\n![Example of a concordance](/concordance.png)\n\n(The example here was produced by [Antconc](https://www.laurenceanthony.net/software/antconc/)) \n\nThis [tutorial](https://slcladal.github.io/kwics.html) from LADAL on concordancing uses a notebook containing R code as a method of extracting concordance data.\n\n\n\n\n#### Clusters and collocations\nTwo methods can be used for counting the co-occurrence of items in text. \nClusters (sometimes known as n-grams) are sequences of adjacent items. A bigram is a sequence of two items, a trigram (3-gram) is a sequence of three items and so on. n-grams are types made up of more than one item, and therefore we can count the number of tokens of each n-gram in texts. n-grams are also the basis for a class of language models. (Google created a very large data set of English n-grams in developing their language-based algorithms and this [data is available](https://storage.googleapis.com/books/ngrams/books/datasetsv3.html).)\nCollocations are patterns of co-occurrence in which the items are not necessarily adjacent. An example of why this is important is verbs and their objects in English. The object of a verb is a noun phrase and in many cases the first item in an English noun phrase is a determiner. This means that for many transitive verbs, the bigram *verb the* will occur quite frequently. But it is much more interesting to know whether there are patterns relating verbs and the entities which are their objects. \nCollocation analysis uncovers such patterns by looking at co-occurrences within a window of a certain size, for example three tokens on either side of the target. Collocation analysis gives information about the frequency of the co-occurrence of words and also a statistical measure of how likely that frequency is, given the overall frequencies of the terms in the corpus. Measures commonly applied include [Mutual Information](https://en.wikipedia.org/wiki/Mutual_information) scores and [Log-Likelihood](https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood) scores.\nCollocations can also tell us about the meanings of words. If a word has collocates which fall into semantically distinct groups, this can indicate ambiguity or polysemy. And if different words share patterns of collocation, this can be evidence that the words are at least partial synonyms. \n\nThis graphic shows collocation relations in Darwin’s Origin of Species visualised as a network - the likelihood of a pair of words occurring in close proximity in the text is indicated by the weight of the line linking them:\n\n![Collocation patterns in *Origin of Species* as a network](/collocation_network.png)\n\nThis article uses bigram frequencies as part of an analysis of language change:\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eSchweinberger, Martin. 2021. Ongoing change in the Australian English amplifier system. \u003ci\u003eAustralian Journal of Linguistics\u003c/i\u003e 41(2). 166–194. \u003ca href=\"https://doi.org/10.1080/07268602.2021.1931028\" target=\"_blank\"\u003ehttps://doi.org/10.1080/07268602.2021.1931028\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\nAn article which uses concordances and collocation analysis:\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eBaker, Paul \u0026 Tony McEnery. 2005. A corpus-based approach to discourses of refugees and asylum seekers in UN and newspaper texts. \u003ci\u003eJournal of Language and Politics\u003c/i\u003e 4(2). 197–226.\u003c/font\u003e\u003c/div\u003e\n\nThis research uses the discovery of shared patterns of collocation as evidence that the words are at least partial synonyms:\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eMcEnery, Tony \u0026 Helen Baker. 2017. \u003ci\u003eCorpus linguistics and 17th-century prostitution: computational linguistics and history\u003c/i\u003e (Corpus and Discourse. Research in Corpus and Discourse). London; New York, NY: Bloomsbury Academic. (especially chapters 4 and 5)\u003c/font\u003e\u003c/div\u003e\n\nThis [tutorial](https://slcladal.github.io/coll.html) from LADAL on analysing co-occurences and collocations uses a notebook containing R code as a method to extract and visualise semantic links between words.\n\n\n\n#### Keywords\nKeyword analysis is a statistically robust method of comparing frequencies of words in corpora. It tells us which words are more frequent (or less frequent) than would be expected in one text compared to another text and gives an estimate of the probability of the result. Keyword analysis uses two corpora: a **target** corpus, which is the material of interest, and a **comparison** corpus. Frequency lists are made for each corpus and then frequency of individual types in each corpus are compared. Keywords are ones which occur more ( or less) frequently in the target corpus than expected given the reference corpus. \nThe **keyness** of individual items is a quantitative measure of how unexpected the frequency is; chi-square is a one possible measure of this, but a log-likelihood measure is more commonly used. Positive keywords are words which occur more commonly than expected; negative keywords are words which occur less commonly than expected.\n\nThis visualisation shows a comparison of positive distinguishing words for three texts (Charles Darwin’s *Origin*, Herman Melville’s *Moby Dick*, and George Orwell’s *1984*), words that occur more commonly than we expect in one text when taking the other two texts as a comparison: \n\n![Keywords from three texts](/keywords.png)\n\nThis paper applies keyword analysis to Australian text data sourced from a television series script:\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eBednarek, Monika. 2020. Keyword analysis and the indexing of Aboriginal and Torres Strait Islander identity: A corpus linguistic analysis of the Australian Indigenous TV drama Redfern Now. \u003ci\u003eInternational Journal of Corpus Linguistics\u003c/i\u003e 25/4: 369-99. \u003ca href=\"http://doi.org/10.1075/ijcl.00031.bed\" target=\"_blank\"\u003ehttp://doi.org/10.1075/ijcl.00031.bed\u003c/a\u003e\u003c/font\u003e\u003c/div\u003e\n\nTony McEnery describes using the keyword analysis method to compare four varieties of English in this chapter:\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eMcEnery, Tony. 2016. Keywords. In Paul Baker \u0026 Jesse Egbert (eds.), *Triangulating methodological approaches in corpus-linguistic research* (Routledge Advances in Corpus Linguistics 17), 20–32. New York: Routledge.\u003c/font\u003e\u003c/div\u003e\n\nThis article explores how to assess Shakespeare’s use of words to build characters by using keyword analysis of the characters' dialog:\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eCulpeper, Jonathan. 2002. Computers, language and characterisation: An analysis of six characters in Romeo and Juliet. In \u003ci\u003eConversation in Life and in Literature: Papers from the ASLA Symposium\u003c/i\u003e (Association Suedoise de Linguistique Appliquee (ASLA) 15), 11–30. Uppsala: Universitetstryckeriet. (\u003ca href=\"https://lexically.net/wordsmith/corpus_linguistics_links/Keywords-Culpeper.pdf\" target=\"_blank\"\u003epdf\u003c/a\u003e)\u003c/font\u003e\u003c/div\u003e\n\n\n\n### More complex methods – Classification {#classification}\n\nClassification methods aim to assign some unit of analysis, such as a word or a document, to a class. For example, a document (or a portion of a document) can be classified as having positive or negative sentiment. These methods are all examples of supervised machine learning. An algorithm is trained on the basis of annotated data to identify classifiers in the data – features which correlate in some way with the annotated classifications. If the algorithm achieves good results on testing data (classified by human judgment), then it can be used to classify unannotated data.\n\n\n\n#### Document Classification\nThe task here is to assign documents to categories automatically. An everyday example of this procedure is spam filtering of email that may be applied by internet service providers and also within email applications. An example of this technique being used in research would be automatically identifying historical court records as referring either to violent crimes, property offences, or other crimes. \n\nThe following two articles taken together give an account of a technique for partially automating the training phase of classification and then of how the classifiers allowed researchers to access new information in a large and complex text.\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eLeavy, Susan, Mark T Keane \u0026 Emilie Pine. 2019. Patterns in language: Text analysis of government reports on the Irish industrial school system with word embedding. \u003ci\u003eDigital Scholarship in the Humanities\u003c/i\u003e 34(Supplement_1). i110–i122. \u003ca href=\"https://doi.org/10.1093/llc/fqz012\" target=\"_blank\"\u003ehttps://doi.org/10.1093/llc/fqz012\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003ePine, Emilie, Susan Leavy \u0026 Mark T. Keane. 2017. Re-reading the Ryan Report: Witnessing via Close and Distant Reading. \u003ci\u003eÉire-Ireland\u003c/i\u003e 52(1–2). 198–215. https://doi.org/10.1353/eir.2017.0009. (\u003ca href=\"https://researchrepository.ucd.ie/handle/10197/10287\" target=\"_blank\"\u003eavailable online\u003c/a\u003e)\u003c/font\u003e\u003c/div\u003e\n\n[Wikipedia](https://en.wikipedia.org/wiki/Document_classification)\n\n\n\n#### Sentiment analysis\nSentiment analysis assigns documents according to the affect which they express. In simple cases, this can mean sorting documents into those which a express a positive view and those which express a negative view (with a neutral position sometimes also included). Such classifications are the basis for aggregated ratings - for example, online listings of movies and restaurants. A sentiment value is assigned to individual reviews then an aggregate score is calculated based on those values and that aggregate score is the rating presented to the user. More sophisticated sentiment analysis can assign values on a scale. Some sentiment analysis tools use dictionaries of terms with sentiment values assigned to those terms; these are known as pre-trained or pre-determined classifiers.\n\nThe following figure shows the results of the sentiment analysis of four texts (*The Adventures of Huckleberry Finn* by Mark Twain, *1984* by George Orwell, *The Colour out of Space* by H.P.Lovecraft, and *On the Origin of Species* by Charles Darwin) using the Word-Emotion Association Lexicon (Mohammad and Turney 2013). The graphic shows what percentage of each text can be assigned to each of eight categories of sentiment:\n \n![Sentiment analysis of four texts](/sentiment_analysis.png)\n\nThe Wikipedia entry for [Sentiment Analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) gives more information and examples particularly in relation to the use of sentiment analysis as a tool used in online settings. \n\nLADAL’s [Sentiment Analysis tutorial](https://slcladal.github.io/sentiment.html) uses a notebook containing R code as a method of performing sentiment analysis.\n\nThis article discusses problems in assembling training data for complex sentiment analysis tasks and then applies the results to oral history interviews with Holocaust survivors:\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eBlanke, Tobias, Michael Bryant \u0026 Mark Hedges. 2020. Understanding memories of the Holocaust—A new approach to neural networks in the digital humanities. \u003ci\u003eDigital Scholarship in the Humanities\u003c/i\u003e 35(1). 17–33. \u003ca href=\"https://doi.org/10.1093/llc/fqy082\" target=\"_blank\"\u003ehttps://doi.org/10.1093/llc/fqy082\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\n\n\n#### Named Entity Recognition\nNamed Entity Recognition involves two levels of classification. First, segments of text are classified as either denoting or not denoting an entity: for example, a person, a place or an organization. The identified entities can then be classified as belonging to one of the types of entity.\n\nThe Wikipedia entry explaining [named-entity recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) gives further detail about the technique.\n\nThis article looks at the problems encountered when applying a well-known entity recognition package ([Stanford](https://nlp.stanford.edu/software/CRF-NER.html)) to historical newspapers in the National Library of Australia’s Trove collection:\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eMac Kim, Sunghwan \u0026 Steve Cassidy. 2015. Finding names in Trove: Named Entity Recognition for Australian historical newspapers. In \u003ci\u003eProceedings of the Australasian Language Technology Association Workshop 2015\u003c/i\u003e, 57–65. (\u003ca href=\"https://aclanthology.org/U15-1007.pdf\" target=\"_blank\"\u003epdf\u003c/a\u003e)\u003c/font\u003e\u003c/div\u003e\n\nThis article (section 6.3) discusses why entity recognition is not as useful as might be expected when studying names in novels:\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eDalen-Oskam, K. van. 2013. Names in novels: An experiment in computational stylistics. \u003ci\u003eLiterary and Linguistic Computing\u003c/i\u003e 28(2). 359–370. \u003ca href=\"https://doi.org/10.1093/llc/fqs007\" target=\"_blank\"\u003ehttps://doi.org/10.1093/llc/fqs007\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\n\n\n\n#### Computational Stylistics (Stylometry) \nThis method is also referred to as authorship attribution as the classification task is to assess patterns of language use in order to decide whether to attribute a piece of text to a particular author (and with what degree of confidence). Seemingly simple classifiers are used for this task as they are assumed to be less open to conscious manipulation by writers. For example, comparative patterns of occurrence of function words such as *the* and *a/an* are considered a better classifier than occurrences of content words. Character n-grams, that is sequences of characters of a specified length, have also proven to be a good classifier for use in this task. A recent example of these techniques being applied in a case which received a good deal of public attention was the controversy about whether [Robert Galbraith was really J.K Rowling](https://languagelog.ldc.upenn.edu/nll/?p=5315).\n\nThe Wikipedia entry on [stylometry](https://en.wikipedia.org/wiki/Stylometry) gives further information on the methodology. \n\n\n\nThis article applies stylometric techniques to a classic of Chinese literature:\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eZhu, Haoran, Lei Lei \u0026 Hugh Craig. 2021. Prose, Verse and Authorship in Dream of the Red Chamber: A Stylometric Analysis. \u003ci\u003eJournal of Quantitative Linguistics\u003c/i\u003e 28(4). 289–305. \u003ca href=\"https://doi.org/10.1080/09296174.2020.1724677\" target=\"_blank\"\u003ehttps://doi.org/10.1080/09296174.2020.1724677\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\nAn overview of the use of function words in stylometry:\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eGarcia, A. M. \u0026 J. C. Martin. 2007. Function Words in Authorship Attribution Studies. \u003ci\u003eLiterary and Linguistic Computing\u003c/i\u003e 22(1). 49–66. \u003ca href=\"https://doi.org/10.1093/llc/fql048\" target=\"_blank\"\u003ehttps://doi.org/10.1093/llc/fql048\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\n\nA classic stylometric study using Bayesian statistics rather than machine learning is:\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eMosteller, Frederick \u0026 David Lee Wallace. 1984. \u003ci\u003eApplied Bayesian and classical inference: the case of the Federalist papers\u003c/i\u003e. New York: Springer-Verlag.\u003c/font\u003e\u003c/div\u003e\n\n\n### More complex methods – Others {#others}\n\n#### Topic models\nTopic modeling is a method which tries to recover abstract ‘topics’ which occur in a collection of documents. The underlying assumption is that different topics will tend to be associated with different words, different documents will tend to be associated with different topics, and therefore the distribution of words across documents allows us to find topics. The complete model includes the strength of association (or probability) between each word and each topic, and between each topic and each document. A topic consists of a group of words and it is up to the researcher to decide if a semantically coherent interpretation can be given to any of the topics recovered. The number of topics to be recovered is specified in advance.\n\nThe example visualisation below is based on topic modeling of the State of the Union addresses given by US presidents, and shows the relative importance of different topics over time. In the right hand part of the figure, the words most closely linked to each topic are listed; the researcher has not attempted to give labels to these (although in some cases, it is not too hard to imagine what labels we might use). Note also that words are not uniquely linked to topics - for example, the word *state* is closely linked to seven of the topics in this model.\n \n![Topics in the State of the Union Address over time](/topic_models.png)\n\nThe Wikipedia entry for [topic models](https://en.wikipedia.org/wiki/Topic_model) gives a more detailed explanation of the process.\n\nThis [topic modeling tutorial](https://slcladal.github.io/topicmodels.html) from LADAL uses R coding to process textual data and generate a topic model from that data.\n\n\u003ci\u003ePoetics\u003c/i\u003e 41(6) is a journal issue devoted to the use of topic models in literary studies: the introduction to the journal (by Mohr and Bogdanov: \u003ca href=\"https://doi.org/10.1016/j.poetic.2013.10.001\" target=\"_blank\"\u003ehttps://doi.org/10.1016/j.poetic.2013.10.001\u003c/a\u003e) provides a useful overview of the method.\n\nAnd this paper uses topic modeling as one tool in trying to improve access to a huge collection of scholarly literature:\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eMimno, David. 2012. Computational historiography: Data mining in a century of classics journals. \u003ci\u003eJournal on Computing and Cultural Heritage\u003c/i\u003e 5(1). 1–19. \u003ca href=\"https://doi.org/10.1145/2160165.2160168\" target=\"_blank\"\u003ehttps://doi.org/10.1145/2160165.2160168\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e\n\n\n\n#### Network Analysis\nNetwork analysis allows us to produce visualisations of the relationships between entities within a dataset. Analysis of social networks is a classic application of the method, but words and documents can also be thought of as entities and the relationships between them can then be analysed with this method. (see example visualisation of Darwin’s *Origin of Species* above)\nHere is another example of a network graph illustrating the relationships between the characters of Shakespeare’s *Romeo and Juliet*:\n\n![Network of characters in Romeo and Juliet](/network_RandJ-1.png)\n\nThis article gives several examples of how representing collocational links between words as a network can lead to insight into meaning relations:\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eBrezina, Vaclav, Tony McEnery \u0026 Stephen Wattam. 2015. Collocations in context: A new perspective on collocation networks. \u003ci\u003eInternational Journal of Corpus Linguistics\u003c/i\u003e 20(2). 139–173. \u003ca href=\"https://doi.org/10.1075/ijcl.20.2.01bre\" target=\"_blank\"\u003ehttps://doi.org/10.1075/ijcl.20.2.01bre\u003c/a\u003e. (pdf)\u003c/font\u003e\u003c/div\u003e\n\nWikipedia has articles on network theory in [general](https://en.wikipedia.org/wiki/Network_theory) and on [social network analysis](https://en.wikipedia.org/wiki/Social_network_analysis).\nin particular.\n\nLADAL’s tutorial on [Network Analysis](https://slcladal.github.io/net.html) introduces this method using R coding.\n\n\n\n### Visualisation {#visualisation}\n\nVisualisation is an important technique for exploring data, allowing us to see patterns easily, and also for presenting results. \nThere are many methods for creating visualisations and this article gives an overview of some possibilities for visualising corpus data:\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eSiirtola, Harri, Terttu Nevalainen, Tanja Säily \u0026 Kari-Jouko Räihä. 2011. Visualisation of text corpora: A case study of the PCEEC. \u003ci\u003eHow to Deal with Data: Problems and Approaches to the Investigation of the English Language over Time and Space\u003c/i\u003e. Helsinki: VARIENG 7. \u003ca href=\"https://varieng.helsinki.fi/series/volumes/07/siirtola_et_al/index.html\" target=\"_blank\"\u003e[html]\u003c/a\u003e\u003c/font\u003e\u003c/div\u003e\n\n\nIf you would like to see something more complex, this article includes animations showing change in use of semantic space over time – but you need to have full access to the online publication to see it.\n\n\n\u003cdiv class=\"reference\"\u003e\u003cfont size=\"3\"\u003eHilpert, Martin \u0026 Florent Perek. 2015. Meaning change in a petri dish: constructions, semantic vector spaces, and motion charts. \u003ci\u003eLinguistics Vanguard\u003c/i\u003e 1(1). \u003ca href=\"https://doi.org/10.1515/lingvan-2015-0013\" target=\"_blank\"\u003ehttps://doi.org/10.1515/lingvan-2015-0013\u003c/a\u003e.\u003c/font\u003e\u003c/div\u003e \n\nThis LADAL tutorial on [data visualisation](https://slcladal.github.io/introviz.html) in R makes use of the [ggplot2](https://ggplot2.tidyverse.org/) package to create some common data visualisations using code.\n\n"},{"title":"Organisation","slug":"organisation","content":"# Organisation\n\nATAP is one strand of the partnership between the Australian Research Data Commons ([ARDC](https://ardc.edu.au/)) and the [School of Languages and Cultures](https://languages-cultures.uq.edu.au/) at the [University of Queensland](https://www.uq.edu.au/). This partnership includes a number of  projects that explore language-related technologies, data collection infrastructure and Indigenous capability programs. These projects are being led out of the Language Technology and Data Analytics Lab ([LADAL](https://slcladal.github.io/index.html)), which is overseen by [Professor Michael Haugh](https://languages-cultures.uq.edu.au/profile/1498/michael-haugh) and [Dr Martin Schweinberger](https://languages-cultures.uq.edu.au/profile/4295/martin-schweinberger).\n\n\u003chr /\u003e\n\n## Partner Institutions:\n\n**University of Queensland:** \n\n- Professor Michael Haugh\n- Dr Martin Schweinberger\n\n**University of Sydney:**\n\n- Professor Monika Bednarek (Sydney Corpus Lab)\n\n**AARNet**\n\n- Dr Sara King\n- Ryan Fraser\n\u003cbr /\u003e\n\u003cbr /\u003e\n\u003chr /\u003e\n\u003cbr /\u003e\n\n## Project Team\n\u003cbr /\u003e\n\n### Technology and Interoperability Team\n\n- **Lead: Peter Sefton**\n\n- Moises Sacal\n- Marco La Rosa\n- Michael D’Silva (AARNet)\n- River Smith\n- Alvin Sebastian\n\n### Applications and Training Team\n\n- **Lead: Ben Foley**\n\n- Michael Niemann\n- Marius Mathers (Sydney Informatics Hub)\n\n### Data and Policy Team\n\n- **Lead: Kathrin Kaiser**\n\n- Cale Johnstone\n- Maria Weaver\n- Adam Bell (AARNet)\n\n### Engagement and Outreach Team\n\n- **Lead: Simon Musgrave**\n\n- Sara King (AARNet)\n- Leah Gustafson\n- Harriet Sheppard\n\n\u003cbr /\u003e\n\n### Project Manager: Marco Fahmi\n\n### Project Coordinator: Leah Gustafson\n\n"},{"title":"Research Objects","slug":"research_objects","content":"\n\nThe ATAP workbench will enable researchers to work in ways which improve the reproducibility and accountability of their research. To do this, the output of your work in ATAP will be a fully documented research object.\n\n![Illustration of an RO-Crate and its contents](/ro-crate.png)\n\nResearch objects are a single package that collects the data, code and other resources used in a piece of research and allows it to be a cite-able research output in its own right. At the end of your work in ATAP, there will be the automated option to package together the components of your workflow, such as:\n- Raw data\n- Transformed data\n- A record of the notebooks which you have used\n- Additional scripts and codes\n- Results\n- Visualisations\n- High quality metadata\n\nATAP will output an [RO-Crate](https://www.researchobject.org/ro-crate/) that contains these objects. An output of this kind can be assigned a unique identifier (such as a doi), and it can be published via services such as Zenodo or figshare so it can be cited in publications. This will make it easy to fulfil the increasingly common requirement of journals to make available the data that supports a publication.\n"},{"title":"Resources","slug":"resources","content":"# Resources\n\n[Language Technology and Data Analysis Laboratory (LADAL)](https://slcladal.github.io/)\n\nLADAL aims to help develop computational and digital skills by providing information and practical, hands-on tutorials on data and text analytics as well as on statistical methods relevant for language research.\n\n\u003cbr /\u003e\n\n[GLAM Workbench](https://glam-workbench.net/)\nThe GLAM workbench is a collection of tools, tutorials, examples, and hacks created by Tim Sherratt to help you work with data from galleries, libraries, archives, and museums (the GLAM sector). The primary focus is Australia and New Zealand, but new collections are being added all the time.\n\n\u003cbr /\u003e\n\nThe [Sydney Corpus Lab](https://sydneycorpuslab.com/) aims to promote corpus linguistics in Australia, hosts a growing list of English-language corpora, and features regular blogs about corpus linguistic analysis.\n\n\u003cbr /\u003e\n\n[CONSTELLATE](https://constellate.org/)\nConstellate is the text analytics service from the not-for-profit ITHAKA - the same people who brought you JSTOR and Portico. It is a platform for teaching, learning, and performing text analysis using the world’s leading archival repositories of scholarly and primary source content.\n\n\u003cbr /\u003e\n\n[The Art of Literary Text Analysis](https://github.com/sgsinclair/alta/blob/master/ipynb/ArtOfLiteraryTextAnalysis.ipynb)\n\nThe Art of Literary Text Analysis (ALTA) has three objectives.\n- First, to introduce concepts and methodologies for literary text analysis programming. It doesn't assume you know how to program or how to use digital tools for analyzing texts.\n- Second, to show a range of analytical techniques for the study of texts. While it cannot explain and demonstrate everything, it provides a starting point for humanists with links to other materials.\n- Third, to provide utility notebooks you can use for operating on different texts. These are less well documented and combine ideas from the introductory notebooks.\n\n\u003cbr /\u003e\n\n[Introduction to Cultural Analytics \u0026 Python](https://melaniewalsh.github.io/Intro-Cultural-Analytics/welcome.html) is an online textbook by Melanie Walsh, which offers an introduction to the programming language Python that is specifically designed for people interested in the humanities and social sciences. This book demonstrates how Python can be used to study cultural materials such as song lyrics, short stories, newspaper articles, tweets, Reddit posts, and film screenplays. It also introduces computational methods such as web scraping, APIs, topic modeling, Named Entity Recognition (NER), network analysis, and mapping.\n\n\u003cbr /\u003e\n\n[Text Analysis Pedagogy Institute](https://labs.jstor.org/tapi/) is an open educational institute for the benefit of teachers (and aspiring teachers) of text analysis in the digital humanities.\n\n\u003cbr /\u003e\n\n[The Programming Historian](https://programminghistorian.org/) publishes novice-friendly, peer-reviewed tutorials that help humanists learn a wide range of digital tools, techniques, and workflows to facilitate research and teaching.\n\n\u003cbr /\u003e\n\n[Quinn Dombrowski's list of relevant courses and tutorials](https://github.com/quinnanya/dh-jupyter)\nA collection of Jupyter notebooks in many human and computer languages for doing digital humanities. \n"},{"title":"Text Analysis Overview","slug":"text_analysis","content":"# Text Analysis Overview\n\n### Understanding Text as Data\n\nMany definitions of *data* include an element such as *individual items of information*. If we consider *text* to include any sort of language in use, covering different modalities (spoken, written signed) and different extents (from individual sounds to multi-volume books), then fitting text to this definition requires some abstraction. We have to define some unit or units of analysis and we can then treat each of those units as an individual item of information. Examples of such units include documents, sentences and words. But *word* is not as simple a unit as you may think.\n\n### What's in a word?\n\nThe term *word* is problematic. It is well-known to linguists that phonological words (defined by sound patterns), syntactic words (defined by combinatorial possibilities) and orthographic words (defined by the conventions of a writing system) do not always coincide. And even when we are only looking at written material, there are problems. How many words are there in this sentence?\n\u003ccenter\u003e\u003cem\u003eThe cat sat on the mat\u003c/em\u003e\u003c/center\u003e\n\nOne answer is that there are six words; that is, there are six groups of characters which are separated according to typographical convention. But there is another answer: There are five words, that is five distinct sequences of characters and one of those sequences (*the*) occurs twice. The terms standardly used to make this distinction are **type** and **token**. **Tokens** are instances of **types**, therefore if we count tokens, we count without considering repetition, while if we count types, we do consider repetition. In our example, there are five types (*the, cat, sat, on, mat*) but six tokens, because there are two tokens of one of the types (*the*).\n\nThere is a further distinction we may need to make which we can see if we consider another question:\n\n\u003ccenter\u003eAre \u003ci\u003ecat\u003c/i\u003e and \u003ci\u003ecats\u003c/i\u003e the same word?\u003c/center\u003e\n\nThey are distinct types, and therefore must also be distinct as tokens. But we have an intuition that at some level they are related, that there is some more abstract item which underlies both of them. This concept is usually referred to as a **lemma** (there is more about this concept on the [Data Preparation](../data_prep) page).\n\n### Text Analysis Workflow\nThis brief introduction to text analysis divides the process into three parts. In the first stage, the text is made into data. It is divided into the units appropriate for the analysis to be carried out and shaped to a format which our analytic tools can work with. The second stage is the analysis proper, including its interpretation. A wide range of analytic methods can be used, and we give a survey of some of the commonly used possibilities. Finally, our data, methods and results can be documented and packaged as a **research object** which can be stored and reused.\n\n[Data Preparation](../data_prep) \u0026emsp;\u0026emsp; [Useful Methods](../methods) \u0026emsp;\u0026emsp; [Research Objects](../research_objects)\n"}]}},"__N_SSG":true},"page":"/[page]","query":{"page":"research_objects"},"buildId":"dHCBNQAcQ0TXdzqZzRIFS","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>